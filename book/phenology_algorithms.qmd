# Phenology algorithms

The example above illustrating Hopkin's law and the explicit temperature sensitivity of vegetation phenology to changes in temperature shows the importance of monitoring these changes within the context of climate change. In the previous example we have used an operational land surface vegetation phenology product.

Many products exist, depending on the remote sensing platform or data used, and cover a wide range of spatial scales. For example the HR-VPP product allows for continuous vegetation monitoring using 10m resolution Sentinel-2 data, while the previous example relies on 500m resolution MODIS data. While MODIS provides a landscape wide measure of vegetation phenology the spatial scale of Sentinel-2 and near real time data allows for fine grained assessments of crop growth.

## Algorithms

Most phenology products rely on a limited set of algorithms, or various iterations of them. Throughout both implementations and scientific literature we can divide the methods used in two categories:

- curve fitting
- threshold based methods

The curve fitting approach fits a prescribed model structure to the data, by varying a number of parameters. This functional description of the vegetation growth can then be used to derive phenology metrics by considering various inflection points of this function. Common approaches here are to use the first or second derivative of a fitted function (and zero crossings) to determine when large changes in vegetation phenology happen.

FIGURE

The most simple approach around, used with or without explicit curve fitting, is the use of a simple threshold based method. Here, a phenology event is registered if a given vegetation index threshold is exceeded. Although scaling these products for global coverage is challenging creating your own algorithm and implementing it on a local or regional scale is fairly simple. Below I will show you how to calculate phenology for a vegetation index time series, for the region outlined in @sec-phenology-trends .

### Acquiring demo data

To get started we need to download some additional information, in particular vegetation time series. In this example, and to lighten the download requirements, I will use Lear Area Index (LAI) data instead of the more common EVI or NDVI time series used in phenology products.

```{r eval = FALSE}
# download data
df <- MODISTools::mt_subset(
  product = "MCD15A3H",
  lat = 42.536669726040884,
  lon = -72.17951595626516,
  band = "Lai_500m",
  start = "2002-01-01",
  end = "2022-12-31",
  km_lr = 0,
  km_ab = 0,
  site_name = "HF",
  internal = TRUE,
  progress = TRUE
)
```

## Data smoothing / interpollation

```{r include = FALSE}
library(signal)
library(terra)
library(dplyr)

# preloading data
# for fast processing
phenology <- readRDS(here::here("data/phenology_2012.rds"))
dem <- terra::rast(here::here("data-raw/srtm_38_03.tif"))
land_cover <- readRDS(here::here("data/land-cover_2012.rds"))
df <- readRDS(here::here("data/lai_time_series.rds"))
```


```{r}
# generally spatialization of these operations is
# embarrassingly parallel
# yes this is a formal term https://en.wikipedia.org/wiki/Embarrassingly_parallel
# in short every pixel can be run separately
# without taking others into account.
# Many spatial operations work in this way
# especially in remote sensing (some exceptions do apply)

# What did we not do?
# - No QA/QC control
# - only for brevity this is skipped
# - should ALWAYS be considered!

# scale values appropriately
df$value <- df$value * as.numeric(df$scale)

# convert dates to proper date formats and
# convert the date to a single year
df$date <- as.Date(df$calendar_date)
df$year <- as.numeric(format(df$date, "%Y"))

# smooth this original input data using a
# savitski-golay filter
df$smooth <- signal::sgolayfilt(df$value, p = 3, n = 31)

# expand the time series to a daily time step
# and merge with the original data
expanded_df <- dplyr::tibble(
  date = seq.Date(min(subset$date), max(subset$date), by = 1)
)
df <- dplyr::left_join(expanded_df, df)

# finally interpolate the expanded dataset (fill in NA values)
df$smooth_int <- signal::interp1(1:nrow(df), df$smooth, 1:nrow(df), 'spline')

# quick plot
plot(df$date, df$value, type = "p", col = "red")
lines(df$date, df$smooth_int)

phenology <- df |>
  group_by(year) |>
  summarize(
    SOS = date[which(smooth_int > 3)][1],
    EOS = last(date[which(smooth_int > 3)])
  )

# plot the phenology dates on the previous plot
# (update graph)
plot(df$date, df$value, type = "p", col = "red")
lines(df$date, df$smooth_int)
abline(v = phenology$SOS)
abline(v = phenology$EOS)

# potential issues?
# - fixed LAI threshold (varies per vegetation type)
# - does not account for incomplete years
# - provides absolute dates (not always helpful)
df$smooth_int_scaled <- scales::rescale(df$smooth_int, to = c(0,1))

phenology <- df |>
  group_by(year) |>
  summarize(
    SOS = date[which(smooth_int_scaled > 0.5)][1],
    EOS = last(date[which(smooth_int_scaled > 0.5)]),
    SOS_25 = date[which(smooth_int_scaled > 0.25)][1],
    EOS_25 = last(date[which(smooth_int_scaled > 0.25)])
  )

# plot the phenology dates on the previous plot
# (update graph)
plot(df$date, df$value, type = "p", col = "red")
lines(df$date, df$smooth_int)
abline(v = phenology$SOS, col = "green")
abline(v = phenology$EOS, col = "red")
abline(v = phenology$SOS_25, col = "green")
abline(v = phenology$EOS_25, col = "red")
```

### Spatial phenology estimates

```{r eval = FALSE}
# Terra based approach, executing a function across a cube

# time series processing
library(MODISTools)
library(signal)
library(terra)
library(dplyr)
last <- function(x) { return( x[length(x)] ) }

# prepare data (tidy way)
r <- readRDS("data/lai_2012.rds")

# convert values using scale parameter
r <- r |>
  filter(
    band == "Lai_500m"
  )

r <- mt_to_terra(r)

phenophases <- function(
    df,
    return = "SOS",
    threshold = 0.25
) {

  # split out useful info
  value <- as.vector(df)
  date <- as.Date(names(df))

  # smooth this original input data using a
  # savitski-golay filter
  smooth <- signal::sgolayfilt(value, p = 3, n = 31)

  # expand the time series to a daily time step
  # and merge with the original data
  date_expanded <- seq.Date(min(date), max(date), by = 1)
  smooth_int <- rep(NA, length(date_expanded))
  smooth_int[which(date_expanded %in% date)] <- smooth

  # finally interpolate the expanded dataset (fill in NA values)
  smooth_int <- signal::interp1(1:length(smooth_int), smooth_int, 1:length(smooth_int), 'spline')
  smooth_int_scaled <- scales::rescale(smooth_int, to = c(0,1))

  phenophase <- ifelse(
    return == "SOS",
    date_expanded[which(smooth_int_scaled > threshold)][1],
    last(date_expanded[which(smooth_int_scaled > threshold)][1])
  )

  # convert to doy
  doy <- as.numeric(format(as.Date(phenophase, origin = "1970-01-01"),"%j"))
  return(doy)
}

phenology_map <- app(r, phenophases)


pheno <- readRDS("data/phenology_2012.rds")
pheno <-  pheno |>
  mutate(
    value = ifelse(value == 32767, NA, value),
    value = format(as.Date(value, origin = "1970-01-01"),"%j")
  )

pheno <- mt_to_terra(pheno)
plot(pheno)

plot(abs(pheno - phenology_map), breaks = c(0, 10, 25, 50, 75))

```

### Inference and comparisons to actual products

Too scale the exercise we need to expand the data used from a single time series to a data cube of LAI values. You therefore need to download some additional data, covering the area previously used when detecting phenology trends in @sec-phenology-trends .

```{r eval = FALSE}
# download LAI data
lai_2012 <- mt_subset(
  product = "MCD15A3H",
  lat = 46.6756,
  lon = 7.85480,
  band = c("Lai_500m","FparLai_QC"),
  start = "2012-01-01",
  end = "2012-12-31",
  km_lr = 100,
  km_ab = 100,
  site_name = "swiss",
  internal = TRUE,
  progress = TRUE
)
```
