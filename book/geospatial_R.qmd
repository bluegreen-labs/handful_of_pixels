# Geospatial data in R {#sec-geospatial}

```{r include = FALSE}
library(dplyr)
library(tidyterra)
library(ggplot2)
library(terra)
library(sf)

set.seed(131)
m = rbind(c(0,0), c(1,0), c(1,1), c(0,1), c(0,0))
p = st_polygon(list(m))
n = 100
l = vector("list", n)
for (i in 1:n)
  l[[i]] = p + 10 * runif(2)
s = st_sfc(l)
sf <- st_sf(s)

# read data from file
r <- terra::rast(here::here("data/demo_data.nc"))
```

Geospatial data procesing in `R` follows the standard raster vs. vector data model dichotomy as seen throughout most geographic information systems (GIS, @fig-datatypes). Here, raster data represent continuous data on a (fixed) grid, while vector data describe geographic features using points, lines and polygons (shapes). The main difference between both types is their sensitivity to resolution, which is predetermined in the case of raster data, while undetermined using a vector topology. 

This difference in data models used also determines the advantages and disadvantages of both models, where vector topology is resolution independent data efficient in storage the handling if mathematical (topological) operations can be computationally expensive. On the other hand raster data has a fixed lower limit to its resolution, while often being computationally efficient when modelling.

```{r}
#| label: fig-datatypes
#| fig-cap: "Image by [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Raster_vector_tikz.svg)"
#| fig-align: "left"
#| out-width: "100%"
#| echo: FALSE
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/c/c2/Raster_vector_tikz.svg")
```

Most geospatial data handling in `R` happens using lower level GDAL/OGR bindings. The [GDAL/OGR library](https://gdal.org/) is an open source framework for geospatial processing and is used across programming languages and geospatial frameworks or GIS systems (e.g. QGIS).

## The R geospatial ecosystem

Spatio-temporal data often comes in the form of dense arrays, with space and time being array dimensions. Examples include socio-economic or demographic data, series of satellite images with multiple spectral bands, spatial simulations, and climate or weather model output.

A number of libraries (packages) make the use of this spatio-temporal data, and geo-computational work in R easy. However, the ecosystem has grown rapidly and therefore is continuously shifting. Unlike other processing environments this makes it at times hard to keep track of what or when to use a particular package.

Here, I give a quick overview of the basic functionality and their uses cases of all these packages, finally there will be a brief overview of some basic geospatial operations using `terra` and `sf` libraries (see @sec-terra and @sec-sf). For a more extensive, deep dive, of all these packages I refer to Nowasad et al. (FIX REF).

### The terra package {#sec-terra}

The `terra` package is the successor of the older `raster` package, but with a simpler interface. This package deals with both geographic raster and vector data, with the explicit requirement that raster data represent spatially continuous processes on a fixed (rectangular) grid.

#### Reading and inspecting data {.unnumbered}
```{r eval = FALSE}
# load the library
library(terra)

# read data from file
r <- terra::rast("demo_data.nc")

# the demo data can be downloaded from
# https://github.com/geco-bern/handfull_of_pixels/raw/main/data/demo_data.nc

```

We can inspect the meta data by calling the object:

```{r}
print(r)
```

Or you can visualize the data by plotting the data (e.g. using `plot()`). Note that @fig-demo-nc is generated using the `ggplot2` library and is a bit more pleasing to the eye than the default `plot()` routine. You can also plot an interactive map using the `terra` function `plet()`, allowing you to scroll and zoom, while including various background tiles (such as satellite imagery, or topographic data).

```{r message=FALSE, warning=FALSE}
#| code-fold: true
#| label: fig-demo-nc
#| fig-cap: "Temperature in Kelvin (K)"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  tidyterra::geom_spatraster(data = r) +
  scale_fill_viridis_c(
    na.value = NA,
    name = "Temperature (K) \n"
    ) +
  theme_bw() +
  theme(
    legend.position = "bottom"
    )
```

```{r}
#| code-fold: true
#| label: fig-demo-dyn
#| fig-cap: "Dynamic output of the generated map"
#| fig-align: "center"
#| out-width: "100%"
library(leaflet)

# set te colour scale manually
pal <- colorNumeric(
  "magma",
  values(r),
  na.color = "transparent"
  )

# build the leaflet map
# using ESRI tile servers
# and the loaded demo raster
leaflet() |> 
  addProviderTiles(providers$Esri.WorldImagery, group = "World Imagery") |>
  addProviderTiles(providers$Esri.WorldTopoMap, group = "World Topo") |>
  addRasterImage(
    r,
    colors = pal,
    opacity = 0.8,
    group = "raster"
    ) |>
  addLayersControl(
    baseGroups = c("World Imagery","World Topo"),
    position = "topleft",
    options = layersControlOptions(collapsed = FALSE),
    overlayGroups = c("raster")
    ) |>
  addLegend(
    pal = pal,
    values = values(r),
    title = "2m Temp (K)")
```

::: callout-note
# Note

Dynamic maps with the `plet()` function in `terra` leverages the `leaflet` library and [Leafletjs javascript framework](https://rstudio.github.io/leaflet/). To use dynamic maps straight from `terra` you require the latest `leaflet` package version. You can install this version by using `remotes::install_github('rstudio/leaflet')`. The above example shows a more advanced example, using two base maps and the demo raster data.
:::

Dedicated functions exist to extract the layer names (`names()`) and the time stamps (`time()`) if there is a time component to the data. These functions allow you to extract these data and use them in analysis.

```{r}
time(r)
names(r)
```

#### Basic math {.unnumbered}

Basic math or logical operations can be performed on maps using standard `R` notations. As shown above the data contains temperature data in Kelvin. You can convert this data from Kelvin to Celsius by subtracting 273.15 from all values in the data set.
```{r}
# conversion from Kelvin to C
r_c <- r - 273.15
```

```{r message=FALSE, warning=FALSE}
#| code-fold: true
#| label: fig-demo-nc-C
#| fig-cap: "Temperature in Celsius (C) as calculated from the original values in Kelvin"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  tidyterra::geom_spatraster(data = r_c) +
  scale_fill_viridis_c(
    na.value = NA,
    name = "Temperature (C) \n"
    ) +
  theme_bw() +
  theme(
    legend.position = "bottom"
    )
```

Logical operations work in the same way. You can create a mask of temperatures above 5C using a simple logical operation.
```{r}
# all locations above freezing
# as a binary mask
m <- r_c > 5
```

```{r message=FALSE, warning=FALSE}
#| code-fold: true
#| label: fig-demo-mask
#| fig-cap: "Mask, defining positions where temperatures are larger than 5C"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  tidyterra::geom_spatraster(data = m) +
  scale_fill_viridis_d(
    name = "Mask \n"
    ) +
  theme_bw() +
  theme(
    legend.position = "bottom"
    )
```

You can exclude locations from calculations using masks. This is useful to restrict the region of interest of an analysis or limit edge cases of complex calculations beforehand. As an example you can mask out all values where the binary mask as generated above if `FALSE` (i.e. temperatures lower than 5C).
```{r}
# all locations above freezing
# as a binary mask
r_m <- terra::mask(r_c, m, maskvalue = FALSE)
```

```{r message=FALSE, warning=FALSE}
#| code-fold: true
#| label: fig-demo-mask-C
#| fig-cap: "Temperature in Celsius (C), with values lower than 5 C masked."
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  tidyterra::geom_spatraster(data = r_m) +
  scale_fill_viridis_c(
    na.value = NA,
    name = "Temperature (C) \n"
    ) +
  theme_bw() +
  theme(
    legend.position = "bottom"
    )
```

#### Writing and exporting data {.unnumbered}

The `terra` library uses pointers when referencing to data (in memory). This means that you can not save the object itself to resume your work later on. Saving the above masked map `r_m` using `saveRDS(r_m, "data.rds")` will only save a pointer to a memory space which will not exist when opening a new session. This is in contrast to for example operations on tabulated data (e.g. JSON, CSV files). As such, you need to save the output of your analysis using a formal geospatial data format using `writeRaster()`. 

To save masked temperature data in Celsius you would use:

```{r eval=FALSE}
# save data to file
terra::writeRaster(r_m, "celsius_data_masked.tif")
```

Alternatively, but for small datasets only, you could convert the geospatial data to a long oriented data frame and save the data using standard methods to save tabulated data. However, you might loose critical meta-data on geographic projections etc. Using this method to save your work is not recommended unless you keep track of all ancillary meta-data separately.

```{r}
# convert geospatial data to a
# data frame notation, where the flag
# xy = TRUE denotes that pixel coordinate
# details should be exported as well
df <- as.data.frame(r, xy = TRUE)
head(df)
```

### The sf package {#sec-sf}

[Simple features](https://en.wikipedia.org/wiki/Simple_Features) are an open standard to store and access geographic data. The `sf` [package](https://r-spatial.github.io/sf/) provides a way to represent geospatial vector data as simple features in R. This results in nested data.frames or tibbles which adhere to the "tidy" data paradigm as previously described. They therefore are long oriented and support piped workflows on geometries. This standard reduces complexity and keeps geometry operations simple.

#### Reading and inspecting data {.unnumbered}

A lot of GIS vector data comes as [shapefiles](https://en.wikipedia.org/wiki/Shapefile) (.shp extention). An example shapefile is include in the `sf` package, and we can read it using:

```{r warning=FALSE, message=FALSE}
# load library
library(sf)

# load included shapefile
nc <- sf::st_read(system.file("shape/nc.shp", package="sf"))
```

When printing the object you will be provided with an overview, when plotting the spatial data (using `plot()`) will be visualized (similar to the raster data above).

```{r warning=FALSE}
print(nc)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
#| label: fig-shapefile
#| fig-cap: "Various layers of the shapefile"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

plot(nc)
```

You can extract basic information such as the overall bounding box of the vector data using `st_bbox()`.

```{r}
st_bbox(nc)
```

The `sf` framework uses a tidy data approach, as such you can use operations upon the list items stored within the larger data set by calculating the bounding box for each geometry.
```{r}
nc |> 
    mutate(
        bbox = purrr::map(geometry, sf::st_bbox)
    )
```

#### Basic vector operations {.unnumbered}

Given the tidy data approach of `sf` data we can use the same logic to filter data. For example, if we only want to retain Camden county from the data set we can use the `filter()` function as shown in @sec-tidy-data.

```{r}
# subset data using the
# tidy filter approach
camden <- nc |>
    filter(
        NAME == "Camden"
    )
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
#| label: fig-camden
#| fig-cap: "Camden county, as filtered from the larger dataset"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

plot(camden)
```

Most common operators vector data are area based logical operations. Such as taking the intersection or union of features, which only retains the outline of a polygon. These operations can be executed on features themselves or should be mediated by geometric binary operations which can be used for filtering the original data.

For example the `st_intersection()` function calculates where and how frequent simple features overlap (interesect) with each other.The results are again a tidy simple feature which can be sorted or filtered using the standard `filter()` function.

```{r}
# calculate the intersection of all simple features
i <- sf::st_intersection(sf)

# filter out polygons with more than 1 overlap
# with 1 being a self overlap
i <- i |>
    filter(
        n.overlaps > 1
    )
```

```{r}
#| code-fold: true
#| label: fig-intersection-nc
#| fig-cap: "A plot with all intetersecting vectors polygons, with the number of overlaps as a coloured index"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  geom_sf(
    data = i["n.overlaps"],
    aes(
      fill = n.overlaps
    )
  ) +
  scale_fill_viridis_c(
    name = "overlaps"
  ) +
  theme_bw()
```

Other functions allow you to group overlapping features. For example grouping of intersecting features can be done using `st_union()`, which only returns the outermost boundary of a feature.

```{r}
u <- st_union(i)
```

```{r}
#| code-fold: true
#| label: fig-union-nc
#| fig-cap: "A plot with all intetersecting vectors polygons aggregated by st_union()"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  geom_sf(
    data = u
  ) +
  theme_bw()
```


The `sf` package is relatively complex and for a full understanding of its components I refer to the [package documentation](https://r-spatial.github.io/sf/) and the book by Nowasad et al (REFERENCE).

#### Writing and exporting data {.unnumbered}

Although read in `sf` objects can be saved as internal `R` formats such as `rds` files using `saveRDS()`, for portability between various GIS software packages `sf` can write out a new shapefile using `st_write()`.

```{r eval = FALSE}
# write the north carolina data
# to shapefile in the tempdir()
sf::st_write(nc, file.path(tempdir(), "nc.shp"))
```

### The stars package

The [`stars` package](https://r-spatial.github.io/stars/) is another geospatial `R` package you should take note of. In contrast to `terra` it is specifically tailored to dealing with data cubes. These are arrays of data on which on axis is time.

```{r}
#| label: fig-datacube
#| fig-cap: "Data cube representation by [Edzer Pebesma](https://r-spatial.github.io/stars/)"
#| fig-align: "left"
#| out-width: "100%"
#| echo: FALSE
knitr::include_graphics("https://raw.githubusercontent.com/r-spatial/stars/master/images/cube1.png")
```

Unlike the `terra` package the grid should not regular, but can of a different sort such as curvi-linear data (@fig-datagrid).

```{r}
#| label: fig-datagrid
#| fig-cap: "Various non-regular grid representations of raster data, handled by the {stars} package. Image by [Edzer Pebesma](https://r-spatial.github.io/stars/)"
#| fig-align: "left"
#| out-width: "100%"
#| echo: FALSE
knitr::include_graphics("https://r-spatial.github.io/stars/reference/figures/README-plot2-1.png")
```

Some other distinctions should be made, such as the fast summarizing of raster data to vector features. However, in most cases, it is best to explore functions within the `terra` package first before considering `stars`.

### Other noteworthy packages

Other critical packages are [`ncdf4`](https://cirrus.ucsd.edu/~pierce/ncdf/) which will be installed by default, but the included functions allow for the manipulation (reading and writing) of the common netCDF format. The [`rstac` package](https://brazil-data-cube.github.io/rstac/) which provides a convenient way to browse Spatio-Temporal Asset Catalogues (STAC), a format commonly used to organize remote sensing images online. The [`sits` package](https://e-sensing.github.io/sitsbook/) which was created by the Brazilian National Institute for Space Research (INPE) and provides tools for machine learning on data cubes.

Note that R (or python for that matter) is infinitely flexible, and many packages exist to address niche problems within the context of geospatial data manipulation. Although often tapping into the power of the GDAL framework these packages are very powerful in their own right but outside the scope of this basic introduction. For more in depth discussion I refer to the list of resources at the end of this book.
