# Geospatial data in R {#sec-geospatial}

```{r include = FALSE}
library(dplyr)
library(tidyterra)
library(ggplot2)
library(terra)
library(sf)
set.seed(131)

# read data from file
r <- terra::rast(here::here("data/demo_data.nc"))
```

Geospatial data procesing in `R` follows the standard raster vs. vector data model dichotomy as seen throughout most geographic information systems (GIS, @fig-datatypes). Here, raster data represent data on a regular grid, while vector data describe geographic features using points, lines and polygons (shapes). The main difference between both types is their sensitivity to resolution, which is predetermined in the case of raster data, while undetermined using a vector topology. 

This difference in data models used also determines the advantages and disadvantages of both models. Vector topology is resolution-independent and data efficient in storage. However, the handling of mathematical (topological) operations can be computationally expensive. In contrast, raster data has a fixed resolution and enables computationally efficient data transformations.

```{r}
#| label: fig-datatypes
#| fig-cap: "Image by [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Raster_vector_tikz.svg)"
#| fig-align: "left"
#| out-width: "100%"
#| echo: FALSE
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/c/c2/Raster_vector_tikz.svg")
```

Most geospatial data handling in `R` happens uses lower-level GDAL/OGR bindings. The [GDAL/OGR library](https://gdal.org/) library is an open source framework for geospatial processing and is used across programming languages and geospatial frameworks or GIS systems (e.g. QGIS).

## The R geospatial ecosystem

Spatio-temporal data often comes in the form of arrays, with space and time being array dimensions. Examples include socio-economic or demographic data, series of satellite images with multiple spectral bands, spatial simulations, and climate or weather model output.

A number of libraries (packages) make the use of this spatio-temporal data, and geo-computational work in R easy. However, the ecosystem has grown rapidly and is therefore continuously shifting. Unlike other processing environments, this makes it at times hard to keep track of what or when to use a particular package.

Here, a quick overview of the basic functionality and uses cases of different geospatial R packages is provided, and a brief overview of some basic geospatial operations using `terra` and `sf` libraries is given (see @sec-terra and @sec-sf). For a more deeper dive into these packages, see @lovelace_geocomputation_2019 .

### The terra package {#sec-terra}

The `terra` package is the successor of the older `raster` package and provides a simpler interface. This package deals with both geographic raster and vector data, with the explicit requirement that raster data represent spatially continuous processes on a fixed (rectangular) grid.

#### Reading and inspecting data {.unnumbered}
```{r eval = FALSE}
# load the library
library(terra)

# read data from file
r <- terra::rast("demo_data.nc")

# the demo data can be downloaded from
# https://github.com/geco-bern/handfull_of_pixels/raw/main/data/demo_data.nc

```

We can inspect the meta data by calling the object:

```{r}
print(r)
```

Or you can visualize the data by plotting the data (e.g., using `plot()`). Note that @fig-demo-nc is generated using the `ggplot2` library and is a bit more pleasing to the eye than the default `plot()` routine. You can also plot an interactive map using the `terra` function `plet()`, allowing you to scroll and zoom, while including various background tiles (such as satellite imagery, or topographic data).

```{r message=FALSE, warning=FALSE}
#| code-fold: true
#| label: fig-demo-nc
#| fig-cap: "Temperature in Kelvin (K)"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  tidyterra::geom_spatraster(data = r) +
  scale_fill_viridis_c(
    na.value = NA,
    name = "2 m temp. (K) \n"
    ) +
  theme_bw() +
  theme(
    legend.position = "bottom"
    )
```

```{r}
#| code-fold: true
#| label: fig-demo-dyn
#| fig-cap: "Dynamic output of the generated map"
#| fig-align: "center"
#| out-width: "100%"
library(leaflet)

# set te colour scale manually
pal <- colorNumeric(
  "magma",
  values(r),
  na.color = "transparent"
  )

# build the leaflet map
# using ESRI tile servers
# and the loaded demo raster
leaflet() |> 
  addProviderTiles(providers$Esri.WorldImagery, group = "World Imagery") |>
  addProviderTiles(providers$Esri.WorldTopoMap, group = "World Topo") |>
  addRasterImage(
    r,
    colors = pal,
    opacity = 0.8,
    group = "raster"
    ) |>
  addLayersControl(
    baseGroups = c("World Imagery","World Topo"),
    position = "topleft",
    options = layersControlOptions(collapsed = FALSE),
    overlayGroups = c("raster")
    ) |>
  addLegend(
    pal = pal,
    values = values(r),
    title = "2 m temp. (K)")
```

::: callout-note
# Note

Dynamic maps with the `plet()` function in `terra` leverages the `leaflet` library and [Leafletjs javascript framework](https://rstudio.github.io/leaflet/). To use dynamic maps straight from `terra` you require the latest `leaflet` package version. You can install this version by using `remotes::install_github('rstudio/leaflet')`. The above example shows a more advanced example, using two base maps and the demo raster data.
:::

Dedicated functions exist to extract the layer names (`names()`) and the time stamps (`time()`) if there is a time component to the data. These functions allow you to extract these data and use them in analyses.

```{r}
time(r)
names(r)
```

#### Basic math {.unnumbered}

Basic math or logical operations can be performed on maps using standard `R` notations. As shown above the data contains temperature data in Kelvin. You can convert this data from Kelvin to Celsius by subtracting 273.15 from all values in the data set.
```{r}
# conversion from Kelvin to C
r_c <- r - 273.15
```

```{r message=FALSE, warning=FALSE}
#| code-fold: true
#| label: fig-demo-nc-C
#| fig-cap: "Temperature in Celsius (C) as calculated from the original values in Kelvin"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  tidyterra::geom_spatraster(data = r_c) +
  scale_fill_viridis_c(
    na.value = NA,
    name = "2 m temp. (deg. C) \n"
    ) +
  theme_bw() +
  theme(
    legend.position = "bottom"
    )
```

Logical operations work in the same way. You can create a mask of temperatures above 5$^\circ$C using a simple logical operation.
```{r}
# all locations above freezing
# as a binary mask
m <- r_c > 5
```

```{r message=FALSE, warning=FALSE}
#| code-fold: true
#| label: fig-demo-mask
#| fig-cap: "Mask, defining positions where temperatures are larger than 5C"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  tidyterra::geom_spatraster(data = m) +
  scale_fill_viridis_d(
    name = "Mask \n"
    ) +
  theme_bw() +
  theme(
    legend.position = "bottom"
    )
```

You can exclude locations from calculations using masks. This is useful to restrict the region of interest of an analysis or limit edge cases of complex calculations beforehand. As an example you can mask out all values where the binary mask as generated above is `FALSE` (i.e. temperatures lower than 5$^\circ$C).
```{r}
# all locations above freezing
# as a binary mask
r_m <- terra::mask(r_c, m, maskvalue = FALSE)
```

```{r message=FALSE, warning=FALSE}
#| code-fold: true
#| label: fig-demo-mask-C
#| fig-cap: "Temperature in Celsius (C), with values lower than 5 degrees C masked."
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  tidyterra::geom_spatraster(data = r_m) +
  scale_fill_viridis_c(
    na.value = NA,
    name = "Temperature (deg. C) \n"
    ) +
  theme_bw() +
  theme(
    legend.position = "bottom"
    )
```

#### Band math (considering multiple layers) {.unnumbered}

Geospatial data often consists of multiple layers of information, either representing different properties (bands) or different times when data was acquired. The `terra` package allows you to manipulate data within and across bands.

```{r}
# create a multi-layer object
# by combining objects
# here the correcte (t2m in C)
# and the masked values (>5 C)
# are combined
multi_layer <- c(r_c, r_m)

print(multi_layer)
```

Operations on individual cells within `terra` within or across layers are governed by the `*app()` functions. Here, flavours such as `app()`, `tapp()`, and `sapp()`, are used to apply a function across all layers (to summarize data), to groups of layers (grouped summaries or complex band math), of manipulations of each individual layer with the same function (respectively).

For example, to calculate the mean between both layers in this multi-layer `terra` object one can use `app()` as such:

```{r}
# apply the mean() function across
# all layers of the multi_layer object
multi_layer_mean <- terra::app(multi_layer, mean, na.rm = TRUE)
```

```{r message=FALSE, warning=FALSE}
#| code-fold: true
#| label: fig-demo-layer-mean
#| fig-cap: "Temperature in Celsius (C) across layers"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  tidyterra::geom_spatraster(data = multi_layer_mean) +
  scale_fill_viridis_c(
    na.value = NA,
    name = "Temperature (deg. C) \n"
    ) +
  theme_bw() +
  theme(
    legend.position = "bottom"
    )
```

In the above analysis the mean across all layers is calculated, while passing an additional argument to the `mean()` function (i.e. na.rm = TRUE), stating that NA values should be ignored. Doing so fills in the values masked out previously and yields the original `r_c` object (i.e. the temperature values in $^\circ$C).

#### Writing and exporting data {.unnumbered}

The `terra` library uses pointers when referencing to data (in memory). This means that you can not save the object itself to resume your work later on. Saving the above masked map `r_m` using `saveRDS(r_m, "data.rds")` will only save a pointer to a memory space which will not exist when opening a new session. This is in contrast to for example operations on tabulated data (e.g. JSON, CSV files). As such, you need to save the output of your analysis using a formal geospatial data format using `writeRaster()`. 

To save masked temperature data in Celsius you would use:

```{r eval=FALSE}
# save data to file
terra::writeRaster(r_m, "celsius_data_masked.tif")
```

Alternatively, but for small datasets only, you could convert the geospatial data to a long oriented data frame and save the data using standard methods to save tabulated data. However, you might loose critical meta-data on geographic projections etc. Using this method to save your work is not recommended unless you keep track of all ancillary meta-data separately.

```{r}
# convert geospatial data to a
# data frame notation, where the flag
# xy = TRUE denotes that pixel coordinate
# details should be exported as well
df <- as.data.frame(r, xy = TRUE)
head(df)
```

### The sf package {#sec-sf}

[Simple features](https://en.wikipedia.org/wiki/Simple_Features) are an open standard to store and access geographic data. The `sf` [package](https://r-spatial.github.io/sf/) provides a way to represent geospatial vector data as simple features in R. This results in nested data.frames or tibbles which adhere to the "tidy" data paradigm as previously described. They therefore are long oriented and support piped workflows on geometries. This standard reduces complexity and keeps geometry operations simple.

#### Reading and inspecting data {.unnumbered}

A lot of GIS vector data comes as [shapefiles](https://en.wikipedia.org/wiki/Shapefile) (.shp extention). An example shapefile is included in the `sf` package, and we can read it using:

```{r warning=FALSE, message=FALSE}
# load library
library(sf)

# load included shapefile
nc <- sf::st_read(system.file("shape/nc.shp", package = "sf"))
```

When printing the object you will be provided with an overview, when plotting the spatial data (using `plot()`) will be visualized (similar to the raster data above).

```{r warning=FALSE}
print(nc)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
#| label: fig-shapefile
#| fig-cap: "Various layers of the shapefile"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

plot(nc)
```

You can extract basic information such as the overall bounding box of the vector data using `st_bbox()`.

```{r}
st_bbox(nc)
```

The `sf` framework uses a tidy data approach. you can use operations on the list items stored within the larger data set by calculating the bounding box for each geometry.
```{r}
nc |> 
    mutate(
        bbox = purrr::map(geometry, sf::st_bbox)
    )
```

#### Basic vector operations {.unnumbered}

Given the tidy data approach of `sf` data we can use the same logic to filter data. For example, if we only want to retain Camden county from the data set we can use the `filter()` function as shown in @sec-tidy-data.

```{r}
# subset data using the
# tidy filter approach
camden <- nc |>
    filter(
        NAME == "Camden"
    )
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
#| label: fig-camden
#| fig-cap: "Camden county, as filtered from the larger dataset"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

plot(camden)
```

Most common operations on vector data are area-based logical operations. Examples include taking the intersection or union of features, which only retains the outline of a polygon. These operations can be executed on features themselves or should be mediated by geometric binary operations which can be used for filtering the original data.

For example the `st_intersection()` function calculates where and how frequent simple features overlap (interesect) with each other. The results are again a tidy simple feature which can be sorted or filtered using the standard `filter()` function.

```{r}
# generate random boxes
m <- rbind(c(0,0), c(1,0), c(1,1), c(0,1), c(0,0))
p <- st_polygon(list(m))
n <- 100
l <- vector("list", n)
for (i in 1:n)
  l[[i]] = p + 10 * runif(2)
s <- st_sfc(l)
sf <- st_sf(s)

# calculate the intersection of all simple features
i <- sf::st_intersection(sf)

# filter out polygons with more than 1 overlap
# with 1 being a self overlap
i <- i |>
    filter(
        n.overlaps > 1
    )
```

```{r}
#| code-fold: true
#| label: fig-intersection-nc
#| fig-cap: "A plot with all intetersecting vectors polygons, with the number of overlaps as a coloured index"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  geom_sf(
    data = i["n.overlaps"],
    aes(
      fill = n.overlaps
    )
  ) +
  scale_fill_viridis_c(
    name = "overlaps"
  ) +
  theme_bw()
```

Other functions allow you to group overlapping features. For example, grouping of intersecting features can be done using `st_union()`, which only returns the outermost boundary of a feature.

```{r}
u <- st_union(i)
```

```{r}
#| code-fold: true
#| label: fig-union-nc
#| fig-cap: "A plot with all intetersecting vectors polygons aggregated by st_union()"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 4

ggplot() +
  geom_sf(
    data = u
  ) +
  theme_bw()
```


The `sf` package is relatively complex and for a full understanding of its components I refer to the [package documentation](https://r-spatial.github.io/sf/) and the book by @lovelace_geocomputation_2019.

#### Writing and exporting data {.unnumbered}

Although read in `sf` objects can be saved as internal `R` formats such as `rds` files using `saveRDS()`, for portability between various GIS software packages `sf` can write out a new shapefile using `st_write()`.

```{r eval = FALSE}
# write the north carolina data
# to shapefile in the tempdir()
sf::st_write(nc, file.path(tempdir(), "nc.shp"))
```

### The stars package

The [`stars` package](https://r-spatial.github.io/stars/) is another geospatial `R` package you should take note of. In contrast to `terra`, it is specifically tailored to dealing with data cubes. These are arrays of data on which one axis is time.

```{r}
#| label: fig-datacube
#| fig-cap: "Data cube representation by [Edzer Pebesma](https://r-spatial.github.io/stars/)"
#| fig-align: "left"
#| out-width: "100%"
#| echo: FALSE
knitr::include_graphics("https://raw.githubusercontent.com/r-spatial/stars/master/images/cube1.png")
```

Unlike for the `terra` package, grids in the `stars` package don't have to be regular and rectangular, but can be of a different sort, such as curvi-linear data (@fig-datagrid).

```{r}
#| label: fig-datagrid
#| fig-cap: "Various non-regular grid representations of raster data, handled by the {stars} package. Image by [Edzer Pebesma](https://r-spatial.github.io/stars/)"
#| fig-align: "left"
#| out-width: "100%"
#| echo: FALSE
knitr::include_graphics("https://r-spatial.github.io/stars/reference/figures/README-plot2-1.png")
```

Some other distinctions should be made, such as the fast summarizing of raster data to vector features. However, in most cases, it is best to explore functions within the `terra` package first before considering `stars`.

### Other noteworthy packages

Other critical packages are [`ncdf4`](https://cirrus.ucsd.edu/~pierce/ncdf/) which will be installed by default, but the included functions allow for the manipulation (reading and writing) of the common netCDF format. The [`rstac` package](https://brazil-data-cube.github.io/rstac/) which provides a convenient way to browse Spatio-Temporal Asset Catalogues (STAC), a format commonly used to organize remote sensing images online. The [`sits` package](https://e-sensing.github.io/sitsbook/) was created by the Brazilian National Institute for Space Research (INPE) and provides tools for machine learning on data cubes.

Note that R (or Python) is infinitely flexible, and many packages exist to address niche problems within the context of geospatial data manipulation. While they most often tap into the the GDAL framework, these packages can be very powerful for certain applications which are outside the scope of this basic introduction. For more in-depth discussion, the reader is referred to the list of resources at the end of this book.
