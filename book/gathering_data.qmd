# Accessing data {#sec-accessing-data}

There is a wealth of data available within the context of geography and environmental sciences. However, how to efficiently access these data sources is a skill in its own right. 

In general you have to know:

- where to find data, what are common sources
- how to download the data
- how to read in the data once downloaded
- how to interpret the data (domain knowledge), and verify content

## Finding open trustworthy data {#sec-trust-data}

Environmental data can be found in a number of locations and a general web search will point you to them if you know the name of a data set you have in mind. If you want to broaden your search into new data products, the best sources are often **governmental organizations**. Governments collect and sponsor data acquisition and retention, with various administrations focusing on on particular topics, e.g. remote sensing, forestry, population statistics. For example, if you are looking for satellite remote sensing data, it makes sense to look at for example the **[European Space Agency (ESA)](https://www.esa.int/)** data repositories or the United States **[National Aeronautics and Space Administration (NASA)](https://www.nasa.gov/)**. If you are looking for spatially explicit **population statistics [Eurostat](https://ec.europa.eu/eurostat)** might be a good starting place to start your search. Most states keep inventories of their protected areas as well as detailed forest inventories. Similarly, weather agencies on a state or European level can provide wealth of data. Directing your searches toward state level agencies will land you with reliable sources of data.

Similarly, non-governmental organizations (NGOs), foundations and other non-profit organizations can be a valuable source of information. General street layouts, and **location based information on businesses and other venues** can be sourced from **Open Street Map (OSM)**. The World Wildlife Fund (WWF) has contributed to biodiversity mapping initiatives. In general, non-profits and NGOs are trustworthy but you should verify if sources are commonly used within the scientific literature.

The scientific literature can also be a valuable source of data products. However, finding these data products is often difficult as they are not necessarily centralized in a structured way or they might not even be shared publicly. Centralized repositories do exist. Noteworthy are **Zenodo**, a data repository for research data supported by CERN but holding vast stores of data on a variety of research topics. Similarly **Dryad and Figshare** provide long term storage of published research data.

Below you find a list of useful data sources:

- [ECMWFR Copernicus Data Services](https://cds.climate.copernicus.eu) (climate data)
- [Copernicus Open Access Hub](https://scihub.copernicus.eu/) (access to the Sentinel remote sensing data)
- [EOSDIS Digital Active Archive Centers](https://www.earthdata.nasa.gov/eosdis/daacs)  (DAACs)
- [Integrated Carbon Observation System](https://www.icos-cp.eu/about/icos-in-nutshell) (ICOS)
- [National Ecosystem Observation Network](https://www.neonscience.org/) (NEON)
- Scientific data repositories (open data downloads or deposits)
  - [Zenodo.org](https://zenodo.org)
  - [Dryad](https://datadryad.org/stash) 
  - [Figshare](https://figshare.com/) 

This list is not extensive and many other sources exist. However, I will source mostly from these data sources in the book. Some familiarity with the names of these data sources is therefore helpful. Depending on where data is hosted you can simply download data through your web browser or use the internal `download.file()` function to grab data to store it locally.

```{r}
# Downloading a time series of mean CO2 levels from NOAA
# and storing it in a temporary file
url <- "https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.csv"

download.file(
  url,
  file.path(tempdir(), "co2.csv")
)
```

## Gathering a handfull of pixels

The sections above (@sec-trust-data) assume that you download data locally, on disk in a particular format. However, many of the data sources described in previous section are warehoused in large cloud facilities. These services allow you to access the underlying (original) data using an Application Programming Interfaces (APIs), hence programmatically, using code. Mastering the use of these services has become key in gathering research data sets. Given the scope of this book I will focus on ways to gather small approximately analysis ready geospatial datasets using APIs. 

### Direct downloads

Before diving into a description of APIs, I remind you that some file reading functions in `R` are "web-enabled", and can not only read local files but also remote ones (i.e., URLs). Getting ahead of ourselves a bit, the example code shows you how to read the content of a URL (of CO~2~ data) directly into your `R` environment.

Although using this functionality isn't equivalent to using an API, the concept is the same. I.e., you load a remote data source (albeit without further subsetting or passing of any parameters).

```{r eval=FALSE}
# read in the data directly from URL
df <- read.table(
  url,
  header = TRUE,
  sep = ","
)
```

### APIs {#sec-apis}

Web-based Application Programming Interfaces (APIs) offer a way to specify the scope of the returned data, and ultimately, the processing which goes on behind the scene in response to a (data) query. APIs are a way to, in a limited way, control a remote server to execute a certain (data) action. In most (RESTful) APIs, such query takes the form of an HTTP URL via an URL-encoded scheme using an API endpoint (or base URL).

To reduce some of the complexity of APIs, it is common that a wrapper is written around an API in the language of choice (e.g., R, Python). These dedicated API libraries make it easier to access data and limit coding overhead.

##### Dedicated API libraries {.unnumbered}

As an example of a dedicated library, we use the [{MODISTools} R package](https://github.com/bluegreen-labs/MODISTools) which queries remote sensing data generated by the MODIS remote sensing (satellite) mission from the [Oak Ridge National Laboratories data archive](https://modis.ornl.gov/data.html). A full description of their API is [provided online](https://modis.ornl.gov/data/modis_webservice.html).

```{r warning=FALSE, message=FALSE}
# load the library
library("MODISTools")

# list all available products
# (only showing first part of the table for brevity)
MODISTools::mt_products() |> 
  head()

# list bands for the MOD11A2
# product (a land surface temperature product)
MODISTools::mt_bands("MOD11A2") |> 
  head()
```

Using this information we can now formulate a full query for use with the API. Here, I download a demo dataset specifying a location, a product, a band (subset of the product) and a date range and a geographic area (1 km above/below and left/right). Data is returned internally to the variable `subset`, and the progress bar of the download is not shown.

```{r include=FALSE}
# load MODIS subset demo data for quick rendering
subset <- readRDS(here::here("data/modis_demo_data.rds"))
```

```{r eval = FALSE}
# Download some data
subset <- MODISTools::mt_subset(
  product = "MOD11A2",
  lat = 40,
  lon = -110,
  band = "LST_Day_1km",
  start = "2004-01-01",
  end = "2004-02-01",
  km_lr = 1,
  km_ab = 1,
  internal = TRUE,
  progress = FALSE
)

# print the dowloaded data
head(subset)
```

```{r echo=FALSE}
# print the dowloaded data
head(subset)
```

A detailed description of all functions of the `MODISTools` R package is beyond the scope of this course. However, the listed commands show you what a dedicated API package does. It is a shortcut to [functional elements of an API](https://modis.ornl.gov/data/modis_webservice.html). For example `mt_products()` allows you to quickly list all products without any knowledge of an API URL. Although more complex, the `mt_subset()` routine allows you to query remote sensing data for a single location (specified with a latitude `lat` and longitude `lon`), and a given date range (e.g., start, end parameters), a physical extent (in km left-right and above-below).

##### GET {.unnumbered}

Depending on your data source, you will either need to rely on a dedicated R package to query the API or study the API documentation. The general scheme for using an API follows the use of the `GET()` command of the {httr} R library. 

You define a query using API parameters, as a named list.

```{r eval=FALSE}
# formulate a named list query to pass to httr
query <- list(
  "argument" = "2",
  "another_argument" = "3"
)
```

You define the endpoint (`url`) where you want to query your data from.

```{r eval=FALSE}
# The URL of the API (varies per product / param)
url <- "https://your.service.endpoint.com"
```

Finally, you combine both in a `GET()` statement to download the data from the endpoint (`url`).

```{r eval=FALSE}
# the write_disk() function captures
# data if available and writes it to
# disk at location "path"
status <- httr::GET(
  url = url,
  query = query,
  httr::write_disk(
    path = "/where/to/store/data/filename.ext",
    overwrite = TRUE
  )
)
```

Below, we provide an example of using the `GET` command to download data from the [Regridded Harmonized World Soil Database (v1.2)](https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1247) as hosted on the Oak Ridge National Laboratory computer infrastructure. In this case we download a subset of a global map of topsoil sand content (`T_SAND`, %), or other paramters as listed on the [ORNL DAAC data portal](https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1247).

```{r eval = TRUE, warning=FALSE, message=FALSE}
# set API URL endpoint
# for the total sand content
url <- "https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1247/T_SAND.nc4"

# formulate query to pass to httr
query <- list(
  "var" = "T_SAND",
  "south" = 32,
  "west" = -81,
  "east" = -80,
  "north" = 34,
  "disableProjSubset" = "on",
  "horizStride" = 1,
  "accept" = "netcdf4"
)

# download data using the
# API endpoint and query data
status <- httr::GET(
  url = url,
  query = query,
  httr::write_disk(
    path = file.path(tempdir(), "T_SAND.nc"),
    overwrite = TRUE
  )
)
```

Plotting the data downloaded shows a map of the percentage of sand in the topsoil.

```{r message=FALSE}
#| code-fold: true
#| label: fig-sand-perc
#| fig-cap: "Soil sand percentage (%)"
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5

# libraries
library(terra)
library(ggplot2)
library(tidyterra)

sand <- terra::rast(file.path(tempdir(), "T_SAND.nc"))
ggplot() +
  tidyterra::geom_spatraster(data = sand) +
  scale_fill_viridis_c(
    na.value = NA,
    name = "sand (%)"
    ) +
  theme_bw()
```

::: callout-note
## Note

Throughout the book, where possible, I will collapse the code which draws figures. This makes for a better reading experience. If you want to see the underlying code you can click on the "> Code" line to unfold the code chunk. If not code is presented a simple `plot()` function call was used.
:::

##### Authentication {.unnumbered}

Depending on the API, authentication using a user name and a key or password is required. Then, the template should be slightly altered to accommodate for these requirements. Note that instead of the `GET()` command we use `POST()` as we need to post some authentication data before we can get the data in return.

```{r eval=FALSE}
# an authenticated API query
status <- httr::POST(
  url = url,
  httr::authenticate(user, key),
  httr::add_headers("Accept" = "application/json",
                    "Content-Type" = "application/json"),
  body = query,
  encode = "json"
)
```
