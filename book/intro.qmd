# Introduction

This book uses the R statistical language in order to demonstrate how to process geo-spatial data. For many this might seem an odd choice but contrary to common believes R does provide solid geo-spatial frameworks. These frameworks provide all the tools you need for small and larger geo-spatial projects.

I will focus on a set of examples which are rather limited in scope, using just a handfull of pixels. Not only does this allow you to experiment with geo-spatial data on limited compute infrastructure, it also shows you that true science can be done with relatively modest means. In many ways this logic runs counter to the current practices either moving analysis to the cloud or trying to provide high resolution wall-to-wall (global) coverage irrespective of the added value to a study.

In addition, most validation data is provided as point scale data (through either surveys or other field work). In effect, much research always starts with considering a handfull of point scale data, before upscaling. Obviously, working at a smaller spatial scale allows you to limit data management, leverage more limited compute infrastructure and experiment with methodological approaches.

The goal of this book is therefore not to teach you a particular set of (industry) tools, but rather a balanced combination of both open source (R based) tools and a focus on conceptual development for rapid prototyping of research ideas (within R). This book is in many ways an extension of [Geocomputation with R](https://r.geocompx.org/index.html) by @lovelace_geocomputation_2019. For strict geo-computational knowledge I refer to this source. Although the book uses R the methods described are rather environment agnostic and a similar work could be created for the python or other languages.

This book **requires prior knowledge** of programming in **R**, **git** and cloud based collaborative tools such as **Github**. However, to refresh some concepts, or for those who want to skip ahead, I will repeat some basic skills at the beginning of the book in @sec-basic-r to get you started.

## Formal course work

All exercises are based upon material as discussed in the book ["A Handfull of Pixels: big science using small data"](https://khufkens.github.io/handfull_of_pixels/). The exercises are taught at Master's level and require more advanced knowledge of the R statistical environment. A brief crash course is given in @sec-basic-r . All exercise boxes in the main text are only there to make you think about the context of the processes described. You should be able to complete all exercises in @sec-exercises easily, when reading the required book chapters, external data documentation (as mentioned/linked to), or basic physical geography knowledge.

When reading this book as formal course work the requested format for handing in the exercises as listed in @sec-exercises is a single R markdown file (see @sec-rmarkdown), including its rendered html file. Different exercise chapters should be divided by the subtitles used in @sec-exercises.

I will grade on the implementation of the code, as well as its critical assessment, and or trouble shooting. If you were stuck, document how you got unstuck in these assignments. If you are still stuck, list what efforts you made to get unstuck (and what failed results they yielded and how this informed your further reasoning). 

Document problem solving extensively in between code blocks. Mention (cite) resources you consulted, packages used, etc. You will have to find the original source if you consult external resources, ChatGPT (or similar large language model chat services) are **not** a valid reference in and on itself.
