# Land-Cover classification {#sec-land-cover}

::: callout-warning
### Warning
Under development...
:::

In previous sections I've explained how seasonality in vegetation canopy density (leaf area inxed, or LAI) or phenology can be detected, and how it varies depending on the geography of the region, and its ties to the exchange of carbon (C) and water between the biosphere and the atmosphere (@sec-phenology-trends). A small, first principles, example was provided on how to write your own phenology detection algorithm (@sec-algorithms).

However, many satellite platforms provide information in various spectral bands (and not only one product in the temporal domain). One can say the data are multi-dimensional, having both a temporal, spatial and spectral component (see @fig-spectral-bands). These various bands, or locations in the spectral domain, provide key insights into the state of the land surface (throughout the year).

For example the combination of information in the red and near-infrared bands (spectral domains) provides key information to calculate the Normalized Difference Vegetation Index (NDVI) [@Huete2002]. Other band combinations and or models lead to other indices with varying properties, tailored to specific ecological, geo-morphological or other purposes [@zeng_optical_2022].

When we plot a time series of a deciduous forest you note the steady seasonal changes when switching between winter, with low LAI values, and summer with high LAI values. However, different vegetation and land-use and land-cover types have different temporal NDVI signals. For example, a glacier will have permanent snow and no seasonal LAI signal. We can therefore discriminate between non-vegetative locations and vegetation based upon the combine spectral and temporal profile of a location.

```{r include = FALSE}
library(terra)
library(dplyr)
library(ggplot2)
library(patchwork)
library(MODISTools)

# for reproducibilty and stability
# of kmeans results (and colours)
set.seed(0)

# read land cover map
lc_r <- readRDS(here::here("data/land-cover_2012.rds")) |>
  mt_to_terra(reproject = TRUE)

# lc <- readRDS(here::here("data/land-cover_2012.rds")) |>
#   mutate(
#     date = as.Date(calendar_date),
#     year = as.numeric(format(date, "%Y"))
#   ) |>
#   rename(
#     lc = "value"
#   ) |>
#   select(
#     pixel,
#     year,
#     lc
#   )
# 
# # # read in LAI
# lai <- readRDS(here::here("data/lai_2012.rds")) |>
#   mutate(
#     date = as.Date(calendar_date),
#     year = as.numeric(format(date, "%Y")),
#     value = value * as.numeric(scale),
#     value = ifelse(value >= 25, NA, value)
#   ) |>
#   rename(
#     lai = "value"
#   ) |>
#   select(
#     pixel,
#     date,
#     year,
#     lai
#   )
# 
# # merge by pixel and date
# df <- left_join(lai, lc, by = c("pixel", "year"))
# 
# # get mean and sd for dbf locations
# dbf <- df |>
#   filter(lc == 4) |>
#   group_by(date) |>
#   summarize(
#     mean = mean(lai, na.rm = TRUE),
#     sd = sd(lai, na.rm = TRUE)
#     )
# 
# saveRDS(
#   dbf,
#   here::here("data/dbf_lai_profile.rds"),
#   compress = "xz"
#   )
# 
# # get mean and sd for barren locations
# snow <- df |>
#   filter(lc == 15) |>
#   group_by(date) |>
#   summarize(
#     mean = mean(lai, na.rm = TRUE),
#     sd = sd(lai, na.rm = TRUE)
#     )
# 
# saveRDS(
#   snow,
#   here::here("data/snow_lai_profile.rds"),
#   compress = "xz"
#   )

snow <- readRDS(here::here("data/snow_lai_profile.rds"))
dbf <- readRDS(here::here("data/dbf_lai_profile.rds"))
```

```{r}
#| label: fig-lai-dbf-snow
#| fig-cap: "The 8-daily mean and standard deviation of the leaf area index (LAI) over all broadleaf forests (a) and permanent snow/ice areas (b) within the Swiss LAI dataset, as previously used."
#| fig-align: "center"
#| out-width: "100%"
#| fig-height: 3
#| echo: FALSE

p <- ggplot(dbf) +
  geom_ribbon(
    aes(
      date,
      ymin = mean - sd,
      ymax = mean + sd
    ),
    fill = "grey"
  ) +
  geom_line(
    aes(
      date,
      mean
    )
  ) +
  labs(
    x = "",
    y = "LAI"
  ) +
  coord_cartesian(ylim=c(0, 7)) +
  theme_bw()

p2 <- ggplot(snow) +
  geom_ribbon(
    aes(
      date,
      ymin = mean - sd,
      ymax = mean + sd
    ),
    fill = "grey"
  ) +
  geom_line(
    aes(
      date,
      mean
    )
  ) +
    labs(
    x = "",
    y = "LAI"
  ) +
  coord_cartesian(ylim=c(0, 7)) +
  theme_bw()

p + p2 +  
  plot_layout(nrow = 1) + 
  plot_annotation(
    tag_levels = "a",
    tag_prefix = "(",
    tag_suffix = ")"
    )
```

::: callout-note
### Note

Land-Use and Land-Cover (LULC) mapping is often mentioned as one concept. However, it is important to note the difference between Land-Use and Land-Cover. Land-Use is determined by human activities, while Land-Cover is the consequence of the natural biotic and abiotic environment. Although in an age of climate change (and global rising CO^2 levels and temperatures) few places are truly free of human influence this nuance remains noteworthy. In this chapter I will focus mostly on the Land-Cover aspect (the natural environment).

:::

## Unsupervised machine learning

As such, we can use this (temporal) information across vegetation types to classify the Swiss alpine scene into locations which have little seasonality and those which have some (e.g. @fig-lai-dbf-snow). For example you can calculate the mean and standard deviation of a full year and see how much variability you see across a year. Regions with a low NDVI signal with little variability are most likely not associated with vegetation (e.g. glaciers, see @fig-lai-dbf-snow).

Classification of data in different classes (or clustering) can be accomplished using various methods. Clustering can either be unsupervised, where clustering is only defined by the number of classes one wants to divide the (multi-dimensional) dataset into.

We can use an unsupervised machine learning approach such as k-means clustering to divide the dataset into two or more classes. These classes are clustered in a way which minimizes within-cluster variances, i.e. it ensures that pixels will look similar to eachother (given a target number of clusters `k` to divide the dataset into).

```{r include = FALSE}
# load buffered data, not shown in render
r <- terra::rast(here::here("data/LAI.tiff"))
```

Here we can use the `lai_2012` dataset we previously downloaded, but we'll use the raster representation as a starting point (as most data will come in multi-layer raster formats).

```{r eval = FALSE}
# conversion from tidy data to a raster format
# as it is common to use raster data
r <- MODISTools::mt_to_terra(
  lai_2012,
  reproject = TRUE
  )
```

As a first step I will convert this raster object back into a dataframe. However, this time it will be a wide data frame, where every pixel location is a row and every column a value for a given date. Alternatively, I could have converted the original `lai_2012` data frame from a long format into a wide format using `tidyr::pivot_wider()`. Every row, representing a year for a given location, is a [feature (vector)](https://en.wikipedia.org/wiki/Feature_(machine_learning)) which contains the information on which the clustering algorithm will operate.

```{r}
# convert a multi-layer raster image
# to wide dataframe
df <- as.data.frame(r, cell = TRUE)

# the content of a single feature (vector)
# limited to the first 5 values for brevity
print(df[1,1:5])
```

We can now use the `kmeans()` algorithm to classify the data into two distinct groups or centers (k = 2). Note that we drop the first column from our dataframe as this contains the pixel indices, which are needed later on.

```{r}
# cluster the data 
clusters <- kmeans(
  df[,-1],
  centers = 2
)

# use the original raster layout as
# a template for the new map (only
# using a single layer)
kmeans_map <- terra::rast(r, nlyr=1)

# assign to each cell value (location) of this
# new map using the previously exported cell
# values (NA values are omitted so a 1:1
# mapping would not work)
kmeans_map[df$cell] <- clusters$cluster
```

```{r warning = FALSE,  message = FALSE}
#| code-fold: true
#| label: fig-kmeans-map
#| fig-cap: "k-means classification map for two clusters and one year (2012) of leaf area index (LAI) data."
#| fig-align: "center"
#| out-width: "100%"

library(leaflet)

# set te colour scale manually
palcol <- colorFactor(
  c("#78d203", "#f9ffa4"),
  domain = 1:2,
  na.color = "transparent"
  )

# build the leaflet map
leaflet() |> 
  addProviderTiles(providers$Esri.WorldImagery, group = "World Imagery") |>
  addProviderTiles(providers$Esri.WorldTopoMap, group = "World Topo") |>
  addRasterImage(
    kmeans_map,
    colors = palcol,
    opacity = 0.5,
    method = "ngb",
    group = "k-means cluster results"
    ) |>
  addLayersControl(
    baseGroups = c("World Imagery","World Topo"),
    position = "topleft",
    options = layersControlOptions(collapsed = FALSE),
    overlayGroups = c("k-means cluster results")
    ) |>
  addLegend(
    colors = palcol(1:2),
    values = c(1, 2),
    title = "cluster",
    labels = c(1, 2)
    )
```

As this is an unsupervised classification we do not know what land cover types are included in this map. The model that we generate is therefore purely data informed. However, a quick visual inspection shows that for a `k` of 2 the split between the clusters divides vegetation from glaciers, water bodies, and urban areas (@fig-kmeans-map). The (seasonal) differences in LAI were used in the k-means analysis to minimize the (seasonal) variance between pixels. In particular, our analysis with two classes separates areas with a seasonal dynamic from those without one.

Note that k-means clustering provides a clustered output only and not a model which can be scaled to other locations. The representation of k-means clustering is therefore dependent on the dataset. Furthermore, a simple index as the NDVI or LAI alone does not provide sufficient information to distinguish between more subtle land-use or land-cover classes (e.g. evergreen forests and or mixed forest types). In short, we need more data and a more sophisticated model approach to create an informed model which scales easily to different land cover types and new locations.

## supervised machine learning

In contrast to k-means, one can use supervised classification of data where reference data is used as a benchmark on which to train a machine learning algorithm. A supervised machine learning algorithm will try to minimize the classification error on a known training or validation dataset. The methodology, model complexity and parameters are dependent on the provided data, the complexity of the problem and at times the available computational resources.

For example the MODIS land-cover and land-use product (MCD12Q2) use >1500 validation locations, three years of temporal data and multiple spectral bands, as well as ancillary data (topography, view angle geometry) to train their boosted classification tree [@Friedl2002, @Friedl2010]. In the below example I will try to recreate a reduced part of the MCD12Q2 LULC workflow (red boxes, @fig-classification-workflow). A full introduction to machine learning is beyond the scope of this course and I refer to @boehmke_hands-machine_2020 and @stocker_applied_2023 for an introduction in machine learning using R.

```{r}
#| label: fig-classification-workflow
#| fig-cap: "The full MODIS MCD12Q1 LULC classification workflow (after Friedl et al. 2010). The red boxes highlights the section covered in the worked example."
#| fig-align: "center"
#| out-width: "50%"
#| echo: FALSE
knitr::include_graphics("./images/Friedl_2010.svg")
```

### Validation data

Critical in our exercise is the reference data we use to classify different land-use and land-cover types. Generally ground truth data is used in order to create a training dataset. This ground truth data are locations which are visually validated to belong to a particular land-use or land-cover class. These data are gathered by *in-situ* surveys, or leverage high(er) resolution data and visual interpretation to confirm the the land-use or land-cover type. 

For example, the LUCAS database of the European Commission Joint Research Center provides three-yearly survey data on the land-use and land-cover state [@dandrimont_harmonised_2020], while the Geo-Wiki project used crowdsourcing to confirm land-use and land-cover types using high resolution remote sensing imagery [@fritz_global_2017]. Other products use in house databases using similar approaches [@Friedl2010]. In this example I will rely on the freely available crowdsourced (Geo-Wiki) dataset [@fritz_global_2017]. These validation labels which differ slightly from the International Geosphere-Biosphere Programme (IGBP) classes used in the MCD12Q1 product, but the general principles are the same. 

```{r eval = FALSE}
# Read the validation sites from
# Fritz et al. 2017 straight from
# Zenodo.org
validation_sites <- readr::read_csv(
  "https://zenodo.org/record/6572482/files/Global%20LULC%20reference%20data%20.csv?download=1"
)
```

In the example I will restrict the number of validation sites to a manageable number of 150 random locations for each land-use or land-cover class, limited to high quality locations on the northern hemisphere (as we will apply our analysis regionally to Switzerland). The training data in @fritz_global_2017 contains various crowdsourcing competition of which nr. 1 refers to land-use and nr. 4 to land-cover classes. The below routine shows how to gather 150 random locations for each land-use or land-cover class from @fritz_global_2017. This data will be used to train and test the supervised machine learning algorithm.

```{r eval = FALSE}
# filter out data by competition,
# coverage percentage and latitude
# (use round brackets to enclose complex
# logical statements in a filter call!)
validation_selection <- validation_sites |>
    dplyr::filter(
      (competition == 4 | competition == 1),
      perc1 > 80,
      lat > 0
    )

# the above selection includes all data
# but we now subsample to 150 random locations
# per (group_by()) land cover class (LC1)
# set a seed for reproducibilty
set.seed(0)

validation_selection <- validation_selection |>
    dplyr::slice_sample(n = 150, by = LC1)

# split validation selection
# by land cover type into a nested
# list, for easier processing
# later on
validation_selection <- validation_selection |>
    dplyr::group_by(LC1) |>
    dplyr::group_split()
```

::: callout-note
### Note
The samples are generated randomly, although a random seed ensures some reproducibility it is wise to save your site selection on file.
:::

### Multi-spectral training data

As with our k-means clustering example we need data to inform our supervised classification. For this example, and in the interest of time, we gather MODIS data for various spectral bands (1 to 7) from the MCD43A4 data product, a subset of what is used in the formal MCD12Q1 LULC product. The MCD43A4 data product provides multi-spectral data corrected for (geometric) view angle effects between the satellite and the sun. These data are delivered as daily values.

The required MCD43A4 data is not readily available using the {MODISTools} R package as previously introduced and I will therefore rely on the [{appears} R package](https://bluegreen-labs.github.io/appeears) [REFERENCE] to query and download training data. 
The {appears} package relies on the NASA AppEEARS service which provides easy access to remote sensing data subsets similar to the ORNL DAAC service, as used by {MODISTools}. The provided data covers more data products, but does require a login (API key) limiting ad-hoc accessibility.

::: callout-warning
### Warning
I refer to the {appears} documentation for the [setup and use of an API key](https://bluegreen-labs.github.io/appeears/articles/appeears_vignette.html). The instructions below assume you have registered and have a working key installed in your R session.
:::

```{r eval = FALSE}
# for every row download the data for this
# location and the specified reflectance
# bands
task_nbar <- lapply(validation_selection, function(x){
  
  # loop over all list items (i.e. land cover classes)
  base_query <- x |>
    dplyr::rowwise() |>
    do({
      data.frame(
        task = paste0("nbar_lc_",.$LC1),
        subtask = as.character(.$pixelID),
        latitude = .$lat,
        longitude = .$lon,
        start = "2012-01-01",
        end = "2012-12-31",
        product = "MCD43A4.061",
        layer = paste0("Nadir_Reflectance_Band", 1:4)
      )
    }) |>
    dplyr::ungroup()
  
  # build a task JSON string 
  task <- rs_build_task(
    df = base_query
  )
  
  # return task
  return(task)
})

# Query the appeears API and process
# data in batches - this function
# requires an active API session/login
rs_request_batch(
  request = task_nbar,
  workers = 10,
  user = "your_api_id",
  path = tempdir(),
  verbose = TRUE,
  time_out = 28800
)
```

With both training and model validation data downloaded we can now train a supervised machine learning model! For convenience we do have to wrangle the data into a format that is acceptable for machine learning tools. In particular we need to convert the data from a long format to a wide format (see @sec-tidy-pivots), where every row is a feature vector. A new package, {vroom}, is introduced to efficiently read in a large amount of similar CSV files into a large dataframe using a single list of files.

```{r eval = FALSE}
# list all MCD43A4 files, note that
# that list.files() uses regular
# expressions when using wildcards
# such as *, you can convert general
# wildcard use to regular expressions
# with glob2rx()
files <- list.files(
  tempdir(),
  glob2rx("*MCD43A4-061-results*"),
  recursive = TRUE,
  full.names = TRUE
)

# read in the data (very fast)
# with {vroom} and set all
# fill values (>=32767) to NA
nbar <- vroom::vroom(files)
nbar[nbar >= 32767] <- NA

# retain the required data only
# and convert to a wide format
nbar_wide <- nbar |>
  dplyr::select(
    Category,
    ID,
    Date,
    Latitude,
    Longitude,
    starts_with("MCD43A4_061_Nadir")
  ) |>
  tidyr::pivot_wider(
    values_from = starts_with("MCD43A4_061_Nadir"),
    names_from = Date
  )

# split out only the site name,
# and land cover class from the
# selection of validation sites
# (this is a nested list so we
# bind_rows across the list)
sites <- validation_selection |>
  dplyr::bind_rows() |>
  dplyr::select(
    pixelID,
    LC1
  ) |>
  dplyr::rename(
    Category = "pixelID"
  )

# combine the NBAR and land-use
# land-cover labels by location
# id (Category)
ml_df <- left_join(nbar_wide, sites)
```

### Model training

This example will try to follow the original MODIS MCD12Q1 workflow closely and therefore calls for a boosted regression classification approach [@Friedl2010]. This is a method allows for the use of a combination of weak learners [fractional models with limited predictive power across the whole domain but good performance in a specific part of the domain] to be combined into a single robust classification model [REFERENCES]. The in depth discussion of the algorithm is beyond the scope of this course and I refer to specialized literature [REFERENCES] for more details. Some notes will be included as reminders though.

To properly evaluate or model we need to split our data in a true training dataset, and a test dataset. The test dataset will be used in model evaluation, where samples are independent of those contained within the training dataset.

```{r eval = FALSE}
# select packages
# avoiding tidy catch alls
library(rsample)

# create a data split across
# land cover classes
ml_df_split <- rsample::initial_split(
  strata = ml_df$LC1,
  prop = 0.8
)

# select training and testing
# data based on this split
train <- rsample::training(ml_df_split)
test <- rsample::testing(ml_df_split)
```

With both a true training and testing dataset in place we can start to implement our supervised machine learning model. In this example I will use a "tidy" machine learning  modelling approach. Similar to the data management practices described in @sec-basic-r, a tidy modelling approach relies on sentence like commands using a pipe (|>) and workflows (recipes). This makes formulating models and their evaluation more intuitive for many. For an in depth discussion on tidy modelling in R I refer to [REFERENCES]. So, let's get started.

#### Model structure and workflow {.unnumbered}

To specify our model I will use the {parsnip} R package which goal it is "... to provide a tidy, unified interface to models that can be used to try a range of models without getting bogged down in the syntactical minutiae of the underlying packages". Parsnip therefore will remove some of the complexity of using a package such as {xgboost} might present. It also allows you to switch between different model structures with ease, by only swapping a few arguments. Parsnip will take care of making sure that model parameters are correctly populated and forwarded.

Following [@Friedl2010] we will implement a boosted regression (classification) regression tree using the {xgboost} R package, via the convenient {parsnip} R package interface. The model allows for a number of hyper-parameters, i.e. parameters which do not define the base {xgboost} algorithm but the implementation and structure of it. In this example hyper-parameters for the number of trees is fixed at 50, while the tree depth is flexible (tuned, see below).

::: callout-note
### Note
There are many more hyper-parameters. For the sake of brevity and speed these are kept at a minimum. However, model performance might increase when tuning hyper-parameters more extensively.
:::

```{r eval = FALSE}
# load the parsnip package
# for tidy machine learning
# modelling and workflows
# to manage workflows
library(parsnip)
library(workflows)

# specify our model structure
# the model to be used and the
# type of task we want to evaluate
model_settings <- parsnip::boost_tree(
  trees = 50,
  tree_depth = tune(),
  # learn_rate = tune()
  ) |>
  set_engine("xgboost") |>
  set_mode("classification")

# create a workflow compatible with
# the {tune} package which combines
# model settings with the desired
# model structure (data / formula)
xgb_workflow <- workflows::workflow() |>
  add_formula(as.factor(LC1) ~ .) |>
  add_model(model_settings)

print(xgb_workflow)
```

#### Hyperparameter settings  {.unnumbered}

To optimize our model we need to configure (tune) the hyper-parameters which were left variable in the model settings. How we select these hyper-parameters is determined by sampling a parameter space, defined in our example by [latin hypercube sampling](https://en.wikipedia.org/wiki/Latin_hypercube_sampling). The {dials} R package provides a "tidymodels" compatible functions to support building latin hypercubes. In this example, I use the `extract_param_set_dials()` function to automatically populate the latin hypercube sampling settings. However, these can be specified manually when needed.

```{r eval = FALSE}
# load the dials package
# responsible for (hyper) parameter
# sampling schemes to tune
# parameters (as extracted)
# from the model specifications
library(tune)
library(dials)

hp_settings <- dials::grid_latin_hypercube(
  tune::extract_param_set_dials(xgb_workflow)
  size = 3
)

print(hp_settings)
```

#### Parameter estimation and cross-validation {.unnumbered}

With the above settings we can move on to the actual model calibration. To ensure robustness of our model (hyper-) parameters across the training dataset the code below also implements a [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) to rotate through our training data using different subsets. The below code tunes the model parameters across the cross-validation folds and returns all these results.

```{r eval = FALSE}
# set the folds (division into different)
# cross-validation training datasets
folds <- rsample::vfold_cv(train, v = 3)

# optimize the model (hyper) parameters
# using the:
# 1. workflow (i.e. model)
# 2. the cross-validation across training data
# 3. the (hyper) parameter specifications
# all data are saved for evaluation
xgb_results <- tune::tune_grid(
  xgb_workflow,
  resamples = folds,
  grid = hp_settings,
  control = tune::control_grid(save_pred = TRUE)
)

print(xgb_results)
```

When the model is tuned across folds and parameters the results show varied model performance. In order to select the best model, according to a set metric, you can use the `select_best()` function (for a specific metric). This will extract the best model parameters. However, the returned object does not specify the model structure. To combine both the model structure (workflow) using the optimal parameters we need to combine both. In short, we need to `finalize_workflow()` which combines the model workflow with the model parameters into a final functional model which can take input data and return the desired results.

```{r include = FALSE}
xgb_best_model <- readRDS(here::here("data/xgboost_model.rds"))
```

```{r eval = FALSE}
# select the best model based upon
# the root mean squared error
xgb_best <- tune::select_best(
  xgb_results,
  metric = "roc_auc"
  )

# cook up a model using finalize_workflow
# which takes workflow (model) specifications
# and combines it with optimal model
# parameters into a functional model
xgb_best_model <- tune::finalize_workflow(
  xgb_workflow,
  xgb_best
)

print(xgb_best_model)
```

Note that the resulting model (workflow) does not contain any tunable (`tune()`) fields as shown above! The selected model, given our provided constrains on the hyperparamters, has 50 trees, has minimum 11 data points in a node (min_n) and a tree depth of 5.

### Model evaluation

In the case of land-use and land-cover mapping classifications results are often reported as an error matrix and their associated metrics, and the Kappa coefficient. The error matrix tells us something about accuracy, while the Kappa coefficient allows us the compare the accuracy (across maps).

```{r eval = FALSE}
test_fit <- fit(xgb_best_model, test)
table(test$LC1, test_fit)
```

### Model scaling

#### Data download {.unnumbered}

The training data for our land-cover mapping exercise will rely on multi-spectral band data, mainly bands 1 through 7. We first define a regions of interest covering an area similar to the previous exercises, roughly spanning the canton of Bern, Switzerland, covering a wide variety of land-cover types.

```{r eval = FALSE}
# We can define an appeears
# download task using a simple
# dataframe and a map from which
# an extent is extracted
task_df <- data.frame(
  task = "raster_download",
  subtask = "swiss",
  start = "2012-01-01",
  end = "2012-12-31",
  product = "MCD43A4.061",
  layer = paste0("Nadir_Reflectance_Band", 1:4)
)

# build the area based request/task
# using the extent of our previous
# kmeans map, export all results
# as geotiff (rather than netcdf)
task <- rs_build_task(
  df = task_df,
  roi = kmeans_map,
  format = "geotiff"
)

# request the task to be executed
# with results stored in a
# temporary location (can be changed)
rs_request(
  request = task,
  user = "your_api_id",
  transfer = TRUE,
  path = tempdir(),
  verbose = TRUE
)
```

#### Model execution {.unnumbered}

```{r eval = FALSE}
files <- list.files(
  tempdir(),
  "*Reflectance*",
  recursive = TRUE,
  full.names = TRUE
)

# load this spatial data to run the model
# spatially
swiss_multispec_data <- terra::rast(files)

# the model only works when variable names
# are consistent we therefore rename them
band_names <- data.frame(
  name = names(swiss_multispec_data)
) |>
  mutate(
    date = as.Date(substr(name, 40, 46), format = "%Y%j"),
    name = paste(substr(name, 1, 35), date, sep = "_"),
    name = gsub("\\.","_", name)
  )

# reassign the names of the terra image stack
names(swiss_multispec_data) <- band_names$name

# return probabilities, where each class is
# associated with a layer in an image stack
# and the probabilities reflect the probabilities
# of the classification for said layer
lulc_probabilities <- terra::predict(
  swiss_multispec_data,
  xgb_best_model,
  type = "prob"
)
```

```{r include = FALSE}
# return probabilities
lulc_probabilities <- rast(here::here("data/xgboost_spatial_probabilities.tif"))
lulc_map <- terra::app(lulc_probabilities, which.max)

modis_lulc <- readRDS(here::here("data/land-cover_2012.rds")) |>
  dplyr::mutate_at(vars(matches("value")),
            function(x) case_when(
              x <= 5  ~ 1,
              (x >= 6 & x <= 9) ~ 2,
              x == 10 ~ 3,
              x == 11 ~ 6,
              x == 12 ~ 4,
              x == 13 ~ 7,
              x == 14 ~ 5,
              x == 15 ~ 8,
              x == 16 ~ 9,
              x == 17 ~ 10
            )) |>
  mt_to_terra()
```

```{r warning = FALSE}
#| code-fold: true
#| label: fig-classification-probs
#| fig-cap: "Classification probabilities"
#| fig-align: "center"
#| out-width: "100%"

ggplot() +
  tidyterra::geom_spatraster(data = lulc_probabilities) +
  scale_fill_viridis_c(
    na.value = NA,
    name = "Class probabilities",
    option = "magma"
    ) +
  scale_x_continuous(breaks = seq(-180, 180, 2)) +
  theme_bw() +
  theme(
    legend.position = "bottom"
    ) +
  facet_wrap(~lyr)
```

Reducing the probabilities to classes.

```{r}
# generate the map by selecting maximum probabilities
# from the model output
lulc_map <- terra::app(lulc_probabilities, which.max)
```

```{r}
#| code-fold: true
#| label: fig-spatial-lulc
#| fig-cap: ""
#| fig-align: "center"
#| out-width: "100%"

classes <- c(
    "Tree Cover",
    "Shrub Cover",
    "Herbaceous Vegetation & Grassland",
    "Cultivated and Managed",
    "Mosaic: Managed & Natural Vegetation",
    "Regularly Flooded & Wetland",
    "Urban & Built Up",
    "Snow and Ice",
    "Barren",
    "Open Water"
  )

# set te colour scale manually
palcol <- colorFactor(
  c(
    "#05450a",
    "#78d203",
    "#009900",
    "#c24f44",
    "#ff6d4c",
    "#27ff87",
    "#a5a5a5",
    "#69fff8",
    "#f9ffa4",
    "#1c0dff"
    ),
  na.color = NA,
  domain = 1:10
)

# build the leaflet map
leaflet() |> 
  addProviderTiles(providers$Esri.WorldImagery, group = "World Imagery") |>
  addProviderTiles(providers$Esri.WorldTopoMap, group = "World Topo") |>
  addRasterImage(
    lulc_map,
    colors = palcol,
    opacity = 0.8,
    method = "ngb",
    group = "LULC"
  ) |>
  addRasterImage(
    modis_lulc,
    colors = palcol,
    opacity = 0.8,
    method = "ngb",
    group = "LULC MODIS"
  ) |>
  addLayersControl(
    baseGroups = c("World Imagery","World Topo"),
    position = "topleft",
    options = layersControlOptions(collapsed = FALSE),
    overlayGroups = c("LULC", "LULC MODIS")
  ) |>
  addLegend(
    colors = palcol(1:10),
    values = 1:10,
    labels = classes,
    title = "LULC"
  )
```

