[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Handfull of Pixels",
    "section": "",
    "text": "Preface\nIn this free book I will focus on a set of examples which are rather limited in spatial scope, using just a hand full of pixels. In contrast to what the cloud computing revolution promised, many ideas start out from site or localized studies. The focus of this book on using just a hand full of pixels is therefore deliberate. Not only does this allow you to experiment with geospatial data on limited compute infrastructure, it also shows you that true science can be done with relatively modest means. Here you will learn to do big science with little data."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This book uses the R statistical language in order to demonstrate how to process geospatial data. For many this might seem an odd choice but contrary to common believes R does provide solid geospatial frameworks. These frameworks provide all the tools you need for small and larger geospatial projects.\nIn this book I will focus on a set of examples which are rather limited in scope, using just a handfull of pixels.Not only does this allow you to experiment with geospatial data on limited compute infrastructure, it also shows you that true science can be done with relatively modest means. In many ways this logic runs counter to the current practices either moving analysis to the cloud and/or trying to provide wall-to-wall (global) coverage irrespective of the added value to a study.\nThe goal of this book is therefore not to teach you a particular set of (industry) tools, but rather a balanced combination of both open source (R based) tools and a focus on conceptual development for rapid prototyping of research ideas within R. This book is in many ways an extension of Geocomputation with R by Lovelace, Nowosad, Muenchow. For strict geocomputational knowledge I refer to this source. Although the book uses R the methods described are rather environment agnostic and a similar work could be created for the python language. Where possible I will point to such alternatives.\nThis book requires prior knowledge of programming in R, git and cloud based collaborative tools such as Github. However, to refresh some concepts, or for those who want to skip ahead, I will repeat some basic skills at the beginning of the book to get you started."
  },
  {
    "objectID": "basicr.html#project-management",
    "href": "basicr.html#project-management",
    "title": "2  Crash course R",
    "section": "2.1 Project management",
    "text": "2.1 Project management\nFrom a very practical and pragmatic point of view a solid workflow requires reproducible results. The smallest unit in a reproducible workflow is the structure of your personal research project (irrespective of its contributions to a larger whole). Structuring a research project.\nStructuring your project covers:\n\nthe organization of your files (i.e. code, data, manuscripts)\nfile naming conventions (how things are named)\ncode documentation (describing what and how you do things)\ndata documentation (describing data sources)\nversion control (logging progress systematically)\n\nWhen using R the easiest solution to many of these issues is to use an Integrated Development Environment (IDE, e.g. RStudio or VS Code), version control to track code changes such as git and its cloud based collaborative components such as Github or Codeberg. The setup of both IDEs is described in Section 5.1.\nIt is recommended to start a new project in the RStudio IDE and provide a consistent structure grouping similar objects in folders, e.g. storing data in a data folder, your R scripts in the R folder etc. You can further select the tracking of used packages using renv when creating a new project.\n\nproject/\n├─ your_project.Rproj\n├─ vignettes/\n│  ├─ your_dynamic_document.Rmd\n├─ data/\n│  ├─ some_data.csv\n├─ R/\n│  ├─ R_scripts.R\n\nFor those familiar with github I provide a Github template which you can use when creating a new project on Github. This provides a populated project structure and removes the overhead of making decisions on how to structure a project.\n\n\n\n\n\n\nNote\n\n\n\nPay close attention to the setup of your project as an intuitive and consistent structure greatly enhances your productivity, reproducibility, replicability within a different context and the overall quality of your work. First and foremost, your project structure, the documentation you write, and the clarity of your work are notes to your future self. Be kind to your future self."
  },
  {
    "objectID": "basicr.html#basic-r",
    "href": "basicr.html#basic-r",
    "title": "2  Crash course R",
    "section": "2.2 Basic R",
    "text": "2.2 Basic R\nUnlike many other frameworks for geocomputation, and in particular graphical geographic information system (GIS) such as ArcGIS and QGIS, geocomputation in R is uniquely code oriented. Some basic knowledge of data types, code structure and execution is therefore required. Within R common data structures are vectors, list objects and data frames and tibbles, which are defined as such:\n\n# A numeric vector\nv <- c(1,4,5,6)\n\n# A named list\nl <- list(\n  element_a = \"a\",\n  number_2 = 2\n)\n\n# A data frame (or structured named lists)\ndf <- data.frame(\n  numbers = c(1,3),\n  letters = c(\"a\", \"b\")\n)\n\nData frames in this context represent tabulated data where the content can vary.\n\n2.2.1 Subsetting and conversions\nYou can access data in the above data types by referring to them by index, or in some cases by name. For example, accessing the 2th element in vector v can be accomplished by using v[2]. Element ‘a’ in list l can be accessed using\n\n\n2.2.2 Basic math\n\n\n2.2.3 Functions\n\n\n\n\n\n\nExercise sandbox\n\n\n\nBelow you can use the in browser R session to execute some basic R commands as shown above. Execute the formatted statement by hitting Run Code or alter the content and re-run it.\n\nLoading webR..."
  },
  {
    "objectID": "basicr.html#libraries",
    "href": "basicr.html#libraries",
    "title": "2  Crash course R",
    "section": "2.3 Libraries",
    "text": "2.3 Libraries\nNot all software components are included in a basic R installation. Additional components can be installed as packages from official R archives (CRAN), or github. A full list of packages used in the rendering of the book and its examples can be found in Section 5.3, where information on the automated installation of all required packages is provided in Section 5.1.\nFor example, we can extend the capabilities of base R by installing the MODISTools package from the official CRAN archive using: install.pacakges(\"MODISTools\"). After a successful installation you can load packages using the library() function, as library(\"MODISTools\"). Functions as defined within MODISTools can then be accessed in scripts or from the command line.\nAt times it can be useful to use the :: notation in calling package functions. The :: notation allows you to call a function without loading the full package using library(). For example, MODISTools::mt_products() calls the mt_products() function regardless of having loaded MODISTools with library() beforehand.\nThis can be done useful if you only need one particular function, and don’t want to load the whole package (as this might have memory/performance implications). Furthermore, the use of :: makes it explicit what the source package of a particular function is. This can be helpful if packages have functions with the same name, leading to confusion on the source of the function used.\nFor installs from external sources we need the remotes package. Installing a package directly from a Github location would then be possible using for example remotes::install_github(\"bluegreen-labs/MODISToosl\"). Note that packages on Github or elsewhere are note reviewed, and might pose security risks to your system."
  },
  {
    "objectID": "basicr.html#tidy-data",
    "href": "basicr.html#tidy-data",
    "title": "2  Crash course R",
    "section": "2.4 Tidy data",
    "text": "2.4 Tidy data\nThroughout the book I will structure data using a tidy data approach. Tidy data is a way of structuring data where every row represents a single sample and every row represents a single variable. The tidy data format is a long row orientated format.\nFIGURE\nThis format allows for easier grouped operations. The dplyr library is critical in this context as it contains most of the components to manipulate tidy data.\nFIGURE\n\n2.4.1 Tidy data manipulations\nAlthough both long or wide data formats have their advantages and drawbacks, the use of long (row oriented) data allows us to use the dplyr library to iterate over data quickly and transparently. Conversions to a long format from a wide (column oriented) format are therefore critical throughout this manual.\n\npivot_longer()\n\n\n\n2.4.2 Tidy data manipulations\n\n\n2.4.3 Tidy operations and pipes\n\nwhat is a pipe?\nwhat are common tidy (rowwise) operations?\n\ngroup_by()\nmutate()\nsummarize()\n\n\nLook for figures in cheat sheets (link back to them)\n\n\n\n\n\n\nExercise\n\n\n\nYour result should look like the plot below\n\n\n\n\n\n\n\n\nExpected Result\n\n\n\n\n\n\n\n                mpg cyl  disp  hp drat    wt qsec vs am gear carb\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.7  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.9  1  1    5    2\nFord Pantera L 15.8   8 351.0 264 4.22 3.170 14.5  0  1    5    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.5  0  1    5    6\nMaserati Bora  15.0   8 301.0 335 3.54 3.570 14.6  0  1    5    8\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.6  1  1    4    2"
  },
  {
    "objectID": "basicr.html#geocomputation-in-r",
    "href": "basicr.html#geocomputation-in-r",
    "title": "2  Crash course R",
    "section": "2.5 Geocomputation in R",
    "text": "2.5 Geocomputation in R\nI briefly re-iterate some of the basic principles of geocomputation in R, highlighting two key libraries to read raster and vector data.\n\n2.5.1 Raster data\nsf and terra."
  },
  {
    "objectID": "getting_data.html#finding-open-trustworthy-data",
    "href": "getting_data.html#finding-open-trustworthy-data",
    "title": "3  Accessing data",
    "section": "3.1 Finding open trustworthy data",
    "text": "3.1 Finding open trustworthy data\nEnvironmental data can be found in a number of locations and a general web search will point you to them if you know the name of a data set you have in mind. If you want to broaden your search into new data products the best sources are often governmental organizations. Governments collect and sponsor data acquisition and retention, with various administrations focusing on on particular topics, e.g. remote sensing, forestry, population statistics. For example, if you are looking for satellite remote sensing data it makes sense to look at for example the European Space Agency (ESA) data repositories or the United States National Aeronautics and Space Administration (NASA). If you are looking for spatially explicit population statistics Eurostat might be a good starting place to start your search. Most states keep inventories of their protected areas as well as detailed forest inventories. Similarly, weather agencies on a state or European level can provide wealth of data. Directing your searches toward state level agencies will land you with reliable sources of data.\nSimilarly, non-governmental organizations (NGOs), foundations and other non-profit organizations can be a valuable source of information. General street layouts, and location based information on businesses and other venues can be sourced from Open Street Map (OSM). The World Wild Fund (WWF) has contributed to biodiversity mapping initiatives. In general, non-profits and NGOs are trustworthy but in an age of disinformation and shadow lobbying you should verify if sources are common within scientific literature.nodo\nScientific literature can also be a valuable source of data products. However, finding these data products is often difficult as they are not necessarily centralized in a structured way or they might not even be shared publicly. Centralized repositories do exist. Noteworthy are Zenodo, an data repository for research data supported by CERN but holding vast stores of data on a variety of research topics. Similarly Dryad and Figshare provide long term storage of published research data.\nBelow you find a list of useful data sources:\n\nECMWFR Copernicus Data Services (climate data)\nCopernicus programme\nDigital Active Archive Centers (DAACs)\n\nORNL DAAC - providing point based environmental data\nLP DAAC\n\nZenodo\nDryad\nFigshare\n\n\nThis list is not extensive and many other sources exist. However, I will source mostly from these data sources in the book. Some familiarity with the names of these data sources is therefore helpful."
  },
  {
    "objectID": "getting_data.html#a-handfull-of-pixels",
    "href": "getting_data.html#a-handfull-of-pixels",
    "title": "3  Accessing data",
    "section": "3.2 A handfull of pixels",
    "text": "3.2 A handfull of pixels\nThe sections above assume that you have inherited some data from someone, or have data files on disk (in a particular format). Yet, most of the time, gathering data is the first step in any analysis. Depending on where data is hosted you can simply download data through your web browser or use the internal download.file() R function to grab data.\nToday, many of the data described in previous sections are warehoused in large cloud facilities. These data (and their underlying data formats) are stored in large databases and displayed through various applications. For example, Google Maps displays remote sensing (satellite) raster image data in addition to street level vector based labels. These services allow you to access the underlying (original) data using an API, hence programmatically using code. Mastering the use of these services has become key in gathering research data.\n\n3.2.0.1 Direct downloads\nBefore diving into a description of APIs, we remind you that some file reading functions in R are web-aware, and can not only read local files but also remote ones (i.e., URLs). Getting ahead of ourselves a bit (see tutorials below), the example code shows you how to read the content of a URL directly into your R environment.\nAlthough using this functionality isn’t equivalent to using an API, the concept is the same. I.e., you load a remote data source.\n\n# define a URL with data of interest\n# in this case annual mean CO2 levels at Mauna Loa\nurl <- \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.csv\"\n\n# read in the data directly from URL\ndf <- read.table(\n  url,\n  header = TRUE,\n  sep = \",\"\n)\n\n\n\n3.2.0.2 APIs\nWeb-based Application Programming Interfaces (APIs) offer a way to specify the scope of the returned data, and ultimately, the processing which goes on behind the scene in response to a (data) query. APIs are a way to, in a limited way, control a remote server to execute a certain (data) action. In most (RESTful) APIs, such query takes the form of an HTTP URL via an URL-encoded scheme using an API endpoint (or base URL).\nTo reduce some of the complexity of APIs, it is common that a wrapper is written around an API in the language of choice (e.g., R, Python). These dedicated API libraries make it easier to access data and limit coding overhead.\n\nDedicated API libraries\nAs an example of a dedicated library, we use the {MODISTools} R package which queries remote sensing data generated by the MODIS remote sensing (satellite) mission from the Oak Ridge National Laboratories data archive.\n\n# load the library\nlibrary(\"MODISTools\")\n\n# list all available products\nproducts <- MODISTools::mt_products()\n\n# print the first few lines\n# of available products\nprint(head(products))\n\n# download a demo dataset\n# specifying a location, a product,\n# a band (subset of the product)\n# and a date range and a geographic\n# area (1 km above/below and left/right).\n# Data is returned internally and the\n# progress bar of the download is not shown.\nsubset <- MODISTools::mt_subset(\n  product = \"MOD11A2\",\n  lat = 40,\n  lon = -110,\n  band = \"LST_Day_1km\",\n  start = \"2004-01-01\",\n  end = \"2004-02-01\",\n  km_lr = 1,\n  km_ab = 1,\n  internal = TRUE,\n  progress = FALSE\n)\n\n# print the dowloaded data\nprint(head(subset))\n\nA detailed description of all functions of the {MODISTools} R package is beyond the scope of this course. However, the listed command show you what a dedicated API package does. It is a shortcut to functional elements of an API. For example mt_products() allows you to quickly list all products without any knowledge of an API URL. Although more complex, as requiring parameters, the mt_subset() routine allows you to query remote sensing data for a single location (specified with a latitude lat and longitude lon), and a given date range (e.g., start, end parameters), a physical extent (in km left-right and above-below).\n\n\nGET\nDepending on your data source, you will either need to rely on a dedicated R package to query the API or study the API documentation. The general scheme for using an API follows the use of the GET() command of the {httr} R library. You define a query using API parameters, as a named list, and then use a GET() statement to download the data from the endpoint (url).\n\n# formulate a named list query to pass to httr\nquery <- list(\n  \"argument\" = \"2\",\n  \"another_argument\" = \"3\"\n)\n\n# The URL of the API (varies per product / param)\nurl <- \"https://your.service.endpoint.com\"\n\n# download data using the\n# API endpoint and query data\n# status variable will include if\n# the download was successful or not\n# the write_disk() function captures\n# data if available and writes it to\n# disk\nstatus <- httr::GET(\n  url = url,\n  query = query,\n  httr::write_disk(\n    path = \"/where/to/store/data/filename.ext\",\n    overwrite = TRUE\n  )\n)\n\nBelow, we provide an example of using the GET command to download data from the Regridded Harmonized World Soil Database (v1.2) as hosted on the Oak Ridge National Laboratory computer infrastructure. In this case we download a subset of a global map of topsoil sand content (T_SAND).\n\n# set API URL endpoint\n# for the total sand content\nurl <- \"https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1247/T_SAND.nc4\"\n\n# formulate query to pass to httr\nquery <- list(\n  \"var\" = \"T_SAND\",\n  \"south\" = 32,\n  \"west\" = -81,\n  \"east\" = -80,\n  \"north\" = 34,\n  \"disableProjSubset\" = \"on\",\n  \"horizStride\" = 1,\n  \"accept\" = \"netcdf4\"\n)\n\n# download data using the\n# API endpoint and query data\nstatus <- httr::GET(\n  url = url,\n  query = query,\n  httr::write_disk(\n    path = file.path(tempdir(), \"T_SAND.nc\"),\n    overwrite = TRUE\n  )\n)\n\n# to visualize the data\n# we need to load the {terra}\n# library\nlibrary(\"terra\")\nsand <- terra::rast(file.path(tempdir(), \"T_SAND.nc\"))\nterra::plot(sand)\n\n\n\nAuthentication\nDepending on the API, authentication using a user name and a key or password is required. Then, the template should be slightly altered to accommodate for these requirements. Note that instead of the GET() command we use POST() as we need to post some authentication data before we can get the data in return.\n\n# an authenticated API query\nstatus <- httr::POST(\n  url = url,\n  httr::authenticate(user, key),\n  httr::add_headers(\"Accept\" = \"application/json\",\n                    \"Content-Type\" = \"application/json\"),\n  body = query,\n  encode = \"json\"\n)\n\n\n# Download basic MODIS dataset, this should be pre-downloaded\n# as the ORNL DAAC can be horrendously slow.\n#\n# Students should initiate the download but probably will realize\n# that it will take too long, we need to offer a fix if they\n# want to proceed quickly\n\n# load libraries\nlibrary(terra)\nlibrary(geodata)\n\n# download SRTM data\nif (!file.exists(\"data-raw/srtm_38_03.tif\")){\n  geodata::elevation_3s(\n    lat = 46.6756,\n    lon = 7.85480,\n    path = \"data-raw/\"\n  )\n}\n\n# post processing for lessons\n\nphenology <- readRDS(\"data/phenology_2012.rds\")\nphenology <- phenology |>\n  mutate(\n    value = ifelse(value > 32656, NA, value),\n    value = format(as.Date(\"1970-01-01\") + value, \"%j\")\n  )\nphenology_raster <- mt_to_terra(phenology, reproject = TRUE)\n\n# crop the dem\ndem <- terra::crop(\n  x = dem,\n  y = phenology_raster\n)\n\n# resample the dem\ndem <- terra::resample(\n  x = dem,\n  y = phenology_raster\n)\n\n\n# load libraries\nlibrary(MODISTools)\nlibrary(terra)\n\n# download and save phenology data\nphenology_2012 <- mt_subset(\n  product = \"MCD12Q2\",\n  lat = 46.6756,\n  lon = 7.85480,\n  band = \"Greenup.Num_Modes_01\",\n  start = \"2012-01-01\",\n  end = \"2012-12-31\",\n  km_lr = 100,\n  km_ab = 100,\n  site_name = \"swiss\",\n  internal = TRUE,\n  progress = FALSE\n)\n\nsaveRDS(\n  phenology_2012,\n  \"data/phenology_2012.rds\",\n  compress = \"xz\"\n)\n\n# download and save land cover data\nland_cover_2012 <- mt_subset(\n  product = \"MCD12Q1\",\n  lat = 46.6756,\n  lon = 7.85480,\n  band = \"LC_Type1\",\n  start = \"2012-01-01\",\n  end = \"2012-12-31\",\n  km_lr = 100,\n  km_ab = 100,\n  site_name = \"swiss\",\n  internal = TRUE,\n  progress = FALSE\n)\n\nsaveRDS(\n  land_cover_2012,\n  \"data/land-cover_2012.rds\",\n  compress = \"xz\"\n)\n\n# download LAI data\nlai_2012 <- mt_subset(\n  product = \"MCD15A3H\",\n  lat = 46.6756,\n  lon = 7.85480,\n  band = c(\"Lai_500m\",\"FparLai_QC\"),\n  start = \"2012-01-01\",\n  end = \"2012-12-31\",\n  km_lr = 100,\n  km_ab = 100,\n  site_name = \"swiss\",\n  internal = TRUE,\n  progress = TRUE\n)\n\nsaveRDS(\n  lai_2012,\n  \"data/lai_2012.rds\",\n  compress = \"xz\"\n)\n\nmessage(\"all done...\")"
  },
  {
    "objectID": "geospatialr.html#basic-operations-on-geospatial-data",
    "href": "geospatialr.html#basic-operations-on-geospatial-data",
    "title": "4  Geospatial data in R",
    "section": "4.1 basic operations on geospatial data",
    "text": "4.1 basic operations on geospatial data"
  },
  {
    "objectID": "appendix.html#sec-setup",
    "href": "appendix.html#sec-setup",
    "title": "5  Appendix",
    "section": "5.1 Setup",
    "text": "5.1 Setup\nA common Integrated Development Environment for R is Rstudio by Posit.co. RStudio can be downloaded for free, and provides you with an interface in which a command line terminal, a text editor, a plotting window and file manager are combined. Many other features are also included.\n\n\nRStudio install\nR packakge requirements\n\ntidyverse\nMODISTools\necmwfr\nterra\nsf\nsp"
  },
  {
    "objectID": "appendix.html#sec-license",
    "href": "appendix.html#sec-license",
    "title": "5  Appendix",
    "section": "5.2 Licensing",
    "text": "5.2 Licensing\n\n\n\n\n\n\nLicense\n\n\n\nPlease note that this work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License."
  },
  {
    "objectID": "appendix.html#sec-bookinfo",
    "href": "appendix.html#sec-bookinfo",
    "title": "5  Appendix",
    "section": "5.3 Book configuration",
    "text": "5.3 Book configuration\nThe book was rendered using the following package configuration:\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.3 (2023-03-15)\n os       Pop!_OS 22.04 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_IE.UTF-8\n ctype    en_IE.UTF-8\n tz       Europe/Zurich\n date     2023-03-30\n pandoc   2.18 @ /usr/lib/rstudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P cli           3.6.0   2023-01-09 [?] CRAN (R 4.2.2)\n P digest        0.6.31  2022-12-11 [?] CRAN (R 4.2.2)\n P evaluate      0.20    2023-01-17 [?] CRAN (R 4.2.2)\n P fastmap       1.1.1   2023-02-24 [?] CRAN (R 4.2.2)\n P htmltools     0.5.4   2022-12-07 [?] CRAN (R 4.2.2)\n P jsonlite      1.8.4   2022-12-06 [?] CRAN (R 4.2.2)\n P knitr         1.42    2023-01-25 [?] CRAN (R 4.2.2)\n P rlang         1.0.6   2022-09-24 [?] CRAN (R 4.2.2)\n P rmarkdown     2.20    2023-01-19 [?] CRAN (R 4.2.2)\n P sessioninfo   1.2.2   2021-12-06 [?] RSPM (R 4.2.2)\n P xfun          0.37    2023-01-31 [?] CRAN (R 4.2.2)\n\n [1] /home/khufkens/GECO/git_repos/handfull_of_pixels/renv/library/R-4.2/x86_64-pc-linux-gnu\n [2] /home/khufkens/GECO/git_repos/handfull_of_pixels/renv/sandbox/R-4.2/x86_64-pc-linux-gnu/9a444a72\n [3] /usr/local/lib/R/site-library\n [4] /usr/lib/R/site-library\n [5] /usr/lib/R/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  }
]