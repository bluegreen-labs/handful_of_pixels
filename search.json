[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Handfull of Pixels",
    "section": "",
    "text": "Preface\nIn this free book I will focus on a set of examples which are rather limited in spatial scope, using just a hand full of pixels. In contrast to what the cloud computing revolution promised, many ideas start out from site or localized studies. The focus of this book on using just a hand full of pixels is therefore deliberate. This allows data to be as large as required, but as small as possible.\nNot only does this allow you to experiment with geospatial data on limited compute infrastructure, it also shows you that true science can be done with relatively modest means. Here you will learn to do big science with little data.\nThis book is proudly GPT free, mistakes are my own."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This book uses the R statistical language in order to demonstrate how to process geospatial data. For many this might seem an odd choice but contrary to common believes R does provide solid geospatial frameworks. These frameworks provide all the tools you need for small and larger geospatial projects.\nIn this book I will focus on a set of examples which are rather limited in scope, using just a handfull of pixels.Not only does this allow you to experiment with geospatial data on limited compute infrastructure, it also shows you that true science can be done with relatively modest means. In many ways this logic runs counter to the current practices either moving analysis to the cloud and/or trying to provide wall-to-wall (global) coverage irrespective of the added value to a study.\nTO INCLUDE:\n\nfurthermore note that much validation data is still only available as point data (although some of this is changing within the context of the merger of computer vission with remote sensing)\nit also limits data management (which is also resolved by cloud platforms and bringing compute towards managed datasets)\n\nThe goal of this book is therefore not to teach you a particular set of (industry) tools, but rather a balanced combination of both open source (R based) tools and a focus on conceptual development for rapid prototyping of research ideas within R. This book is in many ways an extension of Geocomputation with R by Lovelace, Nowosad, Muenchow. For strict geocomputational knowledge I refer to this source. Although the book uses R the methods described are rather environment agnostic and a similar work could be created for the python language. Where possible I will point to such alternatives.\nThis book requires prior knowledge of programming in R, git and cloud based collaborative tools such as Github. However, to refresh some concepts, or for those who want to skip ahead, I will repeat some basic skills at the beginning of the book to get you started."
  },
  {
    "objectID": "basic_R.html#project-management",
    "href": "basic_R.html#project-management",
    "title": "1  Crash course R",
    "section": "1.1 Project management",
    "text": "1.1 Project management\nFrom a very practical and pragmatic point of view a solid workflow requires reproducible results. The smallest unit in a reproducible workflow is the structure of your personal research project (irrespective of its contributions to a larger whole). Structuring a research project.\nStructuring your project covers:\n\nthe organization of your files (i.e. code, data, manuscripts)\nfile naming conventions (how things are named)\ncode documentation (describing what and how you do things)\ndata documentation (describing data sources)\nversion control (logging progress systematically)\n\nWhen using R the easiest solution to many of these issues is to use an Integrated Development Environment (IDE, e.g. RStudio or VS Code), version control to track code changes such as git and its cloud based collaborative components such as Github or Codeberg. The setup of both IDEs is described in Appendix A.\nIt is recommended to start a new project in the RStudio IDE and provide a consistent structure grouping similar objects in folders, e.g. storing data in a data folder, your R scripts in the R folder etc. You can further select the tracking of used packages using renv when creating a new project.\n\nproject/\n├─ your_project.Rproj\n├─ vignettes/\n│  ├─ your_dynamic_document.Rmd\n├─ data/\n│  ├─ some_data.csv\n├─ R/\n│  ├─ R_scripts.R\n\nFor those familiar with github I provide a Github template which you can use when creating a new project on Github. This provides a populated project structure and removes the overhead of making decisions on how to structure a project.\n\n\n\n\n\n\nNote\n\n\n\nPay close attention to the setup of your project as an intuitive and consistent structure greatly enhances your productivity, reproducibility, replicability within a different context, and the overall quality of your work. First and foremost, your project structure, the documentation you write, and the clarity of your work, are notes to your future self. Be kind to your future self!"
  },
  {
    "objectID": "basic_R.html#basic-r",
    "href": "basic_R.html#basic-r",
    "title": "1  Crash course R",
    "section": "1.2 Basic R",
    "text": "1.2 Basic R\nUnlike many other frameworks for geocomputation, and in particular graphical geographic information system (GIS) such as ArcGIS and QGIS, geocomputation in R is uniquely code oriented. Some basic knowledge of data types, code structure and execution is therefore required. Below I give a very very short introduction to the most basic principles. For an in depth discussion on all these aspects I refer to the resources mentioned at the top of this section.\n\n1.2.1 Data types\nWithin R common data structures are vectors, list objects and data frames and tibbles, which are defined as such:\n\n# A numeric vector\nv &lt;- c(1,4,5,6)\n\n# A named list\nl &lt;- list(\n  element_a = \"a\",\n  number_2 = 2\n)\n\n# A data frame (or structured named lists)\ndf &lt;- data.frame(\n  numbers = c(1,3,2),\n  letters = c(\"a\", \"b\",\"c\")\n)\n\nNote that in R variables are assigned using the &lt;- (left arrow), however you can use = as well (which is more in line with for example python). Once assigned these elements are available for recall (from memory) an can be used for computation, or other operations. For example we can print the list using:\n\nprint(l)\n\n$element_a\n[1] \"a\"\n\n$number_2\n[1] 2\n\n\nData frames in this context represent tabulated data where the content can vary by column.\n\nprint(df)\n\n  numbers letters\n1       1       a\n2       3       b\n3       2       c\n\n\n\n\n1.2.2 Sub-setting and type conversions\nYou can access data in the above data types by referring to them by index, or in some cases by name. For example, accessing the 2th element in vector v can be accomplished by using v[2]. Element ‘a’ in list l can be accessed using:\n\nv[2]\n\n[1] 4\n\n\nNamed list or data frame elements can be accessed using their respective names and the dollar sign ($) notation using the following syntax variable + $ + element or column, e.g. :\n\n# access a named list element\nl$number_2\n\n[1] 2\n\n# access a data frame column\ndf$letters\n\n[1] \"a\" \"b\" \"c\"\n\n\n\n\n1.2.3 Basic math & operations\nAll variables are available for mathematical operations using built in functions or basic mathematical operators (+, -, *, /):\n\n# multiplying/dividing the number vector n\nv * 2\n\n[1]  2  8 10 12\n\n# dividing the number vector n\nv / 2\n\n[1] 0.5 2.0 2.5 3.0\n\n# adding a value to number vector n\nv + 2\n\n[1] 3 6 7 8\n\n# subtracting a value to number vector n\nv - 2\n\n[1] -1  2  3  4\n\n\nNote that these operations are applied to all elements of a vector (or column of a dataframe) when called without specifying a particular subset. This sweeping computation of a whole set of data is called vectorization and can be used to greatly increase computational speed by limiting the need to loop over individual variables explicitly.\nR comes with a large library of functions. A function is a pre-assigned operation which is executed on the desired variable. An example of which is the cumsum() function which calculates the cumulative sum of consecutive elements in a vector / data frame column.\n\n# calculating a cummulative sum of number vector n\ncumsum(v)\n\n[1]  1  5 10 16\n\n\n\n\n1.2.4 Functions (custom)\nIn addition to the included functions you can write your own functions in R. This allows you to automate certain routine operations particular to your project / setting.\nFor example, one can define a small function which reports back the last element of a vector:\n\n# defines a function printing\n# the last element of a vector\nlast &lt;- function(x) {\n  x[length(x)]\n}\n\nExecuting this on our vector v, will show the following result:\n\n# apply the defined function\nlast(v)\n\n[1] 6\n\n\nBe mindful of the breath of included functions in R or currently available packages. For example, within the dplyr package, which provides common data manipulation functionality, there is a function called last(). For the use of libraries see Section 1.3.\n\n\n1.2.5 Reading data\nIn the above examples I have shown only simple examples relying on internal data. However, different functions exist to read external data into R. The most basic function to read human readable (text) data into R is read.table(). Below I use a combination of write.table() and read.table() to demonstrate how to write human readable (text) data to file, and read the data back in.\n\n# create a data frame with demo data\ndf_original &lt;- data.frame(\n  col_1 = c(\"a\", \"b\", \"c\"),\n  col_2 = c(\"d\", \"e\", \"f\"),\n  col_3 = c(1,2,3)\n)\n\n# write table as CSV to disk\nwrite.table(\n  x = df_original,\n  file = file.path(tempdir(), \"your_file.csv\"),\n  sep = \",\",\n  row.names = FALSE\n)\n\n# Read a CSV file\ndf &lt;- read.table(\n  file.path(tempdir(), \"your_file.csv\"),\n  header = TRUE,\n  sep = \",\",\n  stringsAsFactors = FALSE\n)\n\nAll digital data which is not represented as text characters can be considered binary data. One of the advantages of binary data is that it is an efficient representation of data, saving space. This comes at the cost of requiring a dedicated software (i.e. libraries), other than a text editor, to manipulate the data.\nIn geography and environmental sciences particular (geospatial) file formats dominate the field. Some of these file formats relate to the content of the data, some of these formats are legacy formats due to the history of the field itself. Below you find a list some of the most common formats you will encounter. For the use of (geo-spatial) libraries I refer to Section 1.3 and Chapter 3 .\n\n\n\nFile format (extension)\nFormat description\nUse case\nR Library\n\n\n\n\n*.csv\ncomma separated tabular data\nGeneral purpose flat files with row and column oriented data\nbase R\n\n\n*.txt\ntabular data with various delimiters\nGeneral purpose flat files with row and column oriented data\nbase R\n\n\n*.json\nstructured human-readable data\nGeneral purpose data format. Often used in web application. Has geospatial extensions (geojson).\njsonlite\n\n\n*.nc\nNetCDF data array data\nArray-oriented data (matrices with &gt; 2 dimensions). Commonly used to store climate data or model outputs. Alternative to HDF data.\nncdf4, terra, raster\n\n\n*.hdf\nHDF array data\nArray-oriented data (matrices with &gt; 2 dimensions). Commonly used to store Earth observation data.\nhdf\n\n\n*.tiff, *.geotiff\nGeotiff multi-dimensional raster data (see below)\nLayered (3D) raster (image) data. Commonly used to represent spatial (raster) data.\nterra, raster\n\n\n*.shp\nShapefile of vector data (see below)\nCommon vector based geospatial data. Used to describe data which can be captured by location/shape and attribute values.\nsp, sf"
  },
  {
    "objectID": "basic_R.html#sec-libraries",
    "href": "basic_R.html#sec-libraries",
    "title": "1  Crash course R",
    "section": "1.3 Libraries / packages",
    "text": "1.3 Libraries / packages\nNot all software components are included in a basic R installation. Additional components can be installed as libraries or packages from official R archives (CRAN), or github (see below). A full list of packages used in the rendering of the book and its examples can be found in Appendix C, where information on the automated installation of all required packages is provided in Appendix A.\nFor example, we can extend the capabilities of base R by installing the dplyr package from the official CRAN archive using: install.pacakges(\"dplyr\"). dplyr provides a set of functions to facilitate common data manipulation challenges covered in Section 1.4. After a successful installation you can load packages using the library() function, as library(\"dplyr\"). Functions as defined within dplyr can then be accessed in scripts or from the command line.\n\n# load the library\nlibrary(dplyr)\n\n# show the first element of the vector\nfirst(v)\n\n[1] 1\n\n\nAt times it can be useful to use the :: notation in calling package functions. The :: notation allows you to call a function without loading the full package using library(). This can be done useful if you only need one particular function, and don’t want to load the whole package (as this might have memory/performance implications). Furthermore, the use of :: makes it explicit what the source package of a particular function is. This can be helpful if packages have functions with the same name, leading to confusion on the source of the function used.\n\n# show the last element of the vector v\n# using the dplyr version of \"last\"\n# calling the function explicitly\n# avoiding confusion with our own\n# last() function\ndplyr::last(v)\n\n[1] 6\n\n\n\nExternal packages (non CRAN)\nFor installs from external sources we need the remotes package. Installing a package directly from a Github location would then be possible using for example:\n\nremotes::install_github(\"tidyverse/dplyr\")\n\nThis command loads the latest development version of dplyr from its Github location.\n\n\n\n\n\n\nWarning\n\n\n\nNote that packages on Github or elsewhere are note reviewed, and might pose security risks to your system."
  },
  {
    "objectID": "basic_R.html#sec-tidy-data",
    "href": "basic_R.html#sec-tidy-data",
    "title": "1  Crash course R",
    "section": "1.4 Tidy data",
    "text": "1.4 Tidy data\nThroughout the book I will structure data using a tidy data approach. Tidy data is a way of structuring data where every row represents a single sample and every row represents a single variable. The tidy data format is a long row orientated format. In short, in tidy data:\n\nevery column is a variable\nevery row is an observation\nevery cell is a single value\n\n\n\n\n\n\nFigure 1.1: Image by Posit Software, PBC (posit.co)\n\n\n\n\nThis format allows for easier grouped operations. As visually illustrated below, filtering data by observation properties is easier when data is rowwise oriented (Figure 1.2).\n\n\n\n\n\nFigure 1.2: Image by Posit Software, PBC (posit.co)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAny other configuration is considered “messy data”, but therefore not invalid or “bad”. Certain messy data formats might be more memory efficient.\n\n\n\n1.4.1 Tidy data conversions\nAlthough both long or wide data formats have their advantages and drawbacks, the use of long (row oriented) data allows us to use the dplyr library to iterate over data quickly and transparently (see Figure 1.2). However, large amounts of data are provided as “messy data” which is not rowwise oriented. An understanding of conversions from a wide to a long data format are therefore critical throughout this manual.\nTo convert the below “messy” data frame listing “cases” by year for three countries to a long format you can use the pivot_longer() function from the tidyr library. The tidyr library contains a set of tools for the conversion to and cleaning of data into a tidy (not messy) data format.\n\n# demo data frame with \"cases\" for\n# three countries, where the cases\n# are listed by year in a columnn by\n# column basis\ndf &lt;- data.frame(\n  country = c(\"CH\",\"BE\",\"FR\"),\n  year_2000 = c(200, 40, 1340),\n  year_2001 = c(21, 56, 5940)\n)\n\nhead(df)\n\n  country year_2000 year_2001\n1      CH       200        21\n2      BE        40        56\n3      FR      1340      5940\n\n\nThe conversion of the data looks like this, ordering the data with the yearly columns ordered in a single “year” column, the containig data in a column called “cases”. Values are taken from columns (cols) which start with “year”. Alternatively you could provide and vector with column names or column indices.\n\ntidy_df &lt;- tidyr::pivot_longer(\n  df,\n  cols = starts_with(\"year\"),\n  names_to = \"year\",\n  values_to = \"cases\"\n)\n\nhead(tidy_df)\n\n# A tibble: 6 × 3\n  country year      cases\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 CH      year_2000   200\n2 CH      year_2001    21\n3 BE      year_2000    40\n4 BE      year_2001    56\n5 FR      year_2000  1340\n6 FR      year_2001  5940\n\n\n\n\n1.4.2 Tidy data manipulations\n\nPiping data\nWhen using dplyr and tidyr libraries we can make use of the pipe concept. A pipe is a way to forward the output of one function to the input of the next, stringing together an analysis as you would read a sentence. Throughout this manuscript the native pipe (|&gt;) is used. Rewriting the previous pivot_longer() operation would yield:\n\ntidy_df &lt;- df |&gt;\n  tidyr::pivot_longer(\n  cols = starts_with(\"year\"),\n  names_to = \"year\",\n  values_to = \"cases\"\n)\n\nhead(tidy_df)\n\n# A tibble: 6 × 3\n  country year      cases\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 CH      year_2000   200\n2 CH      year_2001    21\n3 BE      year_2000    40\n4 BE      year_2001    56\n5 FR      year_2000  1340\n6 FR      year_2001  5940\n\n\nHere the data frame df has it’s output forwarded (|&gt;) to the function pivot_longer(), which will process the data according to the parameters specified. Note that the data frame is not explicitly called by the pivot_longer() function itself.\n\n\nFiltering data\nTidy data is especially convenient when filtering and selecting data. When filtering data you apply criteria rowwise using the dplyr::filter() function. For example we can filter out data based upon “year” or “cases” in our tidy_df.\n\n# filter data for year_2000\n# with more than 50 cases\ntidy_df |&gt;\n  dplyr::filter(\n    year == \"year_2000\",\n    cases &gt; 50\n  ) |&gt;\n  print()\n\n# A tibble: 2 × 3\n  country year      cases\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 CH      year_2000   200\n2 FR      year_2000  1340\n\n\nFiltering thus reduces the number of rows in your dataset. This can be visually represented as such (Figure 1.3).\n\n\n\n\n\nFigure 1.3: Image by Posit Software, PBC (posit.co)\n\n\n\n\n\n\nSelecting data\nSelecting data operates on columns instead of rows. You can select columns using the dplyr::select() function. For example, when selecting only the country code and cases counted:\n\n# only retain the country code\n# and case numbers\ntidy_df |&gt;\n  dplyr::select(\n    country,\n    cases\n  ) |&gt;\n  print()\n\n# A tibble: 6 × 2\n  country cases\n  &lt;chr&gt;   &lt;dbl&gt;\n1 CH        200\n2 CH         21\n3 BE         40\n4 BE         56\n5 FR       1340\n6 FR       5940\n\n\nThe output therefore has the same number of rows, but only outputs the selected columns.\n\n\n\n\n\nFigure 1.4: Image by Posit Software, PBC (posit.co)\n\n\n\n\n\n\nMutate & summarize\nThe tidy logic allows for easy conversions or reduction of column data using the dplyr::mutate() and dplyr::summarize() functions respectively. For example we can double the case count values using:\n\ntidy_df |&gt;\n  dplyr::mutate(\n    caces_2x = cases * 2\n  ) |&gt;\n  head()\n\n# A tibble: 6 × 4\n  country year      cases caces_2x\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 CH      year_2000   200      400\n2 CH      year_2001    21       42\n3 BE      year_2000    40       80\n4 BE      year_2001    56      112\n5 FR      year_2000  1340     2680\n6 FR      year_2001  5940    11880\n\n\nSimilarly we can calculate the mean case count across all the available data as:\n\ntidy_df |&gt;\n  dplyr::summarize(\n    mean_cases = mean(cases)\n  ) |&gt;\n  head()\n\n# A tibble: 1 × 1\n  mean_cases\n       &lt;dbl&gt;\n1      1266.\n\n\n\n\nGrouped operations\nFinally, we can apply the dplyr::mutate() and dplyr::summarize() functions across groups, defined by an index, using the dplyr::group_by() function. For example we can calculate the mean case count per year as follows:\n\ntidy_df |&gt;\n  group_by(year) |&gt;\n  dplyr::summarize(\n    mean_cases = mean(cases)\n  ) |&gt;\n  head()\n\n# A tibble: 2 × 2\n  year      mean_cases\n  &lt;chr&gt;          &lt;dbl&gt;\n1 year_2000       527.\n2 year_2001      2006.\n\n\n\n\n\n\n\nFigure 1.5: Image by Posit Software, PBC (posit.co)\n\n\n\n\nGrouped operations using dplyr::mutate() and dplyr::summarize() and other functions highlighted cover a substantial fraction of day-to-day data wrangling."
  },
  {
    "objectID": "gathering_data.html#sec-trust-data",
    "href": "gathering_data.html#sec-trust-data",
    "title": "2  Accessing data",
    "section": "2.1 Finding open trustworthy data",
    "text": "2.1 Finding open trustworthy data\nEnvironmental data can be found in a number of locations and a general web search will point you to them if you know the name of a data set you have in mind. If you want to broaden your search into new data products the best sources are often governmental organizations. Governments collect and sponsor data acquisition and retention, with various administrations focusing on on particular topics, e.g. remote sensing, forestry, population statistics. For example, if you are looking for satellite remote sensing data it makes sense to look at for example the European Space Agency (ESA) data repositories or the United States National Aeronautics and Space Administration (NASA). If you are looking for spatially explicit population statistics Eurostat might be a good starting place to start your search. Most states keep inventories of their protected areas as well as detailed forest inventories. Similarly, weather agencies on a state or European level can provide wealth of data. Directing your searches toward state level agencies will land you with reliable sources of data.\nSimilarly, non-governmental organizations (NGOs), foundations and other non-profit organizations can be a valuable source of information. General street layouts, and location based information on businesses and other venues can be sourced from Open Street Map (OSM). The World Wild Fund (WWF) has contributed to biodiversity mapping initiatives. In general, non-profits and NGOs are trustworthy but in an age of disinformation and shadow lobbying you should verify if sources are common within scientific literature.nodo\nScientific literature can also be a valuable source of data products. However, finding these data products is often difficult as they are not necessarily centralized in a structured way or they might not even be shared publicly. Centralized repositories do exist. Noteworthy are Zenodo, an data repository for research data supported by CERN but holding vast stores of data on a variety of research topics. Similarly Dryad and Figshare provide long term storage of published research data.\nBelow you find a list of useful data sources:\n\nECMWFR Copernicus Data Services (climate data)\nCopernicus Open Access Hub (access to the Sentinel remote sensing data)\nEOSDIS Digital Active Archive Centers (DAACs)\nIntegrated Carbon Observation System (ICOS)\nNational Ecosystem Observation Network (NEON)\nScientific data repositories (open data downloads or deposits)\n\nZenodo.org\nDryad\nFigshare\n\n\nThis list is not extensive and many other sources exist. However, I will source mostly from these data sources in the book. Some familiarity with the names of these data sources is therefore helpful. Depending on where data is hosted you can simply download data through your web browser or use the internal download.file() function to grab data to store it locally.\n\n# Downloading a time series of mean CO2 levels from NOAA\n# and storing it in a temporary file\nurl &lt;- \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.csv\"\n\ndownload.file(\n  url,\n  file.path(tempdir(), \"co2.csv\")\n)"
  },
  {
    "objectID": "gathering_data.html#gathering-a-handfull-of-pixels",
    "href": "gathering_data.html#gathering-a-handfull-of-pixels",
    "title": "2  Accessing data",
    "section": "2.2 Gathering a handfull of pixels",
    "text": "2.2 Gathering a handfull of pixels\nThe sections above (Section 2.1) assume that you download data locally, on disk in a particular format. However, many of the data sources described in previous section are warehoused in large cloud facilities. These services allow you to access the underlying (original) data using an Application Programming Interfaces (APIs), hence programmatically, using code. Mastering the use of these services has become key in gathering research data sets. Given the scope of this book I will focus on ways to gather small approximately analysis ready geospatial datasets using APIs.\n\n2.2.1 Direct downloads\nBefore diving into a description of APIs, I remind you that some file reading functions in R are “web-enabled”, and can not only read local files but also remote ones (i.e., URLs). Getting ahead of ourselves a bit, the example code shows you how to read the content of a URL (of CO2 data) directly into your R environment.\nAlthough using this functionality isn’t equivalent to using an API, the concept is the same. I.e., you load a remote data source (albeit without further subsetting or passing of any parameters).\n\n# read in the data directly from URL\ndf &lt;- read.table(\n  url,\n  header = TRUE,\n  sep = \",\"\n)\n\n\n\n2.2.2 APIs\nWeb-based Application Programming Interfaces (APIs) offer a way to specify the scope of the returned data, and ultimately, the processing which goes on behind the scene in response to a (data) query. APIs are a way to, in a limited way, control a remote server to execute a certain (data) action. In most (RESTful) APIs, such query takes the form of an HTTP URL via an URL-encoded scheme using an API endpoint (or base URL).\nTo reduce some of the complexity of APIs, it is common that a wrapper is written around an API in the language of choice (e.g., R, Python). These dedicated API libraries make it easier to access data and limit coding overhead.\n\nDedicated API libraries\nAs an example of a dedicated library, we use the {MODISTools} R package which queries remote sensing data generated by the MODIS remote sensing (satellite) mission from the Oak Ridge National Laboratories data archive. A full description of their API is provided online.\n\n# load the library\nlibrary(\"MODISTools\")\n\n# list all available products\n# (only showing first part of the table for brevity)\nMODISTools::mt_products() |&gt; \n  head()\n\n       product\n1       Daymet\n2 ECO4ESIPTJPL\n3      ECO4WUE\n4       GEDI03\n5     GEDI04_B\n6      MCD12Q1\n                                                                         description\n1 Daily Surface Weather Data (Daymet) on a 1-km Grid for North America, Version 4 R1\n2               ECOSTRESS Evaporative Stress Index PT-JPL (ESI) Daily L4 Global 70 m\n3                          ECOSTRESS Water Use Efficiency (WUE) Daily L4 Global 70 m\n4                GEDI Gridded Land Surface Metrics (LSM) L3 1km EASE-Grid, Version 2\n5       GEDI Gridded Aboveground Biomass Density (AGBD) L4B 1km EASE-Grid, Version 2\n6              MODIS/Terra+Aqua Land Cover Type (LC) Yearly L3 Global 500 m SIN Grid\n  frequency resolution_meters\n1     1 day              1000\n2    Varies                70\n3    Varies                70\n4  One time              1000\n5  One time              1000\n6    1 year               500\n\n# list bands for the MOD11A2\n# product (a land surface temperature product)\nMODISTools::mt_bands(\"MOD11A2\") |&gt; \n  head()\n\n              band                          description valid_range fill_value\n1   Clear_sky_days               Day clear-sky coverage    1 to 255          0\n2 Clear_sky_nights             Night clear-sky coverage    1 to 255          0\n3    Day_view_angl View zenith angle of day observation    0 to 130        255\n4    Day_view_time        Local time of day observation    0 to 240        255\n5          Emis_31                   Band 31 emissivity    1 to 255          0\n6          Emis_32                   Band 32 emissivity    1 to 255          0\n   units scale_factor add_offset\n1   &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;\n2   &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;\n3 degree            1        -65\n4    hrs          0.1          0\n5   &lt;NA&gt;        0.002       0.49\n6   &lt;NA&gt;        0.002       0.49\n\n\nUsing this information we can now formulate a full query for use with the API. Here, I download a demo dataset specifying a location, a product, a band (subset of the product) and a date range and a geographic area (1 km above/below and left/right). Data is returned internally to the variable subset, and the progress bar of the download is not shown.\n\n# Download some data\nsubset &lt;- MODISTools::mt_subset(\n  product = \"MOD11A2\",\n  lat = 40,\n  lon = -110,\n  band = \"LST_Day_1km\",\n  start = \"2004-01-01\",\n  end = \"2004-02-01\",\n  km_lr = 1,\n  km_ab = 1,\n  internal = TRUE,\n  progress = FALSE\n)\n\n# print the dowloaded data\nhead(subset)\n\n\n\n      xllcorner  yllcorner         cellsize nrows ncols        band  units\n1.1 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n2.1 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n3.1 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n4.1 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n1.2 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n2.2 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n    scale latitude longitude     site product      start        end complete\n1.1  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n2.1  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n3.1  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n4.1  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n1.2  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n2.2  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n    modis_date calendar_date   tile     proc_date pixel value\n1.1   A2004001    2004-01-01 h09v05 2020168005635     1 13148\n2.1   A2004009    2004-01-09 h09v05 2020168010833     1 13160\n3.1   A2004017    2004-01-17 h09v05 2020168012220     1 13398\n4.1   A2004025    2004-01-25 h09v05 2020168013617     1 13412\n1.2   A2004001    2004-01-01 h09v05 2020168005635     2 13153\n2.2   A2004009    2004-01-09 h09v05 2020168010833     2 13140\n\n\nA detailed description of all functions of the MODISTools R package is beyond the scope of this course. However, the listed command show you what a dedicated API package does. It is a shortcut to functional elements of an API. For example mt_products() allows you to quickly list all products without any knowledge of an API URL. Although more complex, the mt_subset() routine allows you to query remote sensing data for a single location (specified with a latitude lat and longitude lon), and a given date range (e.g., start, end parameters), a physical extent (in km left-right and above-below).\n\n\nGET\nDepending on your data source, you will either need to rely on a dedicated R package to query the API or study the API documentation. The general scheme for using an API follows the use of the GET() command of the {httr} R library.\nYou define a query using API parameters, as a named list.\n\n# formulate a named list query to pass to httr\nquery &lt;- list(\n  \"argument\" = \"2\",\n  \"another_argument\" = \"3\"\n)\n\nYou define the endpoint (url) where you want to query your data from.\n\n# The URL of the API (varies per product / param)\nurl &lt;- \"https://your.service.endpoint.com\"\n\nFinally, you combine both in a GET() statement to download the data from the endpoint (url).\n\n# the write_disk() function captures\n# data if available and writes it to\n# disk at location \"path\"\nstatus &lt;- httr::GET(\n  url = url,\n  query = query,\n  httr::write_disk(\n    path = \"/where/to/store/data/filename.ext\",\n    overwrite = TRUE\n  )\n)\n\nBelow, we provide an example of using the GET command to download data from the Regridded Harmonized World Soil Database (v1.2) as hosted on the Oak Ridge National Laboratory computer infrastructure. In this case we download a subset of a global map of topsoil sand content (T_SAND, %).\n\n# set API URL endpoint\n# for the total sand content\nurl &lt;- \"https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1247/T_SAND.nc4\"\n\n# formulate query to pass to httr\nquery &lt;- list(\n  \"var\" = \"T_SAND\",\n  \"south\" = 32,\n  \"west\" = -81,\n  \"east\" = -80,\n  \"north\" = 34,\n  \"disableProjSubset\" = \"on\",\n  \"horizStride\" = 1,\n  \"accept\" = \"netcdf4\"\n)\n\n# download data using the\n# API endpoint and query data\nstatus &lt;- httr::GET(\n  url = url,\n  query = query,\n  httr::write_disk(\n    path = file.path(tempdir(), \"T_SAND.nc\"),\n    overwrite = TRUE\n  )\n)\n\nPlotting the data downloaded shows a map of the percentage of sand in the topsoil.\n\n\nCode\n# libraries\nlibrary(terra)\nlibrary(ggplot2)\nlibrary(tidyterra)\n\nsand &lt;- terra::rast(file.path(tempdir(), \"T_SAND.nc\"))\nggplot() +\n  tidyterra::geom_spatraster(data = sand) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"sand (%)\"\n    ) +\n  theme_bw()\n\n\n\n\n\nFigure 2.1: Soil sand percentage (%)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThroughout the book, where possible, I will collapse the code which draws figures. This makes for a better reading experience. If you want to see the underlying code you can click on the “&gt; Code” line to unfold the code chunk. If not code is presented a simple plot() function call was used.\n\n\n\n\nAuthentication\nDepending on the API, authentication using a user name and a key or password is required. Then, the template should be slightly altered to accommodate for these requirements. Note that instead of the GET() command we use POST() as we need to post some authentication data before we can get the data in return.\n\n# an authenticated API query\nstatus &lt;- httr::POST(\n  url = url,\n  httr::authenticate(user, key),\n  httr::add_headers(\"Accept\" = \"application/json\",\n                    \"Content-Type\" = \"application/json\"),\n  body = query,\n  encode = \"json\"\n)"
  },
  {
    "objectID": "geospatial_R.html#the-r-geospatial-ecosystem",
    "href": "geospatial_R.html#the-r-geospatial-ecosystem",
    "title": "3  Geospatial data in R",
    "section": "3.1 The R geospatial ecosystem",
    "text": "3.1 The R geospatial ecosystem\nSpatio-temporal data often comes in the form of dense arrays, with space and time being array dimensions. Examples include socio-economic or demographic data, series of satellite images with multiple spectral bands, spatial simulations, and climate or weather model output.\nA number of libraries (packages) make the use of this spatio-temporal data, and geo-computational work in R easy. However, the ecosystem has grown rapidly and therefore is continuously shifting. Unlike other processing environments this makes it at times hard to keep track of what or when to use a particular package.\nHere, I give a quick overview of the basic functionality and their uses cases of all these packages, finally there will be a brief overview of some basic geospatial operations using terra and sf libraries (see Section 3.1.1 and Section 3.1.2). For a more extensive, deep dive, of all these packages I refer to Nowasad et al. (FIX REF).\n\n3.1.1 The terra package\nThe terra package is the successor of the older raster package, but with a simpler interface. This package deals with both geographic raster and vector data, with the explicit requirement that raster data represent spatially continuous processes on a fixed (rectangular) grid.\n\nReading and inspecting data\n\n# load the library\nlibrary(terra)\n\n# read data from file\nr &lt;- terra::rast(\"demo_data.nc\")\n\nWe can inspect the meta data by calling the object:\n\nprint(r)\n\nclass       : SpatRaster \ndimensions  : 41, 71, 1  (nrow, ncol, nlyr)\nresolution  : 0.1, 0.1  (x, y)\nextent      : 4.95, 12.05, 43.95, 48.05  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : demo_data.nc \nvarname     : t2m (2 metre temperature) \nname        : t2m \nunit        :   K \ntime        : 2022-01-01 12:00:00 UTC \n\n\nOr you can visualize the data by plotting the data (e.g. using plot()).\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = r) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"Temperature (K) \\n\"\n    ) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n    )\n\n\n\n\n\nFigure 3.2: Temperature in Kelvin (K)\n\n\n\n\nDedicated functions exist to extract the layer names (names()) and the time stamps (time()) if there is a time component to the data. These functions allow you to extract these data and use them in analysis.\n\ntime(r)\n\n[1] \"2022-01-01 12:00:00 UTC\"\n\nnames(r)\n\n[1] \"t2m\"\n\n\n\n\nBasic math\nBasic math or logical operations can be performed on maps using standard R notations. As shown above the data contains temperature data in Kelvin. You can convert this data from Kelvin to Celsius by subtracting 273.15 from all values in the data set.\n\n# conversion from Kelvin to C\nr_c &lt;- r - 273.15\n\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = r_c) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"Temperature (C) \\n\"\n    ) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n    )\n\n\n\n\n\nFigure 3.3: Temperature in Celsius (C) as calculated from the original values in Kelvin\n\n\n\n\nLogical operations work in the same way. You can create a mask of temperatures above 5C using a simple logical operation.\n\n# all locations above freezing\n# as a binary mask\nm &lt;- r_c &gt; 5\n\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = m) +\n  scale_fill_viridis_d(\n    name = \"Mask \\n\"\n    ) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n    )\n\n\n\n\n\nFigure 3.4: Mask, defining positions where temperatures are larger than 5C\n\n\n\n\nYou can exclude locations from calculations using masks. This is useful to restrict the region of interest of an analysis or limit edge cases of complex calculations beforehand. As an example you can mask out all values where the binary mask as generated above if FALSE (i.e. temperatures lower than 5C).\n\n# all locations above freezing\n# as a binary mask\nr_m &lt;- terra::mask(r_c, m, maskvalue = FALSE)\n\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = r_m) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"Temperature (C) \\n\"\n    ) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n    )\n\n\n\n\n\nFigure 3.5: Temperature in Celsius (C), with values lower than 5 C masked.\n\n\n\n\n\n\nWriting and exporting data\nThe terra library uses pointers when referencing to data (in memory). This means that you can not save the object itself to resume your work later on. Saving the above masked map r_m using saveRDS(r_m, \"data.rds\") will only save a pointer to a memory space which will not exist when opening a new session. This is in contrast to for example operations on tabulated data (e.g. JSON, CSV files). As such, you need to save the output of your analysis using a formal geospatial data format using writeRaster().\nTo save masked temperature data in Celsius you would use:\n\n# save data to file\nterra::writeRaster(r_m, \"celsius_data_masked.tif\")\n\nAlternatively, but for small datasets only, you could convert the geospatial data to a long oriented data frame and save the data using standard methods to save tabulated data. However, you might loose critical meta-data on geographic projections etc. Using this method to save your work is not recommended unless you keep track of all ancillary meta-data separately.\n\n# convert geospatial data to a\n# data frame notation, where the flag\n# xy = TRUE denotes that pixel coordinate\n# details should be exported as well\ndf &lt;- as.data.frame(r, xy = TRUE)\nhead(df)\n\n    x  y      t2m\n1 5.0 48 286.4682\n2 5.1 48 286.0754\n3 5.2 48 285.6437\n4 5.3 48 285.3351\n5 5.4 48 285.0714\n6 5.5 48 284.8469\n\n\n\n\n\n3.1.2 The sf package\nSimple features are an open standard to store and access geographic data. The sf package provides a way to represent geospatial vector data as simple features in R. This results in nested data.frames or tibbles which adhere to the “tidy” data paradigm as previously described. They therefore are long oriented and support piped workflows on geometries. This standard reduces complexity and keeps geometry operations simple.\n\nReading and inspecting data\nA lot of GIS vector data comes as shapefiles (.shp extention). An example shapefile is include in the sf package, and we can read it using:\n\n# load library\nlibrary(sf)\n\n# load included shapefile\nnc &lt;- sf::st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n\nReading layer `nc' from data source \n  `/home/runner/.cache/R/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu/sf/1.0-12/5b41b4f0bd22b38661d82205a87deb4b/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n\nWhen printing the object you will be provided with an overview, when plotting the spatial data (using plot()) will be visualized (similar to the raster data above).\n\nprint(nc)\n\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\nFirst 10 features:\n    AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n1  0.114     1.442  1825    1825        Ashe 37009  37009        5  1091     1\n2  0.061     1.231  1827    1827   Alleghany 37005  37005        3   487     0\n3  0.143     1.630  1828    1828       Surry 37171  37171       86  3188     5\n4  0.070     2.968  1831    1831   Currituck 37053  37053       27   508     1\n5  0.153     2.206  1832    1832 Northampton 37131  37131       66  1421     9\n6  0.097     1.670  1833    1833    Hertford 37091  37091       46  1452     7\n7  0.062     1.547  1834    1834      Camden 37029  37029       15   286     0\n8  0.091     1.284  1835    1835       Gates 37073  37073       37   420     0\n9  0.118     1.421  1836    1836      Warren 37185  37185       93   968     4\n10 0.124     1.428  1837    1837      Stokes 37169  37169       85  1612     1\n   NWBIR74 BIR79 SID79 NWBIR79                       geometry\n1       10  1364     0      19 MULTIPOLYGON (((-81.47276 3...\n2       10   542     3      12 MULTIPOLYGON (((-81.23989 3...\n3      208  3616     6     260 MULTIPOLYGON (((-80.45634 3...\n4      123   830     2     145 MULTIPOLYGON (((-76.00897 3...\n5     1066  1606     3    1197 MULTIPOLYGON (((-77.21767 3...\n6      954  1838     5    1237 MULTIPOLYGON (((-76.74506 3...\n7      115   350     2     139 MULTIPOLYGON (((-76.00897 3...\n8      254   594     2     371 MULTIPOLYGON (((-76.56251 3...\n9      748  1190     2     844 MULTIPOLYGON (((-78.30876 3...\n10     160  2038     5     176 MULTIPOLYGON (((-80.02567 3...\n\n\n\n\n\n\n\nFigure 3.6: Various layers of the shapefile\n\n\n\n\nYou can extract basic information such as the overall bounding box of the vector data using st_bbox().\n\nst_bbox(nc)\n\n     xmin      ymin      xmax      ymax \n-84.32385  33.88199 -75.45698  36.58965 \n\n\nThe sf framework uses a tidy data approach, as such you can use operations upon the list items stored within the larger data set by calculating the bounding box for each geometry.\n\nnc |&gt; \n    mutate(\n        bbox = purrr::map(geometry, sf::st_bbox)\n    )\n\nSimple feature collection with 100 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\nFirst 10 features:\n    AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n1  0.114     1.442  1825    1825        Ashe 37009  37009        5  1091     1\n2  0.061     1.231  1827    1827   Alleghany 37005  37005        3   487     0\n3  0.143     1.630  1828    1828       Surry 37171  37171       86  3188     5\n4  0.070     2.968  1831    1831   Currituck 37053  37053       27   508     1\n5  0.153     2.206  1832    1832 Northampton 37131  37131       66  1421     9\n6  0.097     1.670  1833    1833    Hertford 37091  37091       46  1452     7\n7  0.062     1.547  1834    1834      Camden 37029  37029       15   286     0\n8  0.091     1.284  1835    1835       Gates 37073  37073       37   420     0\n9  0.118     1.421  1836    1836      Warren 37185  37185       93   968     4\n10 0.124     1.428  1837    1837      Stokes 37169  37169       85  1612     1\n   NWBIR74 BIR79 SID79 NWBIR79                       geometry\n1       10  1364     0      19 MULTIPOLYGON (((-81.47276 3...\n2       10   542     3      12 MULTIPOLYGON (((-81.23989 3...\n3      208  3616     6     260 MULTIPOLYGON (((-80.45634 3...\n4      123   830     2     145 MULTIPOLYGON (((-76.00897 3...\n5     1066  1606     3    1197 MULTIPOLYGON (((-77.21767 3...\n6      954  1838     5    1237 MULTIPOLYGON (((-76.74506 3...\n7      115   350     2     139 MULTIPOLYGON (((-76.00897 3...\n8      254   594     2     371 MULTIPOLYGON (((-76.56251 3...\n9      748  1190     2     844 MULTIPOLYGON (((-78.30876 3...\n10     160  2038     5     176 MULTIPOLYGON (((-80.02567 3...\n                                       bbox\n1  -81.74107, 36.23436, -81.23989, 36.58965\n2  -81.34754, 36.36536, -80.90344, 36.57286\n3  -80.96577, 36.23388, -80.43531, 36.56521\n4  -76.33025, 36.07282, -75.77316, 36.55716\n5  -77.90121, 36.16277, -77.07531, 36.55629\n6  -77.21767, 36.23024, -76.70750, 36.55629\n7  -76.56358, 36.16973, -75.95718, 36.55606\n8  -76.95367, 36.29452, -76.46035, 36.55525\n9  -78.32125, 36.19595, -77.89886, 36.55294\n10 -80.45301, 36.25023, -80.02406, 36.55104\n\n\n\n\nBasic vector operations\nGiven the tidy data approach of sf data we can use the same logic to filter data. For example, if we only want to retain Camden county from the data set we can use the filter() function as shown in Section 1.4.\n\n# subset data using the\n# tidy filter approach\ncamden &lt;- nc |&gt;\n    filter(\n        NAME == \"Camden\"\n    )\n\n\n\n\n\n\nFigure 3.7: Camden county, as filtered from the larger dataset\n\n\n\n\nMost common operators vector data are area based logical operations. Such as taking the intersection or union of features, which only retains the outline of a polygon. These operations can be executed on features themselves or should be mediated by geometric binary operations which can be used for filtering the original data.\nFor example the st_intersection() function calculates where and how frequent simple features overlap (interesect) with each other.\n\n# calculate the intersection of all simple features\ni &lt;- sf::st_intersection(sf)\n\n# plot the intersections coloured\n# according to the number of overlaps\nplot(\n i[\"n.overlaps\"],\n main = \"number of intersections\"\n)\n\n\n\n\nThe results are again a tidy simple feature which can be sorted or filtered using the standard filter() function.\n\n# using the tidy data logic and the\n# filter function we can remove all\n# locations which only intersect with\n# itself (n.overlaps == 1)\ni &lt;- i |&gt;\n    filter(\n        n.overlaps &gt; 1\n    )\n\nplot(\n i[\"n.overlaps\"],\n main = \"intersections with other features (n&gt;1)\"\n)\n\n\n\n\nOther functions allow you to group overlapping features. For example grouping of intersecting features can be done using st_union(), which only returns the outermost boundary of a feature.\n\nu &lt;- st_union(i)\nplot(u)\n\n\n\n\nThe sf package is relatively complex and for a full understanding of its components I refer to the package documentation and the book by Nowasad et al (REFERENCE).\n\n\nWriting and exporting data\nAlthough read in sf objects can be saved as internal R formats such as rds files using saveRDS(), for portability between various GIS software packages sf can write out a new shapefile using st_write().\n\n# write the north carolina data\n# to shapefile in the tempdir()\nsf::st_write(nc, file.path(tempdir(), \"nc.shp\"))\n\n\n\n\n3.1.3 The stars package\nThe stars package is another geospatial R package you should take note of. In contrast to terra it is specifically tailored to dealing with data cubes. These are arrays of data on which on axis is time.\n\n\n\n\n\nFigure 3.8: Image by Edzer Pebesma\n\n\n\n\nUnlike the terra package the grid should not regular, but can of a different sort such as curvilinear data.\n\n\n\n\n\nFigure 3.9: Image by Edzer Pebesma\n\n\n\n\nSome other distinctions should be made, such as the fast summarizing of raster data to vector features. However, in most cases, it is best to explore functions within the terra package first before considering stars.\n\n\n3.1.4 Other noteworthy packages\nOther critical packages are ncdf4 which will be installed by default, but the included functions allow for the manipulation (reading and writing) of the common netCDF format. The rstac package which provides a convenient way to browse Spatio-Temporal Asset Catalogues (STAC), a format commonly used to organize remote sensing images online. The sits package which was created by the Brazilian National Institute for Space Research (INPE) and provides tools for machine learning on data cubes.\nNote that R (or python for that matter) is infinitely flexible, and many packages exist to address niche problems within the context of geospatial data manipulation. Although often tapping into the power of the GDAL framework these packages are very powerful in their own right but outside the scope of this basic introduction. For more in depth discussion I refer to the list of resources at the end of this book."
  },
  {
    "objectID": "phenology_trends.html#introduction",
    "href": "phenology_trends.html#introduction",
    "title": "4  Phenology trends",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nLand surface phenology is a first order control on the exchange of water and energy between the biosphere and atmosphere. … As such it is an indicator of climate change. Plant phenology has historically been recorded for many centuries. More recently plant phenology (and its changes) have been recorded by networks of observers, near-surface cameras and not in the least by global satellite monitoring.\nAll these (remote sensing) measurements have provided us with insights in how climate change has altered plant phenology. Overall, climate change moves phenology forward (from spring toward winter) at a rate of 3 days/decade, with rates varying depending on locality and altitude.\nConsequences are many, such as exposing early blooming or leafing plants to increased frosts risks and carbon losses through an explicit opportunity cost. In short, changes to plant and land surface phenology have a profound effect on both the carbon balance and all (co-) dependent processes. Therefore, it is key that we can quantify how phenology changes in response to year-to-year variability and climate change and global heating.\nRemote sensing products provide these insights for almost four decades. Remote sensing, and land surface phenology products they produce have a number of advantages. They provide wall-to-wall coverage, with relative consistency and known (methodological) biases. In this and the following Chapter 5 I will cover several aspects of developing a small research project using the remote sensing of phenology as it’s basis.\nLet’s get started!"
  },
  {
    "objectID": "phenology_trends.html#getting-the-required-data",
    "href": "phenology_trends.html#getting-the-required-data",
    "title": "4  Phenology trends",
    "section": "4.2 Getting the required data",
    "text": "4.2 Getting the required data\nWe want to download topographic data to quantify the influence of topography on phenology. Various sources can be found online but the easiest is to use the geodata package which provides a way to access Shuttle Radar Topography Mission (SRTM) elevation data easily.\n\n# load libraries\nlibrary(geodata)\n\n# download SRTM data\ngeodata::elevation_3s(\n    lat = 46.6756,\n    lon = 7.85480,\n    path = here::here(\"data-raw/\")\n  )\n\n\n\n\n\n\n\nNote\n\n\n\nYou can use the here package to make references to paths consistent within an R project. When wrapping a path with here::here() you reference the file relative to the main project directory, not the current directory. This is especially helpful when using R markdown or other notebook formats such as Quarto. These notebooks render code relative to their current location (path). Notebooks in sub-directories in a project will therefore not be able to easily access data in other sub-directories. The here package resolves this issue.\n\n\nIn this exercise we’ll rely on the MODIS land surface phenology product. This remote sensing based data product quantifies land surface phenology and is a good trade-off between data coverage (global) and precision (on a landscape scale).\nTo gather this data we will use the MODISTools package. For an in depth discussion on gathering data using APIs and this API in particular I refer to Chapter 2.\n\n# load libraries\nlibrary(MODISTools)\n\n# download and save phenology data\nphenology &lt;- MODISTools::mt_subset(\n  product = \"MCD12Q2\",\n  lat = 46.6756,\n  lon = 7.85480,\n  band = \"Greenup.Num_Modes_01\",\n  start = \"2012-01-01\",\n  end = \"2012-12-31\",\n  km_lr = 100,\n  km_ab = 100,\n  site_name = \"swiss\",\n  internal = TRUE,\n  progress = FALSE\n)\n\n\n\n\n\n\n\nExercise: know what you use / download\n\n\n\nFind information on who produced the product, track down the latest literature in this respect and note what the limitations of the product are.\n\nWhat does the band name stand for (mathematically)?\nHow does this relate to other bands within this product?\nWhat are the characteristics of the downloaded data?\n\nis post-processing required?\n\n\nWrite down all these aspects into any report (or code) you create to insure reproducibility.\n\n\nThe downloaded phenology data and the topography data need post-processing in our analysis. There are a number of reasons for this:\n\nMODIS data comes as a tidy data frame\nMODIS data might have missing values\nDEM data extent is larger than MODIS coverage\nTwo non-matching grids (DEM ~ MODIS)\n\nGiven that data downloaded using MODISTools is formatted as tidy data we can change corrupt or missing values into a consistent format. In the case of the MCD12Q2 product all values larger than 32665 can be classified as NA (not available).\nThe documentation of the product also shows that phenology metrics are dates as days counted from January 1st 1970. In order to ease interpretation we will convert these integer values, counted from 1970, to day-of-year values (using as.Date() and format()). We only consider phenological events in the first 200 days of the year, as we focus on spring. Later dates are most likely spurious.\n\n# screening of data\nphenology &lt;- phenology |&gt;\n  mutate(\n    value = ifelse(value &gt; 32656, NA, value),\n    value = as.numeric(format(as.Date(\"1970-01-01\") + value, \"%j\")),\n    value = ifelse (value &lt; 200, value, NA)\n  )\n\nBoth datasets, the DEM and MODIS data, come in two different data formats. For the ease of computation we convert the tidy data to a geospatial (terra SpatRast) format.\n\nphenology_raster &lt;- MODISTools::mt_to_terra(\n  phenology,\n  reproject = TRUE\n)\n\nterra::plot(\n  phenology_raster,\n  main = \"MODIS land surface phenology (as DOY for 2012)\"\n)\n\n\n\n\nWe can now compare both data sets in a spatially explicit way, e.g. compute overlap, reproject or resample data. For example, to limit computational time it is often wise to restrict the region of interest to an overlapping section between both data sets. This allows data to be as large as required but as small as possible. We therefore crop the DEM data to correspond to the size of the coverage of the MODIS phenology data.\n\n# crop the dem\ndem &lt;- terra::crop(\n  x = dem,\n  y = phenology_raster\n)\n\nThe grid of the DEM and MODIS data do not align and so resampling of the data is required. We use the highest resolution data for this resampling, taking the average across the extent of a MODIS pixel.\n\n# resample the dem using\n# the mean DEM value in a\n# MODIS pixel\ndem &lt;- terra::resample(\n  x = dem,\n  y = phenology_raster,\n  method = \"average\"\n)\n\n# mask the locations which\n# have no data\ndem &lt;- terra::mask(\n  dem,\n  is.na(phenology_raster),\n  maskvalues = TRUE\n)\n\nTo provide some context to our results it might be useful to look at different responses by land cover class. In addition to phenology data we can therefore also download the MODIS land cover data product for 2012.\n\n# download and save land cover data\nland_cover &lt;- MODISTools::mt_subset(\n  product = \"MCD12Q1\",\n  lat = 46.6756,\n  lon = 7.85480,\n  band = \"LC_Type1\",\n  start = \"2012-01-01\",\n  end = \"2012-12-31\",\n  km_lr = 100,\n  km_ab = 100,\n  site_name = \"swiss\",\n  internal = TRUE,\n  progress = FALSE\n)\n\nNow convert this data to geospatial format as before.\n\nland_cover_raster &lt;- MODISTools::mt_to_terra(\n  land_cover,\n  reproject = TRUE\n)"
  },
  {
    "objectID": "phenology_trends.html#phenology-trends",
    "href": "phenology_trends.html#phenology-trends",
    "title": "4  Phenology trends",
    "section": "4.3 Phenology trends",
    "text": "4.3 Phenology trends\nWith all data processed we can explore some of the trends in phenology in relation to topography. Plotting the data side by side already provides some insight into expected trends.\n\n# plot the DEM\nplot(\n  dem,\n  main = \"DEM in (m above sea level)\"\n)\n\n# plot the start of season dates\nplot(\n  phenology_raster,\n  main = \"MODIS land surface phenology (as DOY for 2012)\"\n  )\n\n\n\n\nFigure 4.1: DEM / phenology\n\n\n\n\n\n\n\nFigure 4.2: DEM / phenology\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat are the patterns you see when comparing the above two maps?\n\n\nWe can plot the relation between topography and the start of the season (phenology) across the scene (where data is available). Plotting this non-spatially will show a clear relation between topography (altitude) and the start of the season. With an increasing altitude we see the start of the season being delayed. The effect is mild below 1000m and increases above this.\n\n# plotting altitude vs the start of the season\nplot(dem, phenology_raster)\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nAside from the spatial representations as shown above, how is this pattern connected to the physical geography? Consider this broadly without taking into account vegetation type.\n\n\n\nfit &lt;- lm(as.matrix(phenology_raster) ~ as.matrix(dem))\nprint(summary(fit))\n\n\nCall:\nlm(formula = as.matrix(phenology_raster) ~ as.matrix(dem))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-154.235   -8.431    0.092    9.426   78.967 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    5.421e+01  8.691e-02   623.7   &lt;2e-16 ***\nas.matrix(dem) 3.717e-02  6.915e-05   537.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.74 on 145522 degrees of freedom\n  (44020 observations deleted due to missingness)\nMultiple R-squared:  0.6651,    Adjusted R-squared:  0.6651 \nF-statistic: 2.89e+05 on 1 and 145522 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nExercise\n\n\n\nInterpret the results of the above fit model.\n\nWhat does the intercept indicate?\nHow can you interpret the slope?\nConvert the established relationship to one which includes temperature as a covariate.\n\nHow would you go about this?"
  },
  {
    "objectID": "phenology_algorithms.html#algorithms",
    "href": "phenology_algorithms.html#algorithms",
    "title": "5  Phenology algorithms",
    "section": "5.1 Algorithms",
    "text": "5.1 Algorithms\nMost phenology products rely on a limited set of algorithms, or various iterations of them. Throughout both implementations and scientific literature we can divide the methods used in two categories:\n\ncurve fitting\nthreshold based methods\n\nThe curve fitting approach fits a prescribed model structure to the data, by varying a number of parameters. This functional description of the vegetation growth can then be used to derive phenology metrics by considering various inflection points of this function. Common approaches here are to use the first or second derivative of a fitted function (and zero crossings) to determine when large changes in vegetation phenology happen.\n\n\n\n\n\nFigure 5.1: Example of a threshold based phenology algorithm, by Stanimorova et al. 2019\n\n\n\n\nThe most simple approach around, used with or without explicit curve fitting, is the use of a simple threshold based method. Here, a phenology event is registered if a given vegetation index threshold is exceeded (Figure 5.1). Although scaling these products for global coverage is challenging creating your own algorithm and implementing it on a local or regional scale is fairly simple. Below I will show you how to calculate phenology for a vegetation index time series, for the region outlined in Chapter 4 .\n\n5.1.1 Acquiring demo data\nTo get started we need to download some additional information, in particular vegetation time series. In this example, and to lighten the download requirements, I will use Leaf Area Index (LAI) data instead of the more common EVI or NDVI time series used in phenology products.\n\n# download data\ndf &lt;- MODISTools::mt_subset(\n  product = \"MCD15A3H\",\n  lat = 42.536669726040884,\n  lon = -72.17951595626516,\n  band = \"Lai_500m\",\n  start = \"2002-01-01\",\n  end = \"2022-12-31\",\n  km_lr = 0,\n  km_ab = 0,\n  site_name = \"HF\",\n  internal = TRUE,\n  progress = TRUE\n)"
  },
  {
    "objectID": "phenology_algorithms.html#data-quality-control",
    "href": "phenology_algorithms.html#data-quality-control",
    "title": "5  Phenology algorithms",
    "section": "5.2 Data quality control",
    "text": "5.2 Data quality control\nMany data products have data quality flags, which allow you to screen your data for spurious values. However, in this example I will skip this step as the interpretation/use of these quality control flags is rather complex. The below methodology is therefore a toy example and further developing this algorithm would require taking into account these quality control metrics. Instead a general smoothing of the data will be applied."
  },
  {
    "objectID": "phenology_algorithms.html#data-smoothing-interpollation",
    "href": "phenology_algorithms.html#data-smoothing-interpollation",
    "title": "5  Phenology algorithms",
    "section": "5.3 Data smoothing / interpollation",
    "text": "5.3 Data smoothing / interpollation\nWe first multiply the data with their required scaling factor, and transform the date values to a formal date format and split out the individual year (form the date).\n\n# scale values appropriately\ndf$value &lt;- df$value * as.numeric(df$scale)\n\n# convert dates to proper date formats and\n# convert the date to a single year\ndf &lt;- df |&gt;\n  mutate(\n    date = as.Date(calendar_date),\n    year = as.numeric(format(date, \"%Y\")) \n  ) |&gt;\n  filter(\n    year &gt; 2002\n  )\n\nSmoothing can be achieved using various algorithms, such as splines, loess regressions. In this case, I will use the Savitsky-Golay filter, a common algorithm used across various vegetation phenology products. Luckily the methodology is already implemented in the signal package.\n\n# load the signal library\nlibrary(signal)\n\n# smooth this original input data using a\n# savitski-golay filter\ndf &lt;- df |&gt;\n  mutate(\n    smooth = signal::sgolayfilt(value, p = 3, n = 31)\n  )\n\nNote that the sgolayfilt() does not allow for NA values to be present to function properly. As such, we operate on the full data set at its original time step. Since data is only provided at a 8-day time interval you would be limited to this time resolution in determining final phenology metrics when using a threshold based approach.\nTo get estimates close to a daily time step we need to expand the data to a daily time-step and merge the original smoothed data.\n\n# expand the time series to a daily time step\n# and merge with the original data\nexpanded_df &lt;- dplyr::tibble(\n  date = seq.Date(min(df$date), max(df$date), by = 1)\n)\n\n# join the expanded series with the\n# original data\ndf &lt;- dplyr::left_join(expanded_df, df)\n\n# back fill the year column for later\n# processing\ndf &lt;- df |&gt;\n  mutate(\n    year = as.numeric(format(date, \"%Y\"))\n  )\n\nExpanding the 8-day data to a 1-day timestep will result in NA values. I will use a simple linear interpolation between smoothed 8-day values to acquire a complete time series without gaps.\n\n# non NA values\nno_na &lt;- which(!is.na(df$smooth))\n\n# finally interpolate the expanded dataset\n# (fill in NA values)\ndf$smooth_int &lt;- signal::interp1(\n  x = as.numeric(df$date[no_na]),\n  y = df$smooth[no_na],\n  xi = as.numeric(df$date),\n  method = 'linear'\n)\n\n\n\nCode\nggplot(df) +\n  geom_point(\n    aes(\n      date,\n      value\n    ),\n    colour = \"red\"\n  ) +\n  geom_line(\n    aes(\n      date,\n      smooth_int\n    )\n  ) +\n  labs(\n    x = \"\",\n    y = \"LAI\"\n  ) +\n  xlim(\n    c(\n    as.Date(\"2003-01-01\"),\n    as.Date(\"2005-12-31\")\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\nFigure 5.2: Smoothed LAI values as a black line, with original values as red circles"
  },
  {
    "objectID": "phenology_algorithms.html#sec-phenology-estimates",
    "href": "phenology_algorithms.html#sec-phenology-estimates",
    "title": "5  Phenology algorithms",
    "section": "5.4 Phenology estimation",
    "text": "5.4 Phenology estimation\nWith all data prepared we can use an arbitrary threshold to estimate a transition in LAI values for a given year. In the below example we use a LAI value of 3 to mark if the season has started or ended. To show the response to heterogeneous time series, I halved the values for the year 2004\n\n# half the values for 2004\n# to introduce heterogeneity\ndf &lt;- df |&gt;\n  mutate(\n    smooth_int = ifelse(\n      year == 2004,\n      smooth_int/2,\n      smooth_int\n    )\n  )\n\n# calculate phenology dates on\n# the smoothed time series\nphenology &lt;- df |&gt;\n  group_by(year) |&gt;\n  summarize(\n    SOS = date[which(smooth_int &gt; 3)][1],\n    EOS = last(date[which(smooth_int &gt; 3)])\n  )\n\n\n\nCode\nggplot(df) +\n  geom_point(\n    aes(\n      date,\n      value\n    ),\n    colour = \"red\"\n  ) +\n  geom_line(\n    aes(\n      date,\n      smooth_int\n    )\n  ) +\n  labs(\n    x = \"\",\n    y = \"LAI\"\n  ) +\n  xlim(\n    c(\n    as.Date(\"2003-01-01\"),\n    as.Date(\"2005-12-31\")\n    )\n  ) +\n  geom_hline(\n    aes(yintercept = 3),\n    lty = 2\n  ) +\n  geom_vline(\n    data = phenology,\n    aes(xintercept = EOS)\n  ) +\n  geom_vline(\n    data = phenology,\n    aes(xintercept = SOS)\n  ) +\n  theme_bw()\n\n\n\n\n\nFigure 5.3: Calculated phenology dates using a fixed LAI threshold value, where the year 2004 smoothed values are artificially lowered. Smoothed LAI values as a black line, with original values as red circles. Vertical lines indicate the date when the yearly amplitude threshold of an LAI of 3 is exceeded. The dashed horizontal line illustrates that the lowered smoothed response does not meet any threshold requirement and shows no phenology response.\n\n\n\n\nObviously this does not translate well to other locations, when vegetation types and densities vary from place to place. This is illustrated in Figure 5.3, using an artificially lowered signal in the year 2004, were years not meeting the absolute threshold will not produce accurate phenology estimates. Scaling the time series between 0 and 1 will regularize responses across time series and years and resolves this issue.\n\n# potential issues?\n# - fixed LAI threshold (varies per vegetation type)\n# - does not account for incomplete years\n# - provides absolute dates (not always helpful)\ndf &lt;- df |&gt;\n  group_by(year) |&gt;\n  mutate(\n    smooth_int_scaled = scales::rescale(\n      smooth_int,\n      to = c(0,1)\n    )\n  )\n\nNow we can use a relative amplitude (0 - 1) across locations. This ensures a consistent interpretation of what phenology represents. In case of a threshold of 0.5, this would represent the halfway point between a winter baseline and a summer maximum (leaf development). Commonly, one uses multiple thresholds to characterize different phases of the vegetation development. For example, a low threshold (~0.25) characterizes the start of the growing season, while a higher (~0.85) threshold marks the end of vegetation development toward summer.\nAs before, we can now apply relative thresholds (of 0.25 and 0.85) to our time series, to mark the start of the season and maximum crown development. We can reverse this logic, and use the same thresholds on the latter part of the seasonal trajectory and calculate the start of leaf senesence (death) and full leaf loss.\n\n# calculate phenology dates\n# SOS: start of season (25%, start)\n# MAX: max canopy development (85%, start)\n# SEN: canopy sensesence (85%, end)\n# EOS: end of season (25%, end)\nphenology &lt;- df |&gt;\n  group_by(year) |&gt;\n  summarize(\n    SOS = date[which(smooth_int_scaled &gt; 0.25)][1],\n    MAX = date[which(smooth_int_scaled &gt; 0.85)][1],\n    SEN = last(date[which(smooth_int_scaled &gt; 0.85)]),\n    EOS = last(date[which(smooth_int_scaled &gt; 0.25)])[1]\n  )\n\nPlotting these results for a limited set of years shows how phenology dates for different years and phenology metrics (thresholds). Note that using the scaled (yearly) data estimates for the year 2004 are accurate, in contrast to previous results as shown in Figure 5.3.\n\n\nCode\nggplot(df) +\n  geom_point(\n    aes(\n      date,\n      value\n    ),\n    colour = \"red\"\n  ) +\n  geom_line(\n    aes(\n      date,\n      smooth_int\n    )\n  ) +\n  labs(\n    x = \"\",\n    y = \"LAI\"\n  ) +\n  xlim(\n    c(\n    as.Date(\"2003-01-01\"),\n    as.Date(\"2005-12-31\")\n    )\n  ) +\n  geom_vline(\n    data = phenology,\n    aes(xintercept = SOS),\n    colour = \"lightgreen\"\n  ) +\n  geom_vline(\n    data = phenology,\n    aes(xintercept = MAX),\n    colour = \"darkgreen\"\n  ) +\n  geom_vline(\n    data = phenology,\n    aes(xintercept = SEN),\n    colour = \"brown\"\n  ) +\n  geom_vline(\n    data = phenology,\n    aes(xintercept = EOS),\n    colour = \"grey\"\n  ) +\n  theme_bw()\n\n\n\n\n\nFigure 5.4: Calculated phenology dates using a fixed LAI threshold value, where the year 2004 smoothed values are artificially lowered. Smoothed LAI values as a black line, with original values as red circles. Vertical lines indicate the date when the yearly relative amplitude thresholds are exceeded. Light green indicates the start-of-season (SOS), dark green maximum canopy development (MAX), brown the start of senesence (SEN) and grey the end of season (EOS).\n\n\n\n\n\n5.4.1 Spatial phenology estimates\nThe above example introduced the very basics of how to deal with a simple time series and develop a proof of concept. However, to scale our example spatially you need a way to process data along space and time axis. As introduced earlier, the terra package allows you to manipulate 3D data cubes (along latitude,longitude, and time axis).\n\n# Download a larger data cube\n# note that I sample a 100x100 km\n# area around the lat/lon location\nlai_2012 &lt;- MODISTools::mt_subset(\n  product = \"MCD15A3H\",\n  lat = 46.6756,\n  lon = 7.85480,\n  band = c(\"Lai_500m\"),\n  start = \"2012-01-01\",\n  end = \"2012-12-31\",\n  km_lr = 100,\n  km_ab = 100,\n  site_name = \"swiss\",\n  internal = TRUE,\n  progress = TRUE\n)\n\n# save this data for later use\n# to speed up computation\n\nHowever, data downloaded using MODISTools by default is formated as tidy (row oriented) data. We use the mt_to_terra() function to convert this tidy data to a terra raster object.\n\n# conversion from tidy data to a raster format\nr &lt;- MODISTools::mt_to_terra(lai_2012)\n\nRemember the algorithm for a single time series above. To make this collection of steps re-usable within the context of a multi-layer terra raster object (or data cube) you need to define a function. This allows you to run the routine across several pixels (at once).\nBelow I wrap the steps as defined in Section 5.4 in a single function which takes a data frame as input, a phenological phase and threshold value (0 - 1) as parameters. For this example we set the parameter to 0.5, or half the seasonal amplitude.\n\nphenophases &lt;- function(\n    df,\n    return = \"start\",\n    threshold = 0.5\n) {\n\n  # split out useful info\n  value &lt;- as.vector(df) * 0.1\n  \n  # if all values are NA\n  # return NA (missing data error trap)\n  if(all(is.na(value))) {\n    return(NA)\n  }\n  \n  date &lt;- as.Date(names(df))\n  \n  # smooth this original input data using a\n  # savitski-golay filter\n  smooth &lt;- signal::sgolayfilt(value, p = 3, n = 31)\n\n  # expand the time series to a daily time step\n  # and merge with the original data\n  date_expanded &lt;- seq.Date(min(date, na.rm = TRUE), max(date, na.rm = TRUE), by = 1)\n  smooth_int &lt;- rep(NA, length(date_expanded))\n  smooth_int[which(date_expanded %in% date)] &lt;- smooth\n\n  # non NA values for interpolation\n  no_na &lt;- which(!is.na(smooth_int))\n  \n  # finally interpolate the expanded dataset\n  # (fill in NA values)\n  smooth_int &lt;- signal::interp1(\n    x = no_na,\n    y = smooth_int[no_na],\n    xi = 1:length(smooth_int),\n    'linear'\n  )\n\n  # rescale values between 0 and 1\n  smooth_int_scaled &lt;- scales::rescale(\n    smooth_int,\n    to = c(0,1)\n  )\n  \n  # thresholding for phenology detection\n  phenophase &lt;- ifelse(\n    return == \"start\",\n    date_expanded[\n      which(smooth_int_scaled &gt; threshold)\n      ][1],\n    last(\n      date_expanded[\n        which(smooth_int_scaled &gt; threshold)\n        ][1]\n      )\n  )\n\n  # convert to doy\n  doy &lt;- as.numeric(format(as.Date(phenophase, origin = \"1970-01-01\"),\"%j\"))\n  return(doy)\n}\n\nWith the function defined we can now apply this function to all pixels, and along the time axis (layers) of our terra raster stack. The app() function allows you to do exactly this! You can now apply the above function to the time component (z-axis, i.e. various layers) of the LAI data cube.\n\n# apply a function to the z-axis (time / layers) of a data cube\nphenology_map &lt;- app(r, phenophases)\n\n\n\n5.4.2 Inference and comparisons to actual products\nYou now have output from our custom algorithm, and the previously downloaded MODIS (MCD12Q2) phenology data product. You can therefore easily compare both outputs.\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyterra)\nlibrary(patchwork)\n\n# truncate things to first half of the year\nphenology_map[phenology_map &gt; 180] &lt;- NA\nphenology_raster[phenology_raster &gt; 180] &lt;- NA\n\n# rename to sort the legends properly\nnames(phenology_map) &lt;- \"value\"\nnames(phenology_raster) &lt;- \"value\"\n\np &lt;- ggplot() +\n  geom_spatraster(data=phenology_map) +\n  scale_fill_viridis_c(na.value = NA) +\n  labs(\n    title = \"Custom algorithm\"\n  ) +\n  theme_bw()\n\np2 &lt;- ggplot() +\n  geom_spatraster(\n    data=phenology_raster,\n    show.legend = FALSE\n    ) +\n  scale_fill_viridis_c(na.value = NA) +\n  labs(\n    title = \"MODIS MCD12Q2 algorithm\"\n  ) +\n  theme_bw()\n\n# compositing\np + p2 + \n  plot_layout(nrow = 1, guides = \"collect\") + \n  plot_annotation(\n    tag_levels = \"a\",\n    tag_prefix = \"(\",\n    tag_suffix = \")\"\n    )\n\n\n\n\n\nFigure 5.5: Comparison of custom algorithm (a) and MODIS phenology product (b) outputs"
  },
  {
    "objectID": "land_cover_classification.html#introduction",
    "href": "land_cover_classification.html#introduction",
    "title": "6  Land-Cover classification",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction"
  },
  {
    "objectID": "land_cover_classification.html#scope",
    "href": "land_cover_classification.html#scope",
    "title": "6  Land-Cover classification",
    "section": "6.2 Scope",
    "text": "6.2 Scope"
  },
  {
    "objectID": "land_cover_classification.html#demo",
    "href": "land_cover_classification.html#demo",
    "title": "6  Land-Cover classification",
    "section": "6.3 Demo",
    "text": "6.3 Demo"
  },
  {
    "objectID": "phenology_modelling.html#introduction",
    "href": "phenology_modelling.html#introduction",
    "title": "7  Phenology modelling",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction"
  },
  {
    "objectID": "phenology_modelling.html#scope",
    "href": "phenology_modelling.html#scope",
    "title": "7  Phenology modelling",
    "section": "7.2 Scope",
    "text": "7.2 Scope"
  },
  {
    "objectID": "phenology_modelling.html#demo",
    "href": "phenology_modelling.html#demo",
    "title": "7  Phenology modelling",
    "section": "7.3 Demo",
    "text": "7.3 Demo"
  },
  {
    "objectID": "appendix_setup.html",
    "href": "appendix_setup.html",
    "title": "Appendix A — Setup",
    "section": "",
    "text": "A common Integrated Development Environment for R is Rstudio by Posit.co. RStudio can be downloaded for free, and provides you with an interface in which a command line terminal, a text editor, a plotting window and file manager are combined. Many other features are also included.\n\n\nRStudio install\nR packakge requirements\n\ntidyverse\nMODISTools\necmwfr\nterra\nsf\nsp"
  },
  {
    "objectID": "appendix_licensing.html",
    "href": "appendix_licensing.html",
    "title": "Appendix B — Licensing",
    "section": "",
    "text": "All rights belong to the referenced right holders. In absence of any explicit reference to right holders (in figure captions or other materials) the rights reside with the author of this manuscript under the below license.\n\n\n\n\n\n\nLicense\n\n\n\nPlease note that this work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License."
  },
  {
    "objectID": "appendix_config.html",
    "href": "appendix_config.html",
    "title": "Appendix C — Book configuration",
    "section": "",
    "text": "The book was rendered using the following package configuration:\n\nrenv::diagnostics()\n\nDiagnostics Report [renv 0.17.3]\n================================\n\n# Session Info =======================\nR version 4.3.0 (2023-04-21)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nloaded via a namespace (and not attached):\n [1] compiler_4.3.0  fastmap_1.1.1   cli_3.6.1       htmltools_0.5.5\n [5] tools_4.3.0     rmarkdown_2.21  knitr_1.42      jsonlite_1.8.4 \n [9] xfun_0.39       digest_0.6.31   rlang_1.1.1     renv_0.17.3    \n[13] evaluate_0.20  \n\n# Project ============================\nProject path: \"~/work/handfull_of_pixels/handfull_of_pixels\"\n\n# Status =============================\n* The project is already synchronized with the lockfile.\n\n# Packages ===========================\n              Library Source Lockfile Source Path Dependency\nDBI             1.1.3   RSPM    1.1.3   CRAN  [1]   indirect\nKernSmooth    2.23-20   CRAN  2.23-20   CRAN  [2]   indirect\nMASS           7.3-59   RSPM   7.3-59   CRAN  [1]   indirect\nMODISTools      1.1.4   RSPM    1.1.4   CRAN  [1]     direct\nMatrix          1.5-1   RSPM    1.5-1   CRAN  [1]   indirect\nR6              2.5.1   RSPM    2.5.1   CRAN  [1]   indirect\nRColorBrewer    1.1-3   RSPM    1.1-3   CRAN  [1]   indirect\nRcpp           1.0.10   RSPM   1.0.10   CRAN  [1]   indirect\naskpass           1.1   RSPM      1.1   CRAN  [1]   indirect\nbase64enc       0.1-3   RSPM    0.1-3   CRAN  [1]   indirect\nboot         1.3-28.1   CRAN     &lt;NA&gt;   &lt;NA&gt;  [2]       &lt;NA&gt;\nbslib           0.4.2   RSPM    0.4.2   CRAN  [1]   indirect\ncachem          1.0.8   RSPM    1.0.8   CRAN  [1]   indirect\nclass          7.3-21   CRAN   7.3-21   CRAN  [2]   indirect\nclassInt        0.4-9   RSPM    0.4-9   CRAN  [1]   indirect\ncli             3.6.1   RSPM    3.6.1   CRAN  [1]   indirect\ncluster         2.1.4   CRAN     &lt;NA&gt;   &lt;NA&gt;  [2]       &lt;NA&gt;\ncodetools      0.2-19   CRAN     &lt;NA&gt;   &lt;NA&gt;  [2]       &lt;NA&gt;\ncolorspace      2.1-0   RSPM    2.1-0   CRAN  [1]   indirect\ncpp11           0.4.3   RSPM    0.4.3   CRAN  [1]   indirect\ncurl            5.0.0   RSPM    5.0.0   CRAN  [1]   indirect\ndata.table     1.14.8   RSPM   1.14.8   CRAN  [1]   indirect\ndigest         0.6.31   RSPM   0.6.31   CRAN  [1]   indirect\ndplyr           1.1.2   RSPM    1.1.2   CRAN  [1]     direct\ne1071          1.7-13   RSPM   1.7-13   CRAN  [1]   indirect\nellipsis        0.3.2   RSPM    0.3.2   CRAN  [1]   indirect\nevaluate         0.20   RSPM     0.20   CRAN  [1]   indirect\nfansi           1.0.4   RSPM    1.0.4   CRAN  [1]   indirect\nfarver          2.1.1   RSPM    2.1.1   CRAN  [1]   indirect\nfastmap         1.1.1   RSPM    1.1.1   CRAN  [1]   indirect\nfontawesome     0.5.1   RSPM    0.5.1   CRAN  [1]   indirect\nforeign        0.8-84   CRAN     &lt;NA&gt;   &lt;NA&gt;  [2]       &lt;NA&gt;\nfs              1.6.2   RSPM    1.6.2   CRAN  [1]   indirect\ngenerics        0.1.3   RSPM    0.1.3   CRAN  [1]   indirect\ngeodata         0.5-8   RSPM    0.5-8   CRAN  [1]     direct\nggplot2         3.4.2   RSPM    3.4.2   CRAN  [1]     direct\nglue            1.6.2   RSPM    1.6.2   CRAN  [1]   indirect\ngrDevices        &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\ngraphics         &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\ngrid             &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\ngtable          0.3.3   RSPM    0.3.3   CRAN  [1]   indirect\nhere            1.0.1   RSPM    1.0.1   CRAN  [1]     direct\nhighr            0.10   RSPM     0.10   CRAN  [1]   indirect\nhtmltools       0.5.5   RSPM    0.5.5   CRAN  [1]   indirect\nhttr            1.4.5   RSPM    1.4.5   CRAN  [1]     direct\nisoband         0.2.7   RSPM    0.2.7   CRAN  [1]   indirect\njquerylib       0.1.4   RSPM    0.1.4   CRAN  [1]   indirect\njsonlite        1.8.4   RSPM    1.8.4   CRAN  [1]   indirect\nknitr            1.42   RSPM     1.42   CRAN  [1]     direct\nlabeling        0.4.2   RSPM    0.4.2   CRAN  [1]   indirect\nlattice        0.21-8   CRAN   0.21-8   CRAN  [2]   indirect\nlifecycle       1.0.3   RSPM    1.0.3   CRAN  [1]   indirect\nmagrittr        2.0.3   RSPM    2.0.3   CRAN  [1]   indirect\nmemoise         2.0.1   RSPM    2.0.1   CRAN  [1]   indirect\nmethods          &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\nmgcv           1.8-42   CRAN   1.8-42   CRAN  [2]   indirect\nmime             0.12   RSPM     0.12   CRAN  [1]   indirect\nmunsell         0.5.0   RSPM    0.5.0   CRAN  [1]   indirect\nnlme          3.1-162   CRAN  3.1-162   CRAN  [2]   indirect\nnnet           7.3-18   CRAN     &lt;NA&gt;   &lt;NA&gt;  [2]       &lt;NA&gt;\nopenssl         2.0.6   RSPM    2.0.6   CRAN  [1]   indirect\npatchwork       1.1.2   RSPM    1.1.2   CRAN  [1]     direct\npillar          1.9.0   RSPM    1.9.0   CRAN  [1]   indirect\npkgconfig       2.0.3   RSPM    2.0.3   CRAN  [1]   indirect\nproxy          0.4-27   RSPM   0.4-27   CRAN  [1]   indirect\npurrr           1.0.1   RSPM    1.0.1   CRAN  [1]     direct\nrappdirs        0.3.3   RSPM    0.3.3   CRAN  [1]   indirect\nrenv           0.17.3   RSPM   0.17.3   CRAN  [1]     direct\nrlang           1.1.1   RSPM    1.1.1   CRAN  [1]   indirect\nrmarkdown        2.21   RSPM     2.21   CRAN  [1]     direct\nrpart          4.1.19   CRAN     &lt;NA&gt;   &lt;NA&gt;  [2]       &lt;NA&gt;\nrprojroot       2.0.3   RSPM    2.0.3   CRAN  [1]   indirect\ns2              1.1.3   RSPM    1.1.3   CRAN  [1]   indirect\nsass            0.4.5   RSPM    0.4.5   CRAN  [1]   indirect\nscales          1.2.1   RSPM    1.2.1   CRAN  [1]     direct\nsf             1.0-12   RSPM   1.0-12   CRAN  [1]     direct\nsignal          0.7-7   RSPM    0.7-7   CRAN  [1]     direct\nsp              1.6-0   RSPM    1.6-0   CRAN  [1]   indirect\nspatial        7.3-16   CRAN     &lt;NA&gt;   &lt;NA&gt;  [2]       &lt;NA&gt;\nsplines          &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\nstats            &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\nstringi        1.7.12   RSPM   1.7.12   CRAN  [1]   indirect\nstringr         1.5.0   RSPM    1.5.0   CRAN  [1]   indirect\nsurvival        3.5-5   CRAN     &lt;NA&gt;   &lt;NA&gt;  [2]       &lt;NA&gt;\nsys             3.4.1   RSPM    3.4.1   CRAN  [1]   indirect\nterra          1.7-29   RSPM   1.7-29   CRAN  [1]     direct\ntibble          3.2.1   RSPM    3.2.1   CRAN  [1]   indirect\ntidyr           1.3.0   RSPM    1.3.0   CRAN  [1]     direct\ntidyselect      1.2.0   RSPM    1.2.0   CRAN  [1]   indirect\ntidyterra       0.4.0   RSPM    0.4.0   CRAN  [1]     direct\ntinytex          0.45   RSPM     0.45   CRAN  [1]   indirect\ntools            &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\nunits           0.8-2   RSPM    0.8-2   CRAN  [1]   indirect\nutf8            1.2.3   RSPM    1.2.3   CRAN  [1]   indirect\nutils            &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\nvctrs           0.6.2   RSPM    0.6.2   CRAN  [1]   indirect\nviridisLite     0.4.2   RSPM    0.4.2   CRAN  [1]   indirect\nwithr           2.5.0   RSPM    2.5.0   CRAN  [1]   indirect\nwk              0.7.2   RSPM    0.7.2   CRAN  [1]   indirect\nxfun             0.39   RSPM     0.39   CRAN  [1]   indirect\nyaml            2.3.7   RSPM    2.3.7   CRAN  [1]   indirect\n\n[1]: /home/runner/work/handfull_of_pixels/handfull_of_pixels/renv/library/R-4.3/x86_64-pc-linux-gnu\n[2]: /home/runner/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/d4a79710                         \n\n# ABI ================================\n* No ABI conflicts were detected in the set of installed packages.\n\n# User Profile =======================\n[1] Source  Package Require Version Dev    \n&lt;0 rows&gt; (or 0-length row.names)\n\n# Settings ===========================\nList of 11\n $ bioconductor.version     : chr(0) \n $ external.libraries       : chr(0) \n $ ignored.packages         : chr(0) \n $ package.dependency.fields: chr [1:3] \"Imports\" \"Depends\" \"LinkingTo\"\n $ r.version                : chr(0) \n $ snapshot.type            : chr \"implicit\"\n $ use.cache                : logi TRUE\n $ vcs.ignore.cellar        : logi TRUE\n $ vcs.ignore.library       : logi TRUE\n $ vcs.ignore.local         : logi TRUE\n $ vcs.manage.ignores       : logi TRUE\n\n# Options ============================\nList of 9\n $ defaultPackages                     : chr [1:6] \"datasets\" \"utils\" \"grDevices\" \"graphics\" ...\n $ download.file.method                : NULL\n $ download.file.extra                 : NULL\n $ install.packages.compile.from.source: NULL\n $ pkgType                             : chr \"source\"\n $ repos                               : Named chr \"https://cloud.r-project.org\"\n  ..- attr(*, \"names\")= chr \"CRAN\"\n $ renv.consent                        : logi TRUE\n $ renv.project.path                   : chr \"/home/runner/work/handfull_of_pixels/handfull_of_pixels\"\n $ renv.verbose                        : logi TRUE\n\n# Environment Variables ==============\nHOME                        = /home/runner\nLANG                        = C.UTF-8\nMAKE                        = make\nR_LIBS                      = &lt;NA&gt;\nR_LIBS_SITE                 = /opt/R/4.3.0/lib/R/site-library\nR_LIBS_USER                 = /home/runner/work/handfull_of_pixels/handfull_of_pixels/renv/library/R-4.3/x86_64-pc-linux-gnu:/home/runner/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/d4a79710\nRENV_CONFIG_REPOS_OVERRIDE  = https://packagemanager.rstudio.com/cran/__linux__/jammy/latest\nRENV_DEFAULT_R_ENVIRON      = &lt;NA&gt;\nRENV_DEFAULT_R_ENVIRON_USER = &lt;NA&gt;\nRENV_DEFAULT_R_LIBS         = &lt;NA&gt;\nRENV_DEFAULT_R_LIBS_SITE    = /opt/R/4.3.0/lib/R/site-library\nRENV_DEFAULT_R_LIBS_USER    = /home/runner/work/_temp/Library\nRENV_DEFAULT_R_PROFILE      = &lt;NA&gt;\nRENV_DEFAULT_R_PROFILE_USER = &lt;NA&gt;\nRENV_PROJECT                = /home/runner/work/handfull_of_pixels/handfull_of_pixels\n\n# PATH ===============================\n- /home/runner/.local/bin\n- /opt/pipx_bin\n- /home/runner/.cargo/bin\n- /home/runner/.config/composer/vendor/bin\n- /usr/local/.ghcup/bin\n- /home/runner/.dotnet/tools\n- /snap/bin\n- /usr/local/sbin\n- /usr/local/bin\n- /usr/sbin\n- /usr/bin\n- /sbin\n- /bin\n- /usr/games\n- /usr/local/games\n- /snap/bin\n\n# Cache ==============================\nThere are a total of 80 packages installed in the renv cache.\nCache path: \"~/.cache/R/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu\""
  }
]