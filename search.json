[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Handful of Pixels",
    "section": "",
    "text": "Preface\nIn this free book I will focus on a set of examples which are rather limited in spatial scope, using just a handfull of pixels. In contrast to what the cloud computing revolution promised, many ideas start out from site or localized studies. The focus of this book on using just a handfull of pixels is therefore deliberate. This allows data to be as large as required, but as small as possible.\nNot only does this allow you to experiment with geo-spatial data on limited compute infrastructure, it also shows you that true science can be done with relatively modest means. You will learn to do big science with small data.\nThis book is proudly GPT free, mistakes are my own."
  },
  {
    "objectID": "intro.html#formal-course-work",
    "href": "intro.html#formal-course-work",
    "title": "Introduction",
    "section": "Formal course work",
    "text": "Formal course work\nAll exercises are based upon material as discussed in the book “A Handfull of Pixels: big science using small data”. The exercises are taught at Master’s level and require more advanced knowledge of the R statistical environment. A brief crash course is given in Chapter 1 . All exercise boxes in the main text are only there to make you think about the context of the processes described. You should be able to complete all exercises in Chapter 8 easily, when reading the required book chapters, external data documentation (as mentioned/linked to), or basic physical geography knowledge.\nWhen reading this book as formal course work the requested format for handing in the exercises as listed in Chapter 8 is a single R markdown file (see Section 1.5), including its rendered html file. Different exercise chapters should be divided by the subtitles used in Chapter 8.\nI will grade on the implementation of the code, as well as its critical assessment, and or trouble shooting. If you were stuck, document how you got unstuck in these assignments. If you are still stuck, list what efforts you made to get unstuck (and what failed results they yielded and how this informed your further reasoning).\nDocument problem solving extensively in between code blocks. Mention (cite) resources you consulted, packages used, etc. You will have to find the original source if you consult external resources, ChatGPT (or similar large language model chat services) are not a valid reference in and on itself.\n\n\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation in R. Chapman & Hall."
  },
  {
    "objectID": "basic_R.html#project-management",
    "href": "basic_R.html#project-management",
    "title": "1  Crash course R",
    "section": "1.1 Project management",
    "text": "1.1 Project management\nFrom a very practical and pragmatic point of view a solid workflow requires reproducible results. The smallest unit in a reproducible workflow is the structure of your personal research project (irrespective of its contributions to a larger whole). Structuring a research project.\nStructuring your project covers:\n\nthe organization of your files (i.e. code, data, manuscripts)\nfile naming conventions (how things are named)\ncode documentation (describing what and how you do things)\ndata documentation (describing data sources)\nversion control (logging progress systematically)\n\nWhen using R the easiest solution to many of these issues is to use an Integrated Development Environment (IDE, e.g. RStudio or VS Code), version control to track code changes such as git and its cloud based collaborative components such as Github or Codeberg. The setup of both IDEs is described in Appendix A.\nIt is recommended to start a new project in the RStudio IDE and provide a consistent structure grouping similar objects in folders, e.g. storing data in a data folder, your R scripts in the R folder etc. You can further select the tracking of used packages using renv when creating a new project.\n\nproject/\n├─ your_project.Rproj\n├─ vignettes/\n│  ├─ your_dynamic_document.Rmd\n├─ data/\n│  ├─ some_data.csv\n├─ R/\n│  ├─ R_scripts.R\n\nFor those familiar with github I provide a Github template which you can use when creating a new project on Github. This provides a populated project structure and removes the overhead of making decisions on how to structure a project.\n\n\n\n\n\n\nNote\n\n\n\nPay close attention to the setup of your project as an intuitive and consistent structure greatly enhances your productivity, reproducibility, replicability within a different context, and the overall quality of your work. First and foremost, your project structure, the documentation you write, and the clarity of your work, are notes to your future self. Be kind to your future self!"
  },
  {
    "objectID": "basic_R.html#basic-r",
    "href": "basic_R.html#basic-r",
    "title": "1  Crash course R",
    "section": "1.2 Basic R",
    "text": "1.2 Basic R\nUnlike many other frameworks for geocomputation, and in particular graphical geographic information system (GIS) such as ArcGIS and QGIS, geocomputation in R is uniquely code oriented. Some basic knowledge of data types, code structure and execution is therefore required. Below I give a very very short introduction to the most basic principles. For an in depth discussion on all these aspects I refer to the resources mentioned at the top of this section.\n\n1.2.1 Data types\nWithin R common data structures are vectors, list objects and data frames and tibbles, which are defined as such:\n\n# A numeric vector\nv &lt;- c(1,4,5,6)\n\n# A named list\nl &lt;- list(\n  element_a = \"a\",\n  number_2 = 2\n)\n\n# A data frame (or structured named lists)\ndf &lt;- data.frame(\n  numbers = c(1,3,2),\n  letters = c(\"a\", \"b\",\"c\")\n)\n\nNote that in R variables are assigned using the &lt;- (left arrow), however you can use = as well (which is more in line with for example python). Once assigned these elements are available for recall (from memory) an can be used for computation, or other operations. For example we can print the list using:\n\nprint(l)\n\n$element_a\n[1] \"a\"\n\n$number_2\n[1] 2\n\n\nData frames in this context represent tabulated data where the content can vary by column.\n\nprint(df)\n\n  numbers letters\n1       1       a\n2       3       b\n3       2       c\n\n\n\n\n1.2.2 Sub-setting and type conversions\nYou can access data in the above data types by referring to them by index, or in some cases by name. For example, accessing the 2th element in vector v can be accomplished by using v[2]. Element ‘a’ in list l can be accessed using:\n\nv[2]\n\n[1] 4\n\n\nNamed list or data frame elements can be accessed using their respective names and the dollar sign ($) notation using the following syntax variable + $ + element or column, e.g. :\n\n# access a named list element\nl$number_2\n\n[1] 2\n\n# access a data frame column\ndf$letters\n\n[1] \"a\" \"b\" \"c\"\n\n\n\n\n1.2.3 Basic math & operations\nAll variables are available for mathematical operations using built in functions or basic mathematical operators (+, -, *, /):\n\n# multiplying/dividing the number vector n\nv * 2\n\n[1]  2  8 10 12\n\n# dividing the number vector n\nv / 2\n\n[1] 0.5 2.0 2.5 3.0\n\n# adding a value to number vector n\nv + 2\n\n[1] 3 6 7 8\n\n# subtracting a value to number vector n\nv - 2\n\n[1] -1  2  3  4\n\n\nNote that these operations are applied to all elements of a vector (or column of a dataframe) when called without specifying a particular subset. This sweeping computation of a whole set of data is called vectorization and can be used to greatly increase computational speed by limiting the need to loop over individual variables explicitly.\nR comes with a large library of functions. A function is a pre-assigned operation which is executed on the desired variable. An example of which is the cumsum() function which calculates the cumulative sum of consecutive elements in a vector / data frame column.\n\n# calculating a cummulative sum of number vector n\ncumsum(v)\n\n[1]  1  5 10 16\n\n\n\n\n1.2.4 Functions (custom)\nIn addition to the included functions you can write your own functions in R. This allows you to automate certain routine operations particular to your project / setting.\nFor example, one can define a small function which reports back the last element of a vector:\n\n# defines a function printing\n# the last element of a vector\nlast &lt;- function(x) {\n  x[length(x)]\n}\n\nExecuting this on our vector v, will show the following result:\n\n# apply the defined function\nlast(v)\n\n[1] 6\n\n\nBe mindful of the breath of included functions in R or currently available packages. For example, within the dplyr package, which provides common data manipulation functionality, there is a function called last(). For the use of libraries see Section 1.3.\n\n\n1.2.5 Reading data\nIn the above examples I have shown only simple examples relying on internal data. However, different functions exist to read external data into R. The most basic function to read human readable (text) data into R is read.table(). Below I use a combination of write.table() and read.table() to demonstrate how to write human readable (text) data to file, and read the data back in.\n\n# create a data frame with demo data\ndf_original &lt;- data.frame(\n  col_1 = c(\"a\", \"b\", \"c\"),\n  col_2 = c(\"d\", \"e\", \"f\"),\n  col_3 = c(1,2,3)\n)\n\n# write table as CSV to disk\nwrite.table(\n  x = df_original,\n  file = file.path(tempdir(), \"your_file.csv\"),\n  sep = \",\",\n  row.names = FALSE\n)\n\n# Read a CSV file\ndf &lt;- read.table(\n  file.path(tempdir(), \"your_file.csv\"),\n  header = TRUE,\n  sep = \",\",\n  stringsAsFactors = FALSE\n)\n\nWhen dealing with large amounts of files, or files which are large in size, it might be useful to rely on the {vroom} R package. This package has the ability to read a lists of similar files (rectangular data) into a single dataframe, or deal with large data files, through lazy loading. Keep this in mind if you come across many and/or large CSV files.\n\n# a list of two identical files, but\n# can be a large list of random files\nfile_list &lt;- c(\n  file.path(tempdir(), \"your_file.csv\"),\n  file.path(tempdir(), \"your_file.csv\")\n)\n\n# read data into a single data frame\n# using the file list and the vroom\n# package\nvroom_df &lt;- vroom::vroom(\n  file_list,\n  show_col_types = FALSE\n  )\n\nAll digital data which is not represented as text characters can be considered binary data. One of the advantages of binary data is that it is an efficient representation of data, saving space. This comes at the cost of requiring a dedicated software (i.e. libraries), other than a text editor, to manipulate the data.\nIn geography and environmental sciences particular (geospatial) file formats dominate the field. Some of these file formats relate to the content of the data, some of these formats are legacy formats due to the history of the field itself. Below you find a list some of the most common formats you will encounter. For the use of (geo-spatial) libraries I refer to Section 1.3 and Chapter 3 .\n\n\n\nFile format (extension)\nFormat description\nUse case\nR Library\n\n\n\n\n*.csv\ncomma separated tabular data\nGeneral purpose flat files with row and column oriented data\nbase R\n\n\n*.txt\ntabular data with various delimiters\nGeneral purpose flat files with row and column oriented data\nbase R\n\n\n*.json\nstructured human-readable data\nGeneral purpose data format. Often used in web application. Has geospatial extensions (geojson).\njsonlite\n\n\n*.nc\nNetCDF data array data\nArray-oriented data (matrices with &gt; 2 dimensions). Commonly used to store climate data or model outputs. Alternative to HDF data.\nncdf4, terra, raster\n\n\n*.hdf\nHDF array data\nArray-oriented data (matrices with &gt; 2 dimensions). Commonly used to store Earth observation data.\nhdf\n\n\n*.tiff, *.geotiff\nGeotiff multi-dimensional raster data (see below)\nLayered (3D) raster (image) data. Commonly used to represent spatial (raster) data.\nterra, raster\n\n\n*.shp\nShapefile of vector data (see below)\nCommon vector based geospatial data. Used to describe data which can be captured by location/shape and attribute values.\nsp, sf"
  },
  {
    "objectID": "basic_R.html#sec-libraries",
    "href": "basic_R.html#sec-libraries",
    "title": "1  Crash course R",
    "section": "1.3 Libraries / packages",
    "text": "1.3 Libraries / packages\nNot all software components are included in a basic R installation. Additional components can be installed as libraries or packages from official R archives (CRAN), or github (see below). A full list of packages used in the rendering of the book and its examples can be found in Appendix C, where information on the automated installation of all required packages is provided in Appendix A.\nFor example, we can extend the capabilities of base R by installing the dplyr package from the official CRAN archive using: install.pacakges(\"dplyr\"). dplyr provides a set of functions to facilitate common data manipulation challenges covered in Section 1.4. After a successful installation you can load packages using the library() function, as library(\"dplyr\"). Functions as defined within dplyr can then be accessed in scripts or from the command line.\n\n# load the library\nlibrary(dplyr)\n\n# show the first element of the vector\nfirst(v)\n\n[1] 1\n\n\nAt times it can be useful to use the :: notation in calling package functions. The :: notation allows you to call a function without loading the full package using library(). This can be done useful if you only need one particular function, and don’t want to load the whole package (as this might have memory/performance implications). Furthermore, the use of :: makes it explicit what the source package of a particular function is. This can be helpful if packages have functions with the same name, leading to confusion on the source of the function used.\n\n# show the last element of the vector v\n# using the dplyr version of \"last\"\n# calling the function explicitly\n# avoiding confusion with our own\n# last() function\ndplyr::last(v)\n\n[1] 6\n\n\n\nExternal packages (non CRAN)\nFor installs from external sources we need the remotes package. Installing a package directly from a Github location would then be possible using for example:\n\nremotes::install_github(\"tidyverse/dplyr\")\n\nThis command loads the latest development version of dplyr from its Github location.\n\n\n\n\n\n\nWarning\n\n\n\nNote that packages on Github or elsewhere are note reviewed, and might pose security risks to your system."
  },
  {
    "objectID": "basic_R.html#sec-tidy-data",
    "href": "basic_R.html#sec-tidy-data",
    "title": "1  Crash course R",
    "section": "1.4 Tidy data",
    "text": "1.4 Tidy data\nThroughout the book I will structure data using a tidy data approach. Tidy data is a way of structuring data where every row represents a single sample and every row represents a single variable. The tidy data format is a long row orientated format. In short, in tidy data:\n\nevery column is a variable\nevery row is an observation\nevery cell is a single value\n\n\n\n\n\n\nFigure 1.1: Image by Posit Software, PBC (posit.co)\n\n\n\n\nThis format allows for easier grouped operations. As visually illustrated below, filtering data by observation properties is easier when data is rowwise oriented (Figure 1.2).\n\n\n\n\n\nFigure 1.2: Image by Posit Software, PBC (posit.co)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAny other configuration is considered “messy data”, but therefore not invalid or “bad”. Certain messy data formats might be more memory efficient.\n\n\n\n1.4.1 Tidy data conversions\nAlthough both long or wide data formats have their advantages and drawbacks, the use of long (row oriented) data allows us to use the dplyr library to iterate over data quickly and transparently (see Figure 1.2). However, large amounts of data are provided as “messy data” which is not rowwise oriented. An understanding of conversions from a wide to a long data format are therefore critical throughout this manual.\nTo convert the below “messy” data frame listing “cases” by year for three countries to a long format you can use the pivot_longer() function from the tidyr library. The tidyr library contains a set of tools for the conversion to and cleaning of data into a tidy (not messy) data format.\n\n# demo data frame with \"cases\" for\n# three countries, where the cases\n# are listed by year in a columnn by\n# column basis\ndf &lt;- data.frame(\n  country = c(\"CH\",\"BE\",\"FR\"),\n  year_2000 = c(200, 40, 1340),\n  year_2001 = c(21, 56, 5940)\n)\n\nhead(df)\n\n  country year_2000 year_2001\n1      CH       200        21\n2      BE        40        56\n3      FR      1340      5940\n\n\nThe conversion of the data looks like this, ordering the data with the yearly columns ordered in a single “year” column, the containig data in a column called “cases”. Values are taken from columns (cols) which start with “year”. Alternatively you could provide and vector with column names or column indices.\n\ntidy_df &lt;- tidyr::pivot_longer(\n  df,\n  cols = starts_with(\"year\"),\n  names_to = \"year\",\n  values_to = \"cases\"\n)\n\nhead(tidy_df)\n\n# A tibble: 6 × 3\n  country year      cases\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 CH      year_2000   200\n2 CH      year_2001    21\n3 BE      year_2000    40\n4 BE      year_2001    56\n5 FR      year_2000  1340\n6 FR      year_2001  5940\n\n\n\n\n1.4.2 Tidy data manipulations\n\nPiping data\nWhen using dplyr and tidyr libraries we can make use of the pipe concept. A pipe is a way to forward the output of one function to the input of the next, stringing together an analysis as you would read a sentence. Throughout this manuscript the native pipe (|&gt;) is used. Rewriting the previous pivot_longer() operation would yield:\n\ntidy_df &lt;- df |&gt;\n  tidyr::pivot_longer(\n  cols = starts_with(\"year\"),\n  names_to = \"year\",\n  values_to = \"cases\"\n)\n\nhead(tidy_df)\n\n# A tibble: 6 × 3\n  country year      cases\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 CH      year_2000   200\n2 CH      year_2001    21\n3 BE      year_2000    40\n4 BE      year_2001    56\n5 FR      year_2000  1340\n6 FR      year_2001  5940\n\n\nHere the data frame df has it’s output forwarded (|&gt;) to the function pivot_longer(), which will process the data according to the parameters specified. Note that the data frame is not explicitly called by the pivot_longer() function itself.\n\n\nFiltering data\nTidy data is especially convenient when filtering and selecting data. When filtering data you apply criteria rowwise using the dplyr::filter() function. For example we can filter out data based upon “year” or “cases” in our tidy_df.\n\n# filter data for year_2000\n# with more than 50 cases\ntidy_df |&gt;\n  dplyr::filter(\n    year == \"year_2000\",\n    cases &gt; 50\n  ) |&gt;\n  print()\n\n# A tibble: 2 × 3\n  country year      cases\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 CH      year_2000   200\n2 FR      year_2000  1340\n\n\nFiltering thus reduces the number of rows in your dataset. This can be visually represented as such (Figure 1.3).\n\n\n\n\n\nFigure 1.3: Image by Posit Software, PBC (posit.co)\n\n\n\n\n\n\nSelecting data\nSelecting data operates on columns instead of rows. You can select columns using the dplyr::select() function. For example, when selecting only the country code and cases counted:\n\n# only retain the country code\n# and case numbers\ntidy_df |&gt;\n  dplyr::select(\n    country,\n    cases\n  ) |&gt;\n  print()\n\n# A tibble: 6 × 2\n  country cases\n  &lt;chr&gt;   &lt;dbl&gt;\n1 CH        200\n2 CH         21\n3 BE         40\n4 BE         56\n5 FR       1340\n6 FR       5940\n\n\nThe output therefore has the same number of rows, but only outputs the selected columns.\n\n\n\n\n\nFigure 1.4: Image by Posit Software, PBC (posit.co)\n\n\n\n\n\n\nMutate & summarize\nThe tidy logic allows for easy conversions or reduction of column data using the dplyr::mutate() and dplyr::summarize() functions respectively. For example we can double the case count values using:\n\ntidy_df |&gt;\n  dplyr::mutate(\n    cases_2x = cases * 2\n  ) |&gt;\n  head()\n\n# A tibble: 6 × 4\n  country year      cases cases_2x\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 CH      year_2000   200      400\n2 CH      year_2001    21       42\n3 BE      year_2000    40       80\n4 BE      year_2001    56      112\n5 FR      year_2000  1340     2680\n6 FR      year_2001  5940    11880\n\n\nSimilarly we can calculate the mean case count across all the available data as:\n\ntidy_df |&gt;\n  dplyr::summarize(\n    mean_cases = mean(cases)\n  ) |&gt;\n  head()\n\n# A tibble: 1 × 1\n  mean_cases\n       &lt;dbl&gt;\n1      1266.\n\n\n\n\nGrouped operations\nFinally, we can apply the dplyr::mutate() and dplyr::summarize() functions across groups, defined by an index, using the dplyr::group_by() function. For example we can calculate the mean case count per year as follows:\n\ntidy_df |&gt;\n  group_by(year) |&gt;\n  dplyr::summarize(\n    mean_cases = mean(cases)\n  ) |&gt;\n  head()\n\n# A tibble: 2 × 2\n  year      mean_cases\n  &lt;chr&gt;          &lt;dbl&gt;\n1 year_2000       527.\n2 year_2001      2006.\n\n\n\n\n\n\n\nFigure 1.5: Image by Posit Software, PBC (posit.co)\n\n\n\n\nGrouped operations using dplyr::mutate() and dplyr::summarize() and other functions highlighted cover a substantial fraction of day-to-day data wrangling."
  },
  {
    "objectID": "basic_R.html#sec-rmarkdown",
    "href": "basic_R.html#sec-rmarkdown",
    "title": "1  Crash course R",
    "section": "1.5 Reporting using R markdown",
    "text": "1.5 Reporting using R markdown\nR markdown is a dynamic notebook format which allows you to mix written text (reporting) and code results. Often notebooks are used for demonstration purposes, showing the code used alternating with full text descriptions. For those familiar.\nA new R markdown file can be created using RStudio menu item:\nFile &gt; New File &gt; R markdown\nThe default settings will create an html output file. You can specify all other settings.\n\n\n\n\n\nFigure 1.6: Image by the University of Vermont\n\n\n\n\n\n1.5.1 Writing and coding\nAll markdown files have a header which specifies the output format (html if following the above instructions).\n\n\n\n\n\nFigure 1.7: Image by Posit Software, PBC (posit.co)\n\n\n\n\nYou can write text using markdown formatting, and executable code which should be contained within code chunks, starting with “```{r}” in the above image (Figure 1.7). These code chunk/block can be executed using the green “play” button on the right hand side of the code block. This will let you incrementally step through a workflow (with both documentation, and code).\n\n\n\n\n\n\nNote\n\n\n\nYou can use the here package to make references to paths consistent within an R project. When wrapping a path with here::here() you reference the file relative to the main project directory, not the current directory. This is especially helpful when using R markdown or other notebook formats such as Quarto. These notebooks render code relative to their current location (path). Notebooks in sub-directories in a project will therefore not be able to easily access data in other sub-directories. The here package resolves this issue.\n\n\n\n\n1.5.2 Rendering a project\nThe full document can be rendered as an html file (a self-contained web page), by pushing the “knitr” or “render” button at the top of the editor panel. Running all code like this requires that all necesasry data, external code and or packages are loaded.\nWhen using online data resources, such as APIs (see Section 2.2.2), it might be useful to buffer and load the data locally (while disabling the execution of the commands to get the data in the final render, the latter can be done by specifying “```{r eval = FALSE}” at the top of a code block).\n\n\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation in R. Chapman & Hall."
  },
  {
    "objectID": "gathering_data.html#sec-trust-data",
    "href": "gathering_data.html#sec-trust-data",
    "title": "2  Accessing data",
    "section": "2.1 Finding open trustworthy data",
    "text": "2.1 Finding open trustworthy data\nEnvironmental data can be found in a number of locations and a general web search will point you to them if you know the name of a data set you have in mind. If you want to broaden your search into new data products, the best sources are often governmental organizations. Governments collect and sponsor data acquisition and retention, with various administrations focusing on on particular topics, e.g. remote sensing, forestry, population statistics. For example, if you are looking for satellite remote sensing data, it makes sense to look at for example the European Space Agency (ESA) data repositories or the United States National Aeronautics and Space Administration (NASA). If you are looking for spatially explicit population statistics Eurostat might be a good starting place to start your search. Most states keep inventories of their protected areas as well as detailed forest inventories. Similarly, weather agencies on a state or European level can provide wealth of data. Directing your searches toward state level agencies will land you with reliable sources of data.\nSimilarly, non-governmental organizations (NGOs), foundations and other non-profit organizations can be a valuable source of information. General street layouts, and location based information on businesses and other venues can be sourced from Open Street Map (OSM). The World Wildlife Fund (WWF) has contributed to biodiversity mapping initiatives. In general, non-profits and NGOs are trustworthy but you should verify if sources are commonly used within the scientific literature.\nThe scientific literature can also be a valuable source of data products. However, finding these data products is often difficult as they are not necessarily centralized in a structured way or they might not even be shared publicly. Centralized repositories do exist. Noteworthy are Zenodo, a data repository for research data supported by CERN but holding vast stores of data on a variety of research topics. Similarly Dryad and Figshare provide long term storage of published research data.\nBelow you find a list of useful data sources:\n\nECMWFR Copernicus Data Services (climate data)\nCopernicus Open Access Hub (access to the Sentinel remote sensing data)\nEOSDIS Digital Active Archive Centers (DAACs)\nIntegrated Carbon Observation System (ICOS)\nNational Ecosystem Observation Network (NEON)\nScientific data repositories (open data downloads or deposits)\n\nZenodo.org\nDryad\nFigshare\n\n\nThis list is not extensive and many other sources exist. However, I will source mostly from these data sources in the book. Some familiarity with the names of these data sources is therefore helpful. Depending on where data is hosted you can simply download data through your web browser or use the internal download.file() function to grab data to store it locally.\n\n# Downloading a time series of mean CO2 levels from NOAA\n# and storing it in a temporary file\nurl &lt;- \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.csv\"\n\ndownload.file(\n  url,\n  file.path(tempdir(), \"co2.csv\")\n)"
  },
  {
    "objectID": "gathering_data.html#gathering-a-handfull-of-pixels",
    "href": "gathering_data.html#gathering-a-handfull-of-pixels",
    "title": "2  Accessing data",
    "section": "2.2 Gathering a handfull of pixels",
    "text": "2.2 Gathering a handfull of pixels\nThe sections above (Section 2.1) assume that you download data locally, on disk in a particular format. However, many of the data sources described in previous section are warehoused in large cloud facilities. These services allow you to access the underlying (original) data using an Application Programming Interfaces (APIs), hence programmatically, using code. Mastering the use of these services has become key in gathering research data sets. Given the scope of this book I will focus on ways to gather small approximately analysis ready geospatial datasets using APIs.\n\n2.2.1 Direct downloads\nBefore diving into a description of APIs, I remind you that some file reading functions in R are “web-enabled”, and can not only read local files but also remote ones (i.e., URLs). Getting ahead of ourselves a bit, the example code shows you how to read the content of a URL (of CO2 data) directly into your R environment.\nAlthough using this functionality isn’t equivalent to using an API, the concept is the same. I.e., you load a remote data source (albeit without further subsetting or passing of any parameters).\n\n# read in the data directly from URL\ndf &lt;- read.table(\n  url,\n  header = TRUE,\n  sep = \",\"\n)\n\n\n\n2.2.2 APIs\nWeb-based Application Programming Interfaces (APIs) offer a way to specify the scope of the returned data, and ultimately, the processing which goes on behind the scene in response to a (data) query. APIs are a way to, in a limited way, control a remote server to execute a certain (data) action. In most (RESTful) APIs, such query takes the form of an HTTP URL via an URL-encoded scheme using an API endpoint (or base URL).\nTo reduce some of the complexity of APIs, it is common that a wrapper is written around an API in the language of choice (e.g., R, Python). These dedicated API libraries make it easier to access data and limit coding overhead.\n\nDedicated API libraries\nAs an example of a dedicated library, we use the {MODISTools} R package which queries remote sensing data generated by the MODIS remote sensing (satellite) mission from the Oak Ridge National Laboratories data archive. A full description of their API is provided online.\n\n# load the library\nlibrary(\"MODISTools\")\n\n# list all available products\n# (only showing first part of the table for brevity)\nMODISTools::mt_products() |&gt; \n  head()\n\n       product\n1       Daymet\n2 ECO4ESIPTJPL\n3      ECO4WUE\n4       GEDI03\n5     GEDI04_B\n6      MCD12Q1\n                                                                         description\n1 Daily Surface Weather Data (Daymet) on a 1-km Grid for North America, Version 4 R1\n2               ECOSTRESS Evaporative Stress Index PT-JPL (ESI) Daily L4 Global 70 m\n3                          ECOSTRESS Water Use Efficiency (WUE) Daily L4 Global 70 m\n4                GEDI Gridded Land Surface Metrics (LSM) L3 1km EASE-Grid, Version 2\n5       GEDI Gridded Aboveground Biomass Density (AGBD) L4B 1km EASE-Grid, Version 2\n6              MODIS/Terra+Aqua Land Cover Type (LC) Yearly L3 Global 500 m SIN Grid\n  frequency resolution_meters\n1     1 day              1000\n2    Varies                70\n3    Varies                70\n4  One time              1000\n5  One time              1000\n6    1 year               500\n\n# list bands for the MOD11A2\n# product (a land surface temperature product)\nMODISTools::mt_bands(\"MOD11A2\") |&gt; \n  head()\n\n              band                          description valid_range fill_value\n1   Clear_sky_days               Day clear-sky coverage    1 to 255          0\n2 Clear_sky_nights             Night clear-sky coverage    1 to 255          0\n3    Day_view_angl View zenith angle of day observation    0 to 130        255\n4    Day_view_time        Local time of day observation    0 to 240        255\n5          Emis_31                   Band 31 emissivity    1 to 255          0\n6          Emis_32                   Band 32 emissivity    1 to 255          0\n   units scale_factor add_offset\n1   &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;\n2   &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;\n3 degree            1        -65\n4    hrs          0.1          0\n5   &lt;NA&gt;        0.002       0.49\n6   &lt;NA&gt;        0.002       0.49\n\n\nUsing this information we can now formulate a full query for use with the API. Here, I download a demo dataset specifying a location, a product, a band (subset of the product) and a date range and a geographic area (1 km above/below and left/right). Data is returned internally to the variable subset, and the progress bar of the download is not shown.\n\n# Download some data\nsubset &lt;- MODISTools::mt_subset(\n  product = \"MOD11A2\",\n  lat = 40,\n  lon = -110,\n  band = \"LST_Day_1km\",\n  start = \"2004-01-01\",\n  end = \"2004-02-01\",\n  km_lr = 1,\n  km_ab = 1,\n  internal = TRUE,\n  progress = FALSE\n)\n\n# print the dowloaded data\nhead(subset)\n\n\n\n      xllcorner  yllcorner         cellsize nrows ncols        band  units\n1.1 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n2.1 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n3.1 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n4.1 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n1.2 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n2.2 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n    scale latitude longitude     site product      start        end complete\n1.1  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n2.1  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n3.1  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n4.1  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n1.2  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n2.2  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n    modis_date calendar_date   tile     proc_date pixel value\n1.1   A2004001    2004-01-01 h09v05 2020168005635     1 13148\n2.1   A2004009    2004-01-09 h09v05 2020168010833     1 13160\n3.1   A2004017    2004-01-17 h09v05 2020168012220     1 13398\n4.1   A2004025    2004-01-25 h09v05 2020168013617     1 13412\n1.2   A2004001    2004-01-01 h09v05 2020168005635     2 13153\n2.2   A2004009    2004-01-09 h09v05 2020168010833     2 13140\n\n\nA detailed description of all functions of the MODISTools R package is beyond the scope of this course. However, the listed commands show you what a dedicated API package does. It is a shortcut to functional elements of an API. For example mt_products() allows you to quickly list all products without any knowledge of an API URL. Although more complex, the mt_subset() routine allows you to query remote sensing data for a single location (specified with a latitude lat and longitude lon), and a given date range (e.g., start, end parameters), a physical extent (in km left-right and above-below).\n\n\nGET\nDepending on your data source, you will either need to rely on a dedicated R package to query the API or study the API documentation. The general scheme for using an API follows the use of the GET() command of the {httr} R library.\nYou define a query using API parameters, as a named list.\n\n# formulate a named list query to pass to httr\nquery &lt;- list(\n  \"argument\" = \"2\",\n  \"another_argument\" = \"3\"\n)\n\nYou define the endpoint (url) where you want to query your data from.\n\n# The URL of the API (varies per product / param)\nurl &lt;- \"https://your.service.endpoint.com\"\n\nFinally, you combine both in a GET() statement to download the data from the endpoint (url).\n\n# the write_disk() function captures\n# data if available and writes it to\n# disk at location \"path\"\nstatus &lt;- httr::GET(\n  url = url,\n  query = query,\n  httr::write_disk(\n    path = \"/where/to/store/data/filename.ext\",\n    overwrite = TRUE\n  )\n)\n\nBelow, we provide an example of using the GET command to download data from the Regridded Harmonized World Soil Database (v1.2) as hosted on the Oak Ridge National Laboratory computer infrastructure. In this case we download a subset of a global map of topsoil sand content (T_SAND, %), or other paramters as listed on the ORNL DAAC data portal.\n\n# set API URL endpoint\n# for the total sand content\nurl &lt;- \"https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1247/T_SAND.nc4\"\n\n# formulate query to pass to httr\nquery &lt;- list(\n  \"var\" = \"T_SAND\",\n  \"south\" = 32,\n  \"west\" = -81,\n  \"east\" = -80,\n  \"north\" = 34,\n  \"disableProjSubset\" = \"on\",\n  \"horizStride\" = 1,\n  \"accept\" = \"netcdf4\"\n)\n\n# download data using the\n# API endpoint and query data\nstatus &lt;- httr::GET(\n  url = url,\n  query = query,\n  httr::write_disk(\n    path = file.path(tempdir(), \"T_SAND.nc\"),\n    overwrite = TRUE\n  )\n)\n\nPlotting the data downloaded shows a map of the percentage of sand in the topsoil.\n\n\nCode\n# libraries\nlibrary(terra)\nlibrary(ggplot2)\nlibrary(tidyterra)\n\nsand &lt;- terra::rast(file.path(tempdir(), \"T_SAND.nc\"))\nggplot() +\n  tidyterra::geom_spatraster(data = sand) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"sand (%)\"\n    ) +\n  theme_bw()\n\n\n\n\n\nFigure 2.1: Soil sand percentage (%)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThroughout the book, where possible, I will collapse the code which draws figures. This makes for a better reading experience. If you want to see the underlying code you can click on the “&gt; Code” line to unfold the code chunk. If not code is presented a simple plot() function call was used.\n\n\n\n\nAuthentication\nDepending on the API, authentication using a user name and a key or password is required. Then, the template should be slightly altered to accommodate for these requirements. Note that instead of the GET() command we use POST() as we need to post some authentication data before we can get the data in return.\n\n# an authenticated API query\nstatus &lt;- httr::POST(\n  url = url,\n  httr::authenticate(user, key),\n  httr::add_headers(\"Accept\" = \"application/json\",\n                    \"Content-Type\" = \"application/json\"),\n  body = query,\n  encode = \"json\"\n)"
  },
  {
    "objectID": "geospatial_R.html#the-r-geospatial-ecosystem",
    "href": "geospatial_R.html#the-r-geospatial-ecosystem",
    "title": "3  Geospatial data in R",
    "section": "3.1 The R geospatial ecosystem",
    "text": "3.1 The R geospatial ecosystem\nSpatio-temporal data often comes in the form of arrays, with space and time being array dimensions. Examples include socio-economic or demographic data, series of satellite images with multiple spectral bands, spatial simulations, and climate or weather model output.\nA number of libraries (packages) make the use of this spatio-temporal data, and geo-computational work in R easy. However, the ecosystem has grown rapidly and is therefore continuously shifting. Unlike other processing environments, this makes it at times hard to keep track of what or when to use a particular package.\nHere, a quick overview of the basic functionality and uses cases of different geospatial R packages is provided, and a brief overview of some basic geospatial operations using terra and sf libraries is given (see Section 3.1.1 and Section 3.1.2). For a more deeper dive into these packages, see Lovelace, Nowosad, and Muenchow (2019) .\n\n3.1.1 The terra package\nThe terra package is the successor of the older raster package and provides a simpler interface. This package deals with both geographic raster and vector data, with the explicit requirement that raster data represent spatially continuous processes on a fixed (rectangular) grid.\n\nReading and inspecting data\n\n# load the library\nlibrary(terra)\n\n# read data from file\nr &lt;- terra::rast(\"demo_data.nc\")\n\n\n\n\n\n\n\nNote\n\n\n\nThe demo data can be downloaded from from this link!\n\n\nWe can inspect the meta data by calling the object:\n\nprint(r)\n\nclass       : SpatRaster \ndimensions  : 41, 71, 1  (nrow, ncol, nlyr)\nresolution  : 0.1, 0.1  (x, y)\nextent      : 4.95, 12.05, 43.95, 48.05  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : demo_data.nc \nvarname     : t2m (2 metre temperature) \nname        : t2m \nunit        :   K \ntime        : 2022-01-01 12:00:00 UTC \n\n\nOr you can visualize the data by plotting the data (e.g., using plot()). Note that Figure 3.2 is generated using the ggplot2 library and is a bit more pleasing to the eye than the default plot() routine. You can also plot an interactive map using the terra function plet(), allowing you to scroll and zoom, while including various background tiles (such as satellite imagery, or topographic data).\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = r) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"2 m temp. (K) \\n\"\n    ) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n    )\n\n\n\n\n\nFigure 3.2: Temperature in Kelvin (K)\n\n\n\n\n\n\nCode\nlibrary(leaflet)\n\n# set the colour scale manually\npal &lt;- colorNumeric(\n  \"magma\",\n  values(r),\n  na.color = \"transparent\"\n  )\n\n# build the leaflet map\n# using ESRI tile servers\n# and the loaded demo raster\nleaflet() |&gt; \n  addProviderTiles(providers$Esri.WorldImagery, group = \"World Imagery\") |&gt;\n  addProviderTiles(providers$Esri.WorldTopoMap, group = \"World Topo\") |&gt;\n  addRasterImage(\n    r,\n    colors = pal,\n    opacity = 0.8,\n    group = \"raster\"\n    ) |&gt;\n  addLayersControl(\n    baseGroups = c(\"World Imagery\",\"World Topo\"),\n    position = \"topleft\",\n    options = layersControlOptions(collapsed = FALSE),\n    overlayGroups = c(\"raster\")\n    ) |&gt;\n  addLegend(\n    pal = pal,\n    values = values(r),\n    title = \"2 m temp. (K)\")\n\n\n\n\n\nFigure 3.3: Dynamic output of the generated map\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDynamic maps with the plet() function in terra leverages the leaflet library and Leafletjs javascript framework. To use dynamic maps straight from terra you require the latest leaflet package version. You can install this version by using remotes::install_github('rstudio/leaflet'). The above example shows a more advanced example, using two base maps and the demo raster data.\n\n\nDedicated functions exist to extract the layer names (names()) and the time stamps (time()) if there is a time component to the data. These functions allow you to extract these data and use them in analyses.\n\ntime(r)\n\n[1] \"2022-01-01 12:00:00 UTC\"\n\nnames(r)\n\n[1] \"t2m\"\n\n\n\n\nBasic math\nBasic math or logical operations can be performed on maps using standard R notations. As shown above the data contains temperature data in Kelvin. You can convert this data from Kelvin to Celsius by subtracting 273.15 from all values in the data set.\n\n# conversion from Kelvin to C\nr_c &lt;- r - 273.15\n\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = r_c) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"2 m temp. (deg. C) \\n\"\n    ) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n    )\n\n\n\n\n\nFigure 3.4: Temperature in Celsius (C) as calculated from the original values in Kelvin\n\n\n\n\nLogical operations work in the same way. You can create a mask of temperatures above 5\\(^\\circ\\)C using a simple logical operation.\n\n# all locations above freezing\n# as a binary mask\nm &lt;- r_c &gt; 5\n\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = m) +\n  scale_fill_viridis_d(\n    name = \"Mask \\n\"\n    ) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n    )\n\n\n\n\n\nFigure 3.5: Mask, defining positions where temperatures are larger than 5C\n\n\n\n\nYou can exclude locations from calculations using masks. This is useful to restrict the region of interest of an analysis or limit edge cases of complex calculations beforehand. As an example you can mask out all values where the binary mask as generated above is FALSE (i.e. temperatures lower than 5\\(^\\circ\\)C).\n\n# all locations above freezing\n# as a binary mask\nr_m &lt;- terra::mask(r_c, m, maskvalue = FALSE)\n\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = r_m) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"Temperature (deg. C) \\n\"\n    ) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n    )\n\n\n\n\n\nFigure 3.6: Temperature in Celsius (C), with values lower than 5 degrees C masked.\n\n\n\n\n\n\nBand math (considering multiple layers)\nGeospatial data often consists of multiple layers of information, either representing different properties (bands) or different times when data was acquired. The terra package allows you to manipulate data within and across bands.\n\n# create a multi-layer object\n# by combining objects\n# here the correcte (t2m in C)\n# and the masked values (&gt;5 C)\n# are combined\nmulti_layer &lt;- c(r_c, r_m)\n\nprint(multi_layer)\n\nclass       : SpatRaster \ndimensions  : 41, 71, 2  (nrow, ncol, nlyr)\nresolution  : 0.1, 0.1  (x, y)\nextent      : 4.95, 12.05, 43.95, 48.05  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nnames       :       t2m,       t2m \nmin values  : -6.326514,  5.003458 \nmax values  : 16.841455, 16.841455 \ntime        : 2022-01-01 12:00:00 UTC \n\n\nOperations on individual cells within terra within or across layers are governed by the *app() functions. Here, flavours such as app(), tapp(), and sapp(), are used to apply a function across all layers (to summarize data), to groups of layers (grouped summaries or complex band math), of manipulations of each individual layer with the same function (respectively).\nFor example, to calculate the mean between both layers in this multi-layer terra object one can use app() as such:\n\n# apply the mean() function across\n# all layers of the multi_layer object\nmulti_layer_mean &lt;- terra::app(multi_layer, mean, na.rm = TRUE)\n\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = multi_layer_mean) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"Temperature (deg. C) \\n\"\n    ) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n    )\n\n\n\n\n\nFigure 3.7: Temperature in Celsius (C) across layers\n\n\n\n\nIn the above analysis the mean across all layers is calculated, while passing an additional argument to the mean() function (i.e. na.rm = TRUE), stating that NA values should be ignored. Doing so fills in the values masked out previously and yields the original r_c object (i.e. the temperature values in \\(^\\circ\\)C).\n\n\nWriting and exporting data\nThe terra library uses pointers when referencing to data (in memory). This means that you can not save the object itself to resume your work later on. Saving the above masked map r_m using saveRDS(r_m, \"data.rds\") will only save a pointer to a memory space which will not exist when opening a new session. This is in contrast to for example operations on tabulated data (e.g. JSON, CSV files). As such, you need to save the output of your analysis using a formal geospatial data format using writeRaster().\nTo save masked temperature data in Celsius you would use:\n\n# save data to file\nterra::writeRaster(r_m, \"celsius_data_masked.tif\")\n\nAlternatively, but for small datasets only, you could convert the geospatial data to a long oriented data frame and save the data using standard methods to save tabulated data. However, you might loose critical meta-data on geographic projections etc. Using this method to save your work is not recommended unless you keep track of all ancillary meta-data separately.\n\n# convert geospatial data to a\n# data frame notation, where the flag\n# xy = TRUE denotes that pixel coordinate\n# details should be exported as well\ndf &lt;- as.data.frame(r, xy = TRUE)\nhead(df)\n\n    x  y      t2m\n1 5.0 48 286.4682\n2 5.1 48 286.0754\n3 5.2 48 285.6437\n4 5.3 48 285.3351\n5 5.4 48 285.0714\n6 5.5 48 284.8469\n\n\n\n\nExtracting data\nA common operation on geospatial data is the extraction of data for a given (point) location. This operation is as simple as providing a two column dataframe with longitude and latitude coordinates (or those matching the map projection). The data will be returned as an indexed dataframe. Alternatively you can also use an sf object with point, lines or polygons.\n\n# set the longitude and latitude\n# to extract\nlocations &lt;- data.frame(\n  lon = c(8,8),\n  lat = c(46, 47)\n)\n\n# extract the values for a coordinate\n# pair in the same projection as\n# the map layout\nvalues &lt;- terra::extract(\n  r_c,\n  locations\n)\n\nprint(values)\n\n  ID      t2m\n1  1 0.519267\n2  2 8.962656\n\n\nYou can also return the cell number, or coordinates (xy) with this call.\n\n# return the locations together with the\n# extracted data\nvalues &lt;- terra::extract(\n  r_c,\n  locations,\n  xy = TRUE,\n  cell = TRUE\n)\n\nprint(values)\n\n  ID      t2m cell x  y\n1  1 0.519267 1451 8 46\n2  2 8.962656  741 8 47\n\n\n\n\nTransformations\nAll geospatial raster data is defined by a coordinate reference system and a resolution defining its grid. Both the spatial resolution and projection are not always constant between datasets. However, combining two datasets requires that they share a common grid (to execute band math or other operations). It is therefore often required to resample data to a different resolution and/or reproject the data if they do not share the same coordinate reference system.\nIt is common to find data which generated for a particular region to contain regional projections, rather than intuitive geographic (latitude / longitude) projection. To align these data with a dataset with geographic coordinates the data needs to be resampled and reprojected. For example, data with a European extent can use the Albers equal area projection for representing data truthfully area wise.\nThese map projections are often referred to by either their colloquial name, but most commonly in using their Coordinate Reference System (CRS) and EPSG code. You can search for projections on various websites, such as epsg.io, which list all the most common projections. For example, I can transform the temperature data into a projection which is conformal and maintains angles (i.e. shapes, but not distance and area) by using an UTM 32N projection or EPSG:23032. For an in depth discussion on reference systems I refer to specialized courses in Geographic Information Systems (GIS).\n\nr_c_utm &lt;- terra::project(r_c, crs(\"EPSG:23032\"))\n\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = r_c_utm) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"Temperature (deg. C) \\n\"\n    ) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n    )\n\n\n\n\n\nFigure 3.8: Temperature in Celsius (C) across layers\n\n\n\n\nResampling data always needs both a source and a target raster object (i.e. two SpatRaster maps). In this operation using the resample() function you will need to specify a method on how you will approach this resampling operation. Downsampling to a coarse resolution might need a bilinear interpolation for continuous variables. However, resampling land cover maps with discrete factors (classes) might need a nearest neighbour method.\nIn the small worked example below I will first aggregate the temperature data by a factor of two (2) using the aggregate() function. This new coarse resolution map wil then serve as a target for the resample routine.\n\n# aggregate the original data by a factor of two (2)\n# using default settings\nr_c_2x &lt;- terra::aggregate(r_c, fact = 2)\n\n# resample the original data using the new aggregated grid size\n# and the minimum value across the coarser grid size\nr_resampled &lt;- terra::resample(r_c, r_c_2x, method = \"min\")\n\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = r_resampled) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"Temperature (deg. C) \\n\"\n    ) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n    )\n\n\n\n\n\nFigure 3.9: Temperature in Celsius (C) across layers\n\n\n\n\n\n\n\n3.1.2 The sf package\nSimple features are an open standard to store and access geographic data. The sf package provides a way to represent geospatial vector data as simple features in R. This results in nested data.frames or tibbles which adhere to the “tidy” data paradigm as previously described. They therefore are long oriented and support piped workflows on geometries. This standard reduces complexity and keeps geometry operations simple.\n\nReading and inspecting data\nA lot of GIS vector data comes as shapefiles (.shp extention). An example shapefile is included in the sf package, and we can read it using:\n\n# load library\nlibrary(sf)\n\n# load included shapefile\nnc &lt;- sf::st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nReading layer `nc' from data source \n  `/home/runner/.cache/R/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu/sf/1.0-12/5b41b4f0bd22b38661d82205a87deb4b/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n\nWhen printing the object you will be provided with an overview, when plotting the spatial data (using plot()) will be visualized (similar to the raster data above).\n\nprint(nc)\n\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\nFirst 10 features:\n    AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n1  0.114     1.442  1825    1825        Ashe 37009  37009        5  1091     1\n2  0.061     1.231  1827    1827   Alleghany 37005  37005        3   487     0\n3  0.143     1.630  1828    1828       Surry 37171  37171       86  3188     5\n4  0.070     2.968  1831    1831   Currituck 37053  37053       27   508     1\n5  0.153     2.206  1832    1832 Northampton 37131  37131       66  1421     9\n6  0.097     1.670  1833    1833    Hertford 37091  37091       46  1452     7\n7  0.062     1.547  1834    1834      Camden 37029  37029       15   286     0\n8  0.091     1.284  1835    1835       Gates 37073  37073       37   420     0\n9  0.118     1.421  1836    1836      Warren 37185  37185       93   968     4\n10 0.124     1.428  1837    1837      Stokes 37169  37169       85  1612     1\n   NWBIR74 BIR79 SID79 NWBIR79                       geometry\n1       10  1364     0      19 MULTIPOLYGON (((-81.47276 3...\n2       10   542     3      12 MULTIPOLYGON (((-81.23989 3...\n3      208  3616     6     260 MULTIPOLYGON (((-80.45634 3...\n4      123   830     2     145 MULTIPOLYGON (((-76.00897 3...\n5     1066  1606     3    1197 MULTIPOLYGON (((-77.21767 3...\n6      954  1838     5    1237 MULTIPOLYGON (((-76.74506 3...\n7      115   350     2     139 MULTIPOLYGON (((-76.00897 3...\n8      254   594     2     371 MULTIPOLYGON (((-76.56251 3...\n9      748  1190     2     844 MULTIPOLYGON (((-78.30876 3...\n10     160  2038     5     176 MULTIPOLYGON (((-80.02567 3...\n\n\n\n\n\n\n\nFigure 3.10: Various layers of the shapefile\n\n\n\n\nYou can extract basic information such as the overall bounding box of the vector data using st_bbox().\n\nst_bbox(nc)\n\n     xmin      ymin      xmax      ymax \n-84.32385  33.88199 -75.45698  36.58965 \n\n\nThe sf framework uses a tidy data approach. you can use operations on the list items stored within the larger data set by calculating the bounding box for each geometry.\n\nnc |&gt; \n    mutate(\n        bbox = purrr::map(geometry, sf::st_bbox)\n    )\n\nSimple feature collection with 100 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\nFirst 10 features:\n    AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n1  0.114     1.442  1825    1825        Ashe 37009  37009        5  1091     1\n2  0.061     1.231  1827    1827   Alleghany 37005  37005        3   487     0\n3  0.143     1.630  1828    1828       Surry 37171  37171       86  3188     5\n4  0.070     2.968  1831    1831   Currituck 37053  37053       27   508     1\n5  0.153     2.206  1832    1832 Northampton 37131  37131       66  1421     9\n6  0.097     1.670  1833    1833    Hertford 37091  37091       46  1452     7\n7  0.062     1.547  1834    1834      Camden 37029  37029       15   286     0\n8  0.091     1.284  1835    1835       Gates 37073  37073       37   420     0\n9  0.118     1.421  1836    1836      Warren 37185  37185       93   968     4\n10 0.124     1.428  1837    1837      Stokes 37169  37169       85  1612     1\n   NWBIR74 BIR79 SID79 NWBIR79                       geometry\n1       10  1364     0      19 MULTIPOLYGON (((-81.47276 3...\n2       10   542     3      12 MULTIPOLYGON (((-81.23989 3...\n3      208  3616     6     260 MULTIPOLYGON (((-80.45634 3...\n4      123   830     2     145 MULTIPOLYGON (((-76.00897 3...\n5     1066  1606     3    1197 MULTIPOLYGON (((-77.21767 3...\n6      954  1838     5    1237 MULTIPOLYGON (((-76.74506 3...\n7      115   350     2     139 MULTIPOLYGON (((-76.00897 3...\n8      254   594     2     371 MULTIPOLYGON (((-76.56251 3...\n9      748  1190     2     844 MULTIPOLYGON (((-78.30876 3...\n10     160  2038     5     176 MULTIPOLYGON (((-80.02567 3...\n                                       bbox\n1  -81.74107, 36.23436, -81.23989, 36.58965\n2  -81.34754, 36.36536, -80.90344, 36.57286\n3  -80.96577, 36.23388, -80.43531, 36.56521\n4  -76.33025, 36.07282, -75.77316, 36.55716\n5  -77.90121, 36.16277, -77.07531, 36.55629\n6  -77.21767, 36.23024, -76.70750, 36.55629\n7  -76.56358, 36.16973, -75.95718, 36.55606\n8  -76.95367, 36.29452, -76.46035, 36.55525\n9  -78.32125, 36.19595, -77.89886, 36.55294\n10 -80.45301, 36.25023, -80.02406, 36.55104\n\n\n\n\n3.1.2.1 Convert coordinates\nYou can easily convert a list of coordinates to an {sf} object using st_as_sf(), while specifying the columns containing the coordinates and a particular coordinate reference system.\n\n# demo data frame\ndf &lt;- data.frame(\n  lat = c(40, 41),\n  lon = c(20, 10),\n  value = c(3, 5)\n  )\n\n# sf transformation of df\n# sets coordinate columns\n# and projection epsg\ndf_sf &lt;- sf::st_as_sf(\n  df,\n  coords = c(\"lon\", \"lat\"),\n  crs = \"epsg:4326\"\n  )\n\n# print the summary of \n# the sf object\nprint(df_sf)\n\nSimple feature collection with 2 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 10 ymin: 40 xmax: 20 ymax: 41\nGeodetic CRS:  WGS 84\n  value      geometry\n1     3 POINT (20 40)\n2     5 POINT (10 41)\n\n\n\n\nReprojecting data\nYou can reproject our point data to a different coordinate system using the st_transform() function. This function requires a destination CRS. I will transform our data from EPSG 4326 to a Swiss coordinate system (EPSG ), and convert the {sf} object back to an ordinary tibble (including the transformed coordinates).\n\n# transformation to\n# swiss coordinate system\ndf_sf_swiss &lt;- df_sf |&gt;\n  st_transform(crs = \"epsg:2056\")\n\n# print the reprojected data (as sf)\nprint(df_sf_swiss)\n\nSimple feature collection with 2 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2816616 ymin: 512523.9 xmax: 3675355 ymax: 541125.9\nProjected CRS: CH1903+ / LV95\n  value                 geometry\n1     3 POINT (3675355 512523.9)\n2     5 POINT (2816616 541125.9)\n\n\nNote that the conversion back to a dataframe requires you the explicitly put the point coordinates into new columns using a mutate() call.\n\n# return to simple data frame\ndf_swiss &lt;- df_sf_swiss |&gt;\n  dplyr::as_tibble() |&gt;\n  dplyr::mutate(\n    swis_x = st_coordinates(geometry)[, \"X\"],\n    swiss_y = st_coordinates(geometry)[, \"Y\"]\n    ) |&gt;\n  dplyr::select(\n    -geometry\n  )\n\n# print coverted (dataframe) data\nprint(df_swiss)\n\n# A tibble: 2 × 3\n  value   swis_x swiss_y\n  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     3 3675355. 512524.\n2     5 2816616. 541126.\n\n\n\n\nBasic vector operations\nGiven the tidy data approach of sf data we can use the same logic to filter data. For example, if we only want to retain Camden county from the data set we can use the filter() function as shown in Section 1.4.\n\n# subset data using the\n# tidy filter approach\ncamden &lt;- nc |&gt;\n    filter(\n        NAME == \"Camden\"\n    )\n\n\n\n\n\n\nFigure 3.11: Camden county, as filtered from the larger dataset\n\n\n\n\nMost common operations on vector data are area-based logical operations. Examples include taking the intersection or union of features, which only retains the outline of a polygon. These operations can be executed on features themselves or should be mediated by geometric binary operations which can be used for filtering the original data.\nFor example the st_intersection() function calculates where and how frequent simple features overlap (interesect) with each other. The results are again a tidy simple feature which can be sorted or filtered using the standard filter() function.\n\n# generate random boxes\nm &lt;- rbind(c(0,0), c(1,0), c(1,1), c(0,1), c(0,0))\np &lt;- st_polygon(list(m))\nn &lt;- 100\nl &lt;- vector(\"list\", n)\nfor (i in 1:n)\n  l[[i]] = p + 10 * runif(2)\ns &lt;- st_sfc(l)\nsf &lt;- st_sf(s)\n\n# calculate the intersection of all simple features\ni &lt;- sf::st_intersection(sf)\n\n# filter out polygons with more than 1 overlap\n# with 1 being a self overlap\ni &lt;- i |&gt;\n    filter(\n        n.overlaps &gt; 1\n    )\n\n\n\nCode\nggplot() +\n  geom_sf(\n    data = i[\"n.overlaps\"],\n    aes(\n      fill = n.overlaps\n    )\n  ) +\n  scale_fill_viridis_c(\n    name = \"overlaps\"\n  ) +\n  theme_bw()\n\n\n\n\n\nFigure 3.12: A plot with all intetersecting vectors polygons, with the number of overlaps as a coloured index\n\n\n\n\nOther functions allow you to group overlapping features. For example, grouping of intersecting features can be done using st_union(), which only returns the outermost boundary of a feature.\n\nu &lt;- st_union(i)\n\n\n\nCode\nggplot() +\n  geom_sf(\n    data = u\n  ) +\n  theme_bw()\n\n\n\n\n\nFigure 3.13: A plot with all intetersecting vectors polygons aggregated by st_union()\n\n\n\n\nThe sf package is relatively complex and for a full understanding of its components I refer to the package documentation and the book by Lovelace, Nowosad, and Muenchow (2019).\n\n\nWriting and exporting data\nAlthough read in sf objects can be saved as internal R formats such as rds files using saveRDS(), for portability between various GIS software packages sf can write out a new shapefile using st_write().\n\n# write the north carolina data\n# to shapefile in the tempdir()\nsf::st_write(nc, file.path(tempdir(), \"nc.shp\"))\n\n\n\n\n3.1.3 The stars package\nThe stars package is another geospatial R package you should take note of. In contrast to terra, it is specifically tailored to dealing with data cubes. These are arrays of data on which one axis is time.\n\n\n\n\n\nFigure 3.14: Data cube representation by Edzer Pebesma\n\n\n\n\nUnlike for the terra package, grids in the stars package don’t have to be regular and rectangular, but can be of a different sort, such as curvi-linear data (Figure 3.15).\n\n\n\n\n\nFigure 3.15: Various non-regular grid representations of raster data, handled by the {stars} package. Image by Edzer Pebesma\n\n\n\n\nSome other distinctions should be made, such as the fast summarizing of raster data to vector features. However, in most cases, it is best to explore functions within the terra package first before considering stars.\n\n\n3.1.4 Other noteworthy packages\nOther critical packages are ncdf4 which will be installed by default, but the included functions allow for the manipulation (reading and writing) of the common netCDF format. The rstac package which provides a convenient way to browse Spatio-Temporal Asset Catalogues (STAC), a format commonly used to organize remote sensing images online. The sits package was created by the Brazilian National Institute for Space Research (INPE) and provides tools for machine learning on data cubes.\nNote that R (or Python) is infinitely flexible, and many packages exist to address niche problems within the context of geospatial data manipulation. While they most often tap into the the GDAL framework, these packages can be very powerful for certain applications which are outside the scope of this basic introduction. For more in-depth discussion, the reader is referred to the list of resources in the references at the end of this book.\n\n\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation in R. Chapman & Hall."
  },
  {
    "objectID": "phenology_trends.html#introduction",
    "href": "phenology_trends.html#introduction",
    "title": "4  Phenology trends",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nPhenology is broadly defined as seasonally recurring life cycle events, ranging from migration to seasonal plant growth. Land surface phenology is a first order control on the exchange of water and energy between the biosphere and atmosphere (Cleland et al. 2007; Lieth 2013; Piao et al. 2019; Andrew D. Richardson et al. 2010). With phenology being sensitive to temperature, it can be considered an indicator of climate change (Peñuelas and Filella 2009). Plant phenology has historically been recorded for many centuries (Menzel and Dose 2005). More recently, plant phenology (and its changes) have been recorded by networks of observers (Crimmins et al. 2017), near-surface cameras (Andrew D. Richardson 2018; Andrew D. Richardson et al. 2018), and by global satellite monitoring (Zhang et al. 2003; Ganguly et al. 2010).\n\n\n\n\n\nFigure 4.1: Split image of canopy change between winter and spring/summer at Harvard Forest, Petersham, MA, USA\n\n\n\n\nAll these (remote sensing) measurements have provided us with insights in how climate change has altered plant phenology. Overall, climate change drives temperatures up, and moves the phenology of leaf-unfolding by deciduous trees in winter-cold climates (temperate and boreal forest ecosystems) forward in time (from spring toward winter) at a rate of ~ 1 - 5 days per decade, with rates varying depending on locality and altitude (Wang et al. 2015; Vitasse et al. 2009; Menzel et al. 2006).\nConsequences are manyfold, such as exposing early blooming or leafing plants to an increased risk of late frost events and related damage to leaves (Hufkens, Friedl, Keenan, et al. 2012; Augspurger 2013; Gu et al. 2008). In short, changes to plant and land surface phenology have a profound effect on both the carbon balance and all (co-) dependent processes (Hufkens, Friedl, Keenan, et al. 2012). Therefore, it is key that we can detect and quantify how phenology changes in response to year-to-year variability in temperatures, and long-term trends related to anthropogenic climate change and global heating (Andrew D. Richardson et al. 2010; Peñuelas and Filella 2009).\nSatellite remote sensing data products provide these insights for almost four decades (Zhang et al. 2003; Hufkens, Friedl, Sonnentag, et al. 2012). Remote sensing products of land surface phenology provide wall-to-wall coverage with a relatively high level of data consistency across space and time and well-studied known (methodological) biases. This chapter covers several aspects of developing a small research project using remote sensing data of surface phenology, gathered for a small set of locations (a handfull of pixels).\nThis chapter (Chapter 4) highlights how to detect landscape-wide trends in phenology and how they relate to topography (elevation, aspect) and land cover type. This knowledge can then be used to infer some general properties of vegetation phenology. Chapter Chapter 5 demonstrates how you can detect phenological dates (timing of leaf-unfolding) from vegetation time series yourself, using a simple algorithm, and scale this regionally and globally. Note that throughout the phenology chapters I use phenology and leaf-out interchangeably (depending on the context).\nNow, let’s get started!"
  },
  {
    "objectID": "phenology_trends.html#getting-the-required-data",
    "href": "phenology_trends.html#getting-the-required-data",
    "title": "4  Phenology trends",
    "section": "4.2 Getting the required data",
    "text": "4.2 Getting the required data\nTo detect and quantify relationships between phenology and topography, we require relevant data sources. Various sources can be found online but the easiest is to use the geodata package which provides a way to access Shuttle Radar Topography Mission (SRTM) elevation data (digital elevation model, DEM) easily. The below command downloads DEM tiled data from the larger Bern area, as specified by a latitude and longitude.\n\n# load libraries\nlibrary(geodata)\n\n# download SRTM data\n# This stores file srtm_38_03.tif in tempdir()\ngeodata::elevation_3s(\n    lat = 46.6756,\n    lon = 7.85480,\n    path = tempdir()\n  )\n\n# read the downloaded data\n# use file.path() to combine\n# a directory path with a filename\ndem &lt;- terra::rast(\n  file.path(\n    tempdir(),\n    \"srtm_38_03.tif\"\n    )\n)\n\nIn this exercise, we will rely on the MODIS land surface phenology product (MCD12Q2). This remote sensing-based data product quantifies land surface phenology and is a good trade-off between data coverage (global) and precision (on a landscape scale).\nTo access this data, we will use the MODISTools package. See Chapter Chapter 2 for an in depth discussion on accessing data using APIs and the MODISTools API in particular. Data is downloaded here for the same location as used for the elevation data above.\n\n# load libraries\nlibrary(MODISTools)\n\n# download and save phenology data\nphenology &lt;- MODISTools::mt_subset(\n  product = \"MCD12Q2\",\n  lat = 46.6756,\n  lon = 7.85480,\n  band = \"Greenup.Num_Modes_01\",\n  start = \"2012-01-01\",\n  end = \"2012-12-31\",\n  km_lr = 100,\n  km_ab = 100,\n  site_name = \"swiss\",\n  internal = TRUE,\n  progress = FALSE\n)\n\n\n\n\n\n\n\nNote\n\n\n\nIt is always important to understand the data products you use. Find information on who produced the product, track down the latest literature in this respect and note what the limitations of the product are.\n\nWhat does the band name stand for?\nHow does this relate to other bands within this product?\nWhat are the characteristics of the downloaded data?\n\nis post-processing required?\n\n\nWrite down all these aspects into any report (or code) you create to ensure reproducibility.\n\n\nThe downloaded phenology data and the topography data need post-processing in our analysis. There are a number of reasons for this:\n\nMODIS data comes as a tidy data frame\nMODIS data might have missing values\nDEM data extent is larger than MODIS coverage\nTwo non-matching grids (DEM ~ MODIS)\n\nGiven that data downloaded using MODISTools is formatted as tidy data we can change corrupt or missing values into a consistent format. In the case of the MCD12Q2 product, all values larger than 32665 can be classified as NA (not available).\nThe documentation of the product also shows that phenology metrics are dates as days counted from January 1st 1970. In order to ease interpretation, we will convert these integer values, counted from 1970, to day-of-year values (using as.Date() and format()). We only consider phenological events in the first 200 days of the year, as we focus on spring. Later dates are most likely spurious.\n\n# screening of data\nphenology &lt;- phenology |&gt;\n  mutate(\n    value = ifelse(value &gt; 32656, NA, value),\n    value = as.numeric(format(as.Date(\"1970-01-01\") + value, \"%j\")),\n    value = ifelse (value &lt; 200, value, NA)\n  )\n\nBoth datasets, the DEM and MODIS data, come in two different data formats. For the ease of computation, we convert the tidy data to a geospatial (terra SpatRast) format.\n\nphenology_raster &lt;- MODISTools::mt_to_terra(\n  phenology,\n  reproject = TRUE\n)\n\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = phenology_raster) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"DOY\"\n    ) +\n  theme_bw()\n\n\n\n\n\nFigure 4.2: MODIS land surface phenology (as DOY for 2012)\n\n\n\n\nWe can now compare both data sets in a spatially explicit way, e.g. compute overlap, reproject or resample data. For example, to limit computational time, it is often wise to restrict the region of interest to an overlapping section between both data sets. This allows data to be as large as required but as small as possible. We therefore crop the DEM data to correspond to the size of the coverage of the MODIS phenology data.\n\n# crop the dem\ndem &lt;- terra::crop(\n  x = dem,\n  y = phenology_raster\n)\n\nThe grid of the DEM and MODIS data do not align. Therefore, resampling of the data to a common grid is required. We use the grid of the highest resolution data as a template for this resampling, taking the average across the extent of a MODIS pixel.\n\n# resample the dem using\n# the mean DEM value in a\n# MODIS pixel\ndem &lt;- terra::resample(\n  x = dem,\n  y = phenology_raster,\n  method = \"average\"\n)\n\n# mask the locations which\n# have no data\ndem &lt;- terra::mask(\n  dem,\n  is.na(phenology_raster),\n  maskvalues = TRUE\n)\n\nTo provide some context to our results, it might be useful to look at different responses by land cover class. In addition to phenology data, we can therefore also download the MODIS land cover data product for 2012.\n\n# download and save land cover data\nland_cover &lt;- MODISTools::mt_subset(\n  product = \"MCD12Q1\",\n  lat = 46.6756,\n  lon = 7.85480,\n  band = \"LC_Type1\",\n  start = \"2012-01-01\",\n  end = \"2012-12-31\",\n  km_lr = 100,\n  km_ab = 100,\n  site_name = \"swiss\",\n  internal = TRUE,\n  progress = FALSE\n)\n\nNow, convert this data to a geospatial format as before.\n\nland_cover_raster &lt;- MODISTools::mt_to_terra(\n  land_cover,\n  reproject = TRUE\n)"
  },
  {
    "objectID": "phenology_trends.html#sec-regression",
    "href": "phenology_trends.html#sec-regression",
    "title": "4  Phenology trends",
    "section": "4.3 Phenology trends",
    "text": "4.3 Phenology trends\nWith all data processed, we can explore some of the trends in phenology in relation to topography. Plotting the data side by side already provides some insight into expected trends.\n\n\nCode\np &lt;- ggplot() +\n  tidyterra::geom_spatraster(data = dem) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"altitude (m)\"\n    ) +\n  theme_bw()\n\np2 &lt;- ggplot() +\n  tidyterra::geom_spatraster(data = phenology_raster) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"DOY\"\n    ) +\n  theme_bw()\n\n# compositing\np + p2 + \n  plot_layout(ncol = 1) + \n  plot_annotation(\n    tag_levels = \"a\",\n    tag_prefix = \"(\",\n    tag_suffix = \")\"\n    )\n\n\n\n\n\nFigure 4.3: Panel plot showing both the Digital Elevation Model data (a) and the MODIS phenology data (b).\n\n\n\n\nWe can plot the relation between topography and the start of the season (phenology) across the scene (where data is available). Plotting this non-spatially will show a clear relation between topography (altitude) and the start of the season. With an increasing altitude, we see the start of the season being delayed. The effect is mild below 1000m and increases above this.\n\n\nCode\n# convert to data frame and merge\ndem_df &lt;- as.vector(dem)\nphenology_df &lt;- as.vector(phenology_raster)\nsct_df &lt;- data.frame(\n  altitude = dem_df,\n  doy = phenology_df\n  )\n\nggplot(\n  data = sct_df,\n      aes(\n      altitude,\n      doy\n    )\n  ) +\n  geom_hex() +\n  scale_fill_viridis_c(trans=\"log10\") +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE,\n    colour = \"white\",\n    lty = 2\n  ) +\n  labs(\n    x = \"altitude (m)\",\n    y = \"MODIS vegetation greenup (DOY)\"\n  ) +\n  theme_bw()\n\n\n\n\n\nFigure 4.4: Scatterplot of MODIS phenology values (as day-of-year, DOY) in function of altitude (m). The white dashed line is a linear regression fit to all available data.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe fit linear regression above has the following parameters.\n\n# fit a linear regression to the data of the figure above\n# (for the pre-processing see the collapsed code of the figure)\nfit &lt;- lm(doy ~ altitude, data = sct_df)\nprint(summary(fit))\n\n\nCall:\nlm(formula = doy ~ altitude, data = sct_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-154.235   -8.431    0.092    9.426   78.967 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.421e+01  8.691e-02   623.7   &lt;2e-16 ***\naltitude    3.717e-02  6.915e-05   537.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.74 on 145522 degrees of freedom\n  (44020 observations deleted due to missingness)\nMultiple R-squared:  0.6651,    Adjusted R-squared:  0.6651 \nF-statistic: 2.89e+05 on 1 and 145522 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nAugspurger, Carol K. 2013. “Reconstructing Patterns of Temperature, Phenology, and Frost Damage over 124 Years: {Spring} Damage Risk Is Increasing.” Ecology 94 (1): 41–50. https://doi.org/10.1890/12-0200.1.\n\n\nCleland, E, I Chuine, A Menzel, H Mooney, and M Schwartz. 2007. “Shifting Plant Phenology in Response to Global Change.” Trends in Ecology & Evolution 22 (7): 357–65. https://doi.org/10.1016/j.tree.2007.04.003.\n\n\nCrimmins, Theresa M., Michael A. Crimmins, Katharine L. Gerst, Alyssa H. Rosemartin, and Jake F. Weltzin. 2017. “USA National Phenology Network’s Volunteer-Contributed Observations Yield Predictive Models of Phenological Transitions.” PLoS ONE 12 (8): 1–17. https://doi.org/10.1371/journal.pone.0182919.\n\n\nGanguly, Sangram, Mark a. Friedl, Bin Tan, Xiaoyang Y Zhang, and Manish Verma. 2010. “Land Surface Phenology from MODIS: Characterization of the Collection 5 Global Land Cover Dynamics Product.” Remote Sensing of Environment 114 (8): 1805–16. https://doi.org/10.1016/j.rse.2010.04.005.\n\n\nGu, Lianhong, Paul J Hanson, W Mac Post, Dale P Kaiser, Bai Yang, Ramakrishna Nemani, Stephen G Pallardy, and Tilden Meyers. 2008. “The 2007 Eastern US Spring Freeze: Increased Cold Damage in a Warming World.” BioScience 58 (3): 253–62. https://doi.org/10.1641/b580311.\n\n\nHufkens, Koen, Mark A. Friedl, Trevor F. Keenan, Oliver Sonnentag, Amey Bailey, John O’Keefe, and Andrew D. Richardson. 2012. “Ecological Impacts of a Widespread Frost Event Following Early Spring Leaf-Out.” Global Change Biology 18 (7): 2365–77. https://doi.org/10.1111/j.1365-2486.2012.02712.x.\n\n\nHufkens, Koen, Mark Friedl, Oliver Sonnentag, Bobby H. Braswell, Thomas Milliman, and Andrew D. Richardson. 2012. “Linking Near-Surface and Satellite Remote Sensing Measurements of Deciduous Broadleaf Forest Phenology.” Remote Sensing of Environment 117 (February): 307–21. https://doi.org/10.1016/j.rse.2011.10.006.\n\n\nLieth, Helmut. 2013. Phenology and Seasonality Modeling. Vol. 8. Springer Science & Business Media.\n\n\nMenzel, Annette, and Volker Dose. 2005. “Analysis of Long-Term Time Series of the Beginning of Flowering by Bayesian Function Estimation.” Meteorologische Zeitschrift 14 (3): 429–34. https://doi.org/10.1127/0941-2948/2005/0040.\n\n\nMenzel, Annette, Tim H. Sparks, Nicole Estrella, Elisabeth Koch, Anto Aasa, Rein Ahas, Kerstin Alm-Kübler, et al. 2006. “European Phenological Response to Climate Change Matches the Warming Pattern.” Global Change Biology 12 (10): 1969–76. https://doi.org/10.1111/j.1365-2486.2006.01193.x.\n\n\nPeñuelas, J., and I. Filella. 2009. “Phenology Feedbacks on Climate Change.” Science 324 (5929): 887. http://www.sciencemag.org/content/324/5929/887.short.\n\n\nPiao, Shilong, Qiang Liu, Anping Chen, Ivan A Janssens, Yongshuo Fu, Junhu Dai, Lingli Liu, Xu Lian, Miaogen Shen, and Xiaolin Zhu. 2019. “Plant Phenology and Global Climate Change: Current Progresses and Challenges.” Global Change Biology, March. https://doi.org/10.1111/gcb.14619.\n\n\nRichardson, Andrew D. 2018. “Tracking Seasonal Rhythms of Plants in Diverse Ecosystems with Digital Camera Imagery.” New Phytologist, no. 11: 1–9. https://doi.org/10.1111/nph.15591.\n\n\nRichardson, Andrew D, T Andy Black, Philippe Ciais, Nicolas Delbart, Mark a Friedl, Nadine Gobron, David Y Hollinger, et al. 2010. “Influence of Spring and Autumn Phenological Transitions on Forest Ecosystem Productivity.” Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences 365 (1555): 3227–46. https://doi.org/10.1098/rstb.2010.0102.\n\n\nRichardson, Andrew D., Koen Hufkens, Tom Milliman, Donald M. Aubrecht, Min Chen, Josh M. Gray, Miriam R. Johnston, et al. 2018. “Tracking Vegetation Phenology Across Diverse North American Biomes Using PhenoCam Imagery.” Scientific Data 5 (1). https://doi.org/10.1038/sdata.2018.28.\n\n\nVitasse, Yann, Annabel Josée Porté, Antoine Kremer, Richard Michalet, and Sylvain Delzon. 2009. “Responses of Canopy Duration to Temperature Changes in Four Temperate Tree Species: Relative Contributions of Spring and Autumn Leaf Phenology.” Oecologia 161 (1): 187–98. https://doi.org/10.1007/s00442-009-1363-4.\n\n\nWang, Xuhui, Shilong Piao, Xiangtao Xu, Philippe Ciais, Natasha MacBean, Ranga B. Myneni, and Laurent Li. 2015. “Has the Advancing Onset of Spring Vegetation Green-up Slowed down or Changed Abruptly over the Last Three Decades?: 30-Year Change of Spring Vegetation Phenology.” Global Ecology and Biogeography 24 (6): 621–31. https://doi.org/10.1111/geb.12289.\n\n\nZhang, Xiaoyang, Mark A Friedl, Crystal B Schaaf, Alan H Strahler, John C F Hodges, Feng Gao, Bradley C Reed, and Alfredo Huete. 2003. “Monitoring Vegetation Phenology Using MODIS.” Remote Sensing of Environment 84 (3): 471–75. http://www.sciencedirect.com/science/article/B6V6V-478RS7T-1/2/19c385401eecea8964bca155ac01eb71."
  },
  {
    "objectID": "phenology_algorithms.html#algorithms",
    "href": "phenology_algorithms.html#algorithms",
    "title": "5  Phenology algorithms",
    "section": "5.1 Algorithms",
    "text": "5.1 Algorithms\n\n5.1.1 Vegetation indices and time series\nMost phenology data products rely on a limited set of algorithms, or various iterations of them, operating on time series of vegetation greenness indices. Throughout the scientific literature, we can divide most methods for detection of phenological dates used in two categories:\n\ncurve fitting\nthreshold-based methods\n\nThe curve fitting approach fits a prescribed model structure to the data, by varying a number of parameters. This functional description of the vegetation growth can then be used to derive phenology metrics by considering various inflection points of this function. Common approaches are to use the first or second derivative of a fitted function (and zero crossings) to determine the timing of the most rapid change in vegetation phenology.\n\n\n\n\n\nFigure 5.1: Example of a threshold-based phenology algorithm, by Stanimorova et al. 2019\n\n\n\n\nThe simplest approach, used with or without explicit curve-fitting, is the use of a simple threshold-based method. Here, a phenology event is registered if a given vegetation index threshold is crossed (Figure 5.1). Although scaling these products for global coverage is challenging, creating your own algorithm and implementing it on a local or regional scale is fairly simple. Below, it will be demonstrated how to calculate phenology for a vegetation index time series, for the region outlined in Chapter 4 .\nThese methods are mostly applied to various vegetation indices, which link spectral characteristics as measured by satellites to the physical development of the canopy (Myneni et al. 1995; Huete et al. 2002). For example, the Normalized Vegetation Index (NDVI) uses both the red and near-infrared spectral domain (Figure 5.2) to sense the state of the vegetation, while the Enhanced Vegetation Index (EVI, Figure 5.1) does the same while accounting for the saturation of the signal over dense canopies. This multi-spectral nature of satellite remote sensing data, as well as their temporal dynamics, can be leveraged using a veritable zoo of indices tailored to specific properties and research needs (Zeng et al. 2022). An in-depth analysis on how to use these multi-spectral data in a different context is given in Chapter 7 .\n\n\n\n\n\n\nNote\n\n\n\nGiven the non-continuous nature of the spectral bands used on most satellite platforms one calls these kinds of data multi-spectral. This in contrast to hyperspectral data which is quasi-continuous throughout a large spectral domain (Figure 5.2).\n\n\n\n\n\nFigure 5.2: Figure from Zeng et al. (2022): ‘Satellite sensors, the vegetation and the soil spectrum across wavelengths. Top: the spectral response range in the atmospheric window of a few widely used satellites109. The coloured blocks and vertical lines illustrate the spectral band range or bandpass for each satellite sensor. Bottom: reflectance of vegetation and soil. These spectral features of vegetation and soil are the foundation of the rationale of vegetation indices and support the design of various vegetation indices.’ (if no image is shown you do not have institutional access to the original article content)\n\n\n\n\n\n\n\n\n5.1.2 Acquiring demo data\nTo get started we need to download some additional information, in particular vegetation time series. In this example, and to lighten the download requirements, I will use Leaf Area Index (LAI) data instead of the more common EVI or NDVI time series used in phenology products. This is a pragmatic decision, based upon the availability of data.\n\n# download data\ndf &lt;- MODISTools::mt_subset(\n  product = \"MCD15A3H\",\n  lat = 42.536669726040884,\n  lon = -72.17951595626516,\n  band = \"Lai_500m\",\n  start = \"2002-01-01\",\n  end = \"2022-12-31\",\n  km_lr = 0,\n  km_ab = 0,\n  site_name = \"HF\",\n  internal = TRUE,\n  progress = TRUE\n)"
  },
  {
    "objectID": "phenology_algorithms.html#data-quality-control",
    "href": "phenology_algorithms.html#data-quality-control",
    "title": "5  Phenology algorithms",
    "section": "5.2 Data quality control",
    "text": "5.2 Data quality control\nMany data products have data quality flags, which allow you to screen your data for spurious values. However, in this example I will skip this step as the interpretation/use of these quality control flags is rather complex. The below methodology is therefore a toy example and further developing this algorithm would require taking into account these quality control metrics. Instead a general smoothing of the data will be applied."
  },
  {
    "objectID": "phenology_algorithms.html#data-smoothing-and-interpolation",
    "href": "phenology_algorithms.html#data-smoothing-and-interpolation",
    "title": "5  Phenology algorithms",
    "section": "5.3 Data smoothing and interpolation",
    "text": "5.3 Data smoothing and interpolation\nWe first multiply the data with their required scaling factor, transform the date values to a formal date format, and extract the individual year from the date.\n\n# convert dates to proper date formats and\n# convert the date to a single year\ndf &lt;- df |&gt;\n  mutate(\n    # scale the values correctly\n    value =  value * as.numeric(scale),\n    date = as.Date(calendar_date),\n    year = as.numeric(format(date, \"%Y\")) \n  )\n\nSmoothing can be achieved using various algorithms, such as splines, LOESS regressions, and other techniques. In this case, I will use the Savitsky-Golay filter, a common algorithm used across various vegetation phenology products. Luckily the methodology is already implemented in the signal package.\n\n# load the signal library\nlibrary(signal)\n\n# smooth this original input data using a\n# savitski-golay filter\ndf &lt;- df |&gt;\n  mutate(\n    smooth = signal::sgolayfilt(value, p = 3, n = 31)\n  )\n\nNote that the sgolayfilt() does not allow for NA values to be present to function properly. Therefore, we operate on the full data set at its original time step. Since data is only provided at a 8-day time interval, you would be limited to this time resolution in determining final phenology metrics when using a threshold-based approach.\nTo get estimates close to a daily time step, we need to expand the data to a daily time-step and merge the original smoothed data.\n\n# expand the time series to a daily time step\n# and merge with the original data\nexpanded_df &lt;- dplyr::tibble(\n  date = seq.Date(min(df$date), max(df$date), by = 1)\n)\n\n# join the expanded series with the\n# original data\ndf &lt;- dplyr::left_join(expanded_df, df)\n\n# back fill the year column for later\n# processing\ndf &lt;- df |&gt;\n  mutate(\n    year = as.numeric(format(date, \"%Y\"))\n  )\n\nExpanding the 8-day data to a 1-day timestep will result in NA values. I will use a simple linear interpolation between smoothed 8-day values to acquire a complete time series without gaps.\n\n# non NA values\nno_na &lt;- which(!is.na(df$smooth))\n\n# finally interpolate the expanded dataset\n# (fill in NA values)\ndf$smooth_int &lt;- signal::interp1(\n  x = as.numeric(df$date[no_na]),\n  y = df$smooth[no_na],\n  xi = as.numeric(df$date),\n  method = 'linear'\n)\n\n\n\nCode\nggplot(df) +\n  geom_point(\n    aes(\n      date,\n      value\n    ),\n    colour = \"red\"\n  ) +\n  geom_line(\n    aes(\n      date,\n      smooth_int\n    )\n  ) +\n  labs(\n    x = \"\",\n    y = \"LAI\"\n  ) +\n  xlim(\n    c(\n    as.Date(\"2003-01-01\"),\n    as.Date(\"2005-12-31\")\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\nFigure 5.3: Smoothed LAI values as a black line, with original values as red circles"
  },
  {
    "objectID": "phenology_algorithms.html#sec-phenology-estimates",
    "href": "phenology_algorithms.html#sec-phenology-estimates",
    "title": "5  Phenology algorithms",
    "section": "5.4 Phenology estimation",
    "text": "5.4 Phenology estimation\nWith all data prepared we can use an arbitrary threshold to estimate a transition in LAI values for a given year. In the example below, we use a LAI value of 3 to mark if the season has started or ended. To show the response to heterogeneous time series, I halved the values for the year 2004.\n\n# half the values for 2004\n# to introduce heterogeneity\ndf &lt;- df |&gt;\n  mutate(\n    smooth_int = ifelse(\n      year == 2004,\n      smooth_int/2,\n      smooth_int\n    )\n  )\n\n# calculate phenology dates on\n# the smoothed time series\nphenology &lt;- df |&gt;\n  group_by(year) |&gt;\n  summarize(\n    SOS = date[which(smooth_int &gt; 3)][1],\n    EOS = last(date[which(smooth_int &gt; 3)])\n  )\n\n\n\nCode\nggplot(df) +\n  geom_point(\n    aes(\n      date,\n      value\n    ),\n    colour = \"red\"\n  ) +\n  geom_line(\n    aes(\n      date,\n      smooth_int\n    )\n  ) +\n  labs(\n    x = \"\",\n    y = \"LAI\"\n  ) +\n  xlim(\n    c(\n    as.Date(\"2003-01-01\"),\n    as.Date(\"2005-12-31\")\n    )\n  ) +\n  geom_hline(\n    aes(yintercept = 3),\n    lty = 2\n  ) +\n  geom_vline(\n    data = phenology,\n    aes(xintercept = EOS)\n  ) +\n  geom_vline(\n    data = phenology,\n    aes(xintercept = SOS)\n  ) +\n  theme_bw()\n\n\n\n\n\nFigure 5.4: Calculated phenology dates using a fixed LAI threshold value, where the year 2004 smoothed values are artificially lowered. Smoothed LAI values as a black line, with original values as red circles. Vertical lines indicate the date when the yearly amplitude threshold of an LAI of 3 is exceeded. The dashed horizontal line illustrates that the lowered smoothed response does not meet any threshold requirement and shows no phenology response.\n\n\n\n\nObviously, this does not translate well to other locations, when vegetation types and densities vary from place to place. This is illustrated in Figure 5.4, using an artificially lowered signal in the year 2004, where years not meeting the absolute threshold will not produce accurate phenology estimates. Scaling the time series between 0 and 1 will regularize responses across time series and years and resolves this issue.\n\n# potential issues?\n# - fixed LAI threshold (varies per vegetation type)\n# - does not account for incomplete years\n# - provides absolute dates (not always helpful)\ndf &lt;- df |&gt;\n  group_by(year) |&gt;\n  mutate(\n    smooth_int_scaled = scales::rescale(\n      smooth_int,\n      to = c(0,1)\n    )\n  )\n\nNow, we can use a relative amplitude (0 - 1) across locations and years. This ensures a consistent interpretation of what phenology represents. In case of a threshold of 0.5, this would represent the halfway point between a winter baseline and a summer maximum (leaf development) of a specific year and location. Commonly, one uses multiple thresholds to characterize different phases of the vegetation development. For example, a low threshold (~0.25) characterizes the start of the growing season, while a higher (~0.85) threshold marks the end of vegetation development toward summer.\nAs before, we can now apply relative thresholds (of 0.25 and 0.85) to our time series to mark the start of the season (SOS) and the maximum crown development (MAX). We can reverse this logic and use the same thresholds on the latter part of the seasonal trajectory and calculate the start of leaf senesence (SEN) and full leaf loss, or the end-of-season (EOS).\n\n# calculate phenology dates\n# SOS: start of season (25%, start)\n# MAX: max canopy development (85%, start)\n# SEN: canopy sensesence (85%, end)\n# EOS: end of season (25%, end)\nphenology &lt;- df |&gt;\n  group_by(year) |&gt;\n  summarize(\n    SOS = date[which(smooth_int_scaled &gt; 0.25)][1],\n    MAX = date[which(smooth_int_scaled &gt; 0.85)][1],\n    SEN = last(date[which(smooth_int_scaled &gt; 0.85)]),\n    EOS = last(date[which(smooth_int_scaled &gt; 0.25)])\n  )\n\nPlotting these results for a limited set of years shows phenology dates for different years and phenology metrics (thresholds). Note that using the scaled (yearly) data estimates for the year 2004 yield robust phenology dates in spite of the artefactual multiplicative scaling that was introduced. This is in contrast to previous results as shown in Figure 5.4.\n\n\nCode\nggplot(df) +\n  geom_point(\n    aes(\n      date,\n      value\n    ),\n    colour = \"red\"\n  ) +\n  geom_line(\n    aes(\n      date,\n      smooth_int\n    )\n  ) +\n  labs(\n    x = \"\",\n    y = \"LAI\"\n  ) +\n  xlim(\n    c(\n    as.Date(\"2003-01-01\"),\n    as.Date(\"2005-12-31\")\n    )\n  ) +\n  geom_vline(\n    data = phenology,\n    aes(xintercept = SOS),\n    colour = \"lightgreen\"\n  ) +\n  geom_vline(\n    data = phenology,\n    aes(xintercept = MAX),\n    colour = \"darkgreen\"\n  ) +\n  geom_vline(\n    data = phenology,\n    aes(xintercept = SEN),\n    colour = \"brown\"\n  ) +\n  geom_vline(\n    data = phenology,\n    aes(xintercept = EOS),\n    colour = \"grey\"\n  ) +\n  theme_bw()\n\n\n\n\n\nFigure 5.5: Calculated phenology dates using a fixed LAI threshold value, where the year 2004 smoothed values are artificially lowered. Smoothed LAI values as a black line, with original values as red circles. Vertical lines indicate the date when the yearly relative amplitude thresholds are exceeded. Light green indicates the start-of-season (SOS), dark green maximum canopy development (MAX), brown the start of senesence (SEN) and grey the end of season (EOS).\n\n\n\n\n\n5.4.1 Spatial phenology estimates\nThe above example introduced the very basics of how to deal with a simple time series and develop a proof of concept. However, to scale our example spatially you need a way to process data along space and time axis. As introduced earlier, the terra package allows you to manipulate 3D data cubes (along latitude, longitude, and time axis).\n\n# Download a larger data cube\n# note that I sample a 100x100 km\n# area around the lat/lon location\nlai_2012 &lt;- MODISTools::mt_subset(\n  product = \"MCD15A3H\",\n  lat = 46.6756,\n  lon = 7.85480,\n  band = \"Lai_500m\",\n  start = \"2012-01-01\",\n  end = \"2012-12-31\",\n  km_lr = 100,\n  km_ab = 100,\n  site_name = \"swiss\",\n  internal = TRUE,\n  progress = TRUE\n)\n\n# save this data for later use\n# to speed up computation\n\nHowever, data downloaded using MODISTools by default is formated as tidy (row oriented) data. We use the mt_to_terra() function to convert this tidy data to a terra raster object.\n\n# conversion from tidy data to a raster format\nr &lt;- MODISTools::mt_to_terra(\n  lai_2012,\n  reproject = TRUE\n  )\n\nRemember the algorithm for a single time series above. To make this collection of steps re-usable within the context of a multi-layer terra raster object (or data cube) you need to define a function. This allows you to run the routine across several pixels (at once).\nBelow I wrap the steps as defined in Section 5.4 in a single function which takes a data frame as input, a phenological phase and threshold value (0 - 1) as parameters. For this example we set the parameter to 0.5, or half the seasonal amplitude.\n\nphenophases &lt;- function(\n    df,\n    return = \"start\",\n    threshold = 0.5\n) {\n\n  # split out useful info\n  value &lt;- as.vector(df)\n  \n  # if all values are NA\n  # return NA (missing data error trap)\n  if(all(is.na(value))) {\n    return(NA)\n  }\n  \n  date &lt;- as.Date(names(df))\n  \n  # pick n to be 1/3th the total length\n  # of the time series (in recorded values)\n  # n must be odd\n  n &lt;- 31\n  \n  # smooth this original input data using a\n  # savitski-golay filter\n  smooth &lt;- try(signal::sgolayfilt(\n    value,\n    p = 3,\n    n = n\n    )\n  )\n  \n  # expand the time series to a daily time step\n  # and merge with the original data\n  date_expanded &lt;- seq.Date(min(date, na.rm = TRUE), max(date, na.rm = TRUE), by = 1)\n  smooth_int &lt;- rep(NA, length(date_expanded))\n  smooth_int[which(date_expanded %in% date)] &lt;- smooth\n\n  # non NA values for interpolation\n  no_na &lt;- which(!is.na(smooth_int))\n  \n  # finally interpolate the expanded dataset\n  # (fill in NA values)\n  smooth_int &lt;- signal::interp1(\n    x = no_na,\n    y = smooth_int[no_na],\n    xi = 1:length(smooth_int),\n    'linear'\n  )\n\n  # rescale values between 0 and 1\n  smooth_int_scaled &lt;- scales::rescale(\n    smooth_int,\n    to = c(0,1)\n  )\n  \n  # thresholding for phenology detection\n  phenophase &lt;- ifelse(\n    return == \"start\",\n    date_expanded[\n      which(smooth_int_scaled &gt; threshold)\n      ][1],\n    last(\n      date_expanded[\n        which(smooth_int_scaled &gt; threshold)\n        ]\n      )\n  )\n\n  # convert to doy\n  doy &lt;- as.numeric(format(as.Date(phenophase, origin = \"1970-01-01\"),\"%j\"))\n  return(doy)\n}\n\nWith the function defined we can now apply this function to all pixels, and along the time axis (layers) of our terra raster stack. The app() function allows you to do exactly this! You can now apply the above function to the time component (z-axis, i.e. various layers) of the LAI data cube.\n\n# apply a function to the z-axis (time / layers) of a data cube\nphenology_map &lt;- app(r, phenophases)\n\nThe generated dynamic map Figure 5.6 shows the result of the algorithm and allows us to explore some of the underlying reasons for the observed patterns. For example, meadows and agricultural fields often show early leaf development, while forested areas exhibit later phenology dates. Our own new “product” shows differences with the MODIS (MCD12Q2) phenology product, which can be attributed to methodological choices.\n\n\n\n\n\n\nExercise\n\n\n\nThink of the various ways the algorithm design could change the results relative to the algorithm used in the MODIS (MCD12Q2) phenology product.\n\n\n\n\nCode\nlibrary(leaflet)\n\n# set te colour scale manually\npal &lt;- colorNumeric(\n  \"magma\",\n  values(phenology_map),\n  na.color = \"transparent\"\n  )\n\n# build the leaflet map\n# using ESRI tile servers\n# and the loaded demo raster\nleaflet() |&gt; \n  addProviderTiles(providers$Esri.WorldImagery, group = \"World Imagery\") |&gt;\n  addProviderTiles(providers$Esri.WorldTopoMap, group = \"World Topo\") |&gt;\n  addRasterImage(\n    phenology_map,\n    colors = pal,\n    opacity = 0.8,\n    group = \"Custom phenology\"\n    ) |&gt;\n  addRasterImage(\n    phenology_raster,\n    colors = pal,\n    opacity = 1,\n    group = \"MODIS phenology\"\n    ) |&gt;\n  addLayersControl(\n    baseGroups = c(\"World Imagery\",\"World Topo\"),\n    position = \"topleft\",\n    options = layersControlOptions(collapsed = FALSE),\n    overlayGroups = c(\"Custom phenology\",\"MODIS phenology\")\n    ) |&gt;\n  addLegend(\n    pal = pal,\n    values = values(phenology_map),\n    title = \"DOY\")\n\n\n\n\n\nFigure 5.6: Dynamic output of the generated phenology map. Toggle different layers to visualize the different products (MODIS or custom).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe downloading of the Leaf Area Index data can be slow. If this process takes too long you can load the data from a pre-loaded online source:\nhttps://github.com/geco-bern/handfull_of_pixels/raw/main/data/lai_2012.rds\n# load RDS data using\nr &lt;- readRDS(\"url\")\n\n\n\n\n\n\n\nHuete, A, K Didan, T Miura, E P Rodriguez, X Gao, and L G Ferreira. 2002. “Overview of the Radiometric and Biophysical Performance of the MODIS Vegetation Indices.” Remote Sensing of Environment 83 (1-2): 195–213. citeulike-article-id:274524http://dx.doi.org/10.1016/S0034-4257(02)00096-2.\n\n\nMyneni, RB, FG Hall, Piers J. Sellers, and Alexander L. Marshak. 1995. “The Interpretation of Spectral Vegetation Indexes.” Transactions on Geoscience and Remote Sensing 33 (2): 481–86. http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=377948.\n\n\nZeng, Yelu, Dalei Hao, Alfredo Huete, Benjamin Dechant, Joe Berry, Jing M. Chen, Joanna Joiner, et al. 2022. “Optical Vegetation Indices for Monitoring Terrestrial Ecosystems Globally.” Nature Reviews Earth & Environment 3 (7): 477–93. https://doi.org/10.1038/s43017-022-00298-5."
  },
  {
    "objectID": "phenology_modelling.html#introduction",
    "href": "phenology_modelling.html#introduction",
    "title": "6  Phenology modelling",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nIn the previous chapters (Chapter 4, Chapter 5), we investigated how the development of vegetation greenness throughout the seasons (that is, phenology) is associated with altitutde, and therefore with temperature, following the characteristic lapse rate. Although these preceeding chapters give an intuition on how phenology might change with global heating, they did not provide a model for predicting phenology as a function of temperature.\nLuckily, the science on phenology has a long tradition and has yielded a rich body of literature. People have been observing vegetation phenology for hundreds of years, for example through observations of cherry blossom dates in Kyoto going back to the 9th century (Aono and Kazui 2008). Furthermore, there is a good understanding of how phenology is triggered in spring and how this can be modelled. The most basic principle uses the concept of growing degree days (GDD, see below) and first mentions go back to the 18th century as described by De Reaumur (1735).\nThis growing degree day concept stipulates that spring phenology depends on the accumulated temperature above a certain threshold. If a set amount of temperature accumulation is reached, it will trigger spring phenology (leaf unfolding). A model that predicts the spring phenology as a function of a temperature time series can be considered a mechanistic model - a mathematical description of how elements of a system are connected and how processes operate (see Introduction Chapter in Applied Geodata Science). GDD-based phenology models can be considered dynamic since the temporal structure (sequence of dates in the time series) matters and a starting date and value for the temperature summation has to be specified.\n\n\n\n\n\n\nNote\n\n\n\nThis is a very dense chapter, covering both the concepts of constructing a (pseudo) mechanistic model and the basics of model parameterization. Take your time to read through the chapter multiple times to get a good grasp of the material presented."
  },
  {
    "objectID": "phenology_modelling.html#sec-phenocam",
    "href": "phenology_modelling.html#sec-phenocam",
    "title": "6  Phenology modelling",
    "section": "6.2 PhenoCam validation data",
    "text": "6.2 PhenoCam validation data\nTo illustrate the use of growing degree days in modelling spring phenology, I will use PhenoCam data. The PhenoCam network (Andrew D. Richardson 2018; Andrew D. Richardson et al. 2018) uses network connected cameras to collect seasonal trajectories of vegetation greenness, as characterized with green chromatic coordinate (GCC) values. PhenoCam data can be downloaded using the phenocamr R package (Hufkens et al. 2018).\nThe phenocamr R package uses the PhenoCam API to access the latest GCC time series and derive phenology using a threshold-based methodology similar to that described in Chapter 5. The data in Figure 6.1 should therefore be familiar. The phenocamr API call also downloads DAYMET data, which includes both daily minimum and maximum data. This ancillary data will be used in this basic modelling example below.\n\n# I will use the phenocamr package which \n# interfaces with the phenocam network API\n# to download time series of vegetation \n# greenness and derived phenology metrics\nlibrary(phenocamr)\n\n# download greenness time series,\n# calculate phenology (phenophases),\n# amend with DAYMET data\nphenocamr::download_phenocam(\n  site = \"harvard$\",\n  veg_type = \"DB\",\n  roi_id = \"1000\",\n  daymet = TRUE,\n  phenophase = TRUE,\n  trim = 2022,\n  out_dir = tempdir()\n  )\n\nharvard_phenocam_data &lt;- readr::read_csv(\n  file.path(tempdir(), \"harvard_DB_1000_3day.csv\"), \n  comment = \"#\"\n  )\n\n# reading in harvard phenology only retaining\n# spring (rising) phenology for the GCC 90th\n# percentile time series (the default)\nharvard_phenology &lt;- readr::read_csv(\n  file.path(\n    tempdir(),\n    \"harvard_DB_1000_3day_transition_dates.csv\"\n    ),\n  comment = \"#\"\n) |&gt;\n  dplyr::filter(\n    direction == \"rising\",\n    gcc_value == \"gcc_90\"\n  )\n\n\n\nCode\nggplot(harvard_phenocam_data) +\n  geom_line(\n    aes(\n      as.Date(date),\n      smooth_gcc_90\n    ),\n    colour = \"grey25\"\n  ) +\n  geom_point(\n    data = harvard_phenology,\n    aes(\n      as.Date(transition_25),\n      threshold_25\n    )\n  ) +\n  labs(\n    x = \"\",\n    y = \"GCC\"\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\nFigure 6.1: A time series of the 90th percentile 3-daily green chromatic coordinate (GCC) values for the Harvard forest EMS tower. PhenoCam GCC data are shown by the dark grey line, with threshold based (25% seasonal amplitude) derived phenology marked as black dots.\n\n\n\n\nGrowing degree days are defined as the cumulative sum of temperatures above a specified threshold (\\(T_0\\), most commonly \\(T_0 = 5^\\circ\\)C).\n\\(\\text{GDD}_{T_0,j}=\\sum_{i=M}^j \\max(T_i-T_0, 0)\\).\n\\(M\\) is the date at which the summation is initiated, and \\(\\text{GDD}_{T_0,M-1} = 0\\). In R, this can be implemented as shown below.\n\n# return mean daily temperature as well\n# as formal dates (for plotting)\nharvard_temp &lt;- harvard_phenocam_data |&gt;\n  group_by(year) |&gt;\n  dplyr::mutate(\n    tmean = (tmax..deg.c. + tmin..deg.c.)/2\n  ) |&gt; \n  dplyr::mutate(\n    date = as.Date(date),\n    gdd = cumsum(ifelse(tmean &gt;= 5, tmean - 5, 0))\n  ) |&gt;\n  dplyr::select(\n    date,\n    year,\n    tmean,\n    gdd\n  ) |&gt;\n  ungroup()\n\n# convert the harvard phenology data and only\n# retain required data\nharvard_phenology &lt;- harvard_phenology |&gt;\n  mutate(\n    doy = as.numeric(format(as.Date(transition_25),\"%j\")),\n    year = as.numeric(format(as.Date(transition_25),\"%Y\"))\n  ) |&gt;\n  select(\n    year,\n    doy,\n    transition_25,\n    threshold_25\n    )\n\nUsing this knowledge, we can plot plot the temperature time series of all days, distinguishing between days where the temperature threshold is exceeded or not, and calculate the cumulative temperature sum of days at which the temperature was above the specified threshold Figure 6.2.\n\n\nCode\n# grab only the 2010 value of spring phenology\nharvard_phenology_2010 &lt;- harvard_phenology |&gt;\n  dplyr::filter(\n    year == 2010\n  )\n\nharvard_gdd_value &lt;- harvard_temp |&gt;\n  dplyr::filter(\n    date == harvard_phenology_2010$transition_25\n  )\n\np &lt;- ggplot(harvard_temp) +\n  geom_line(\n    aes(\n      date,\n      tmean\n    )\n  ) +\n  geom_point(\n    aes(\n      date,\n      tmean,\n      colour = tmean &gt; 5,\n      group = 1\n    )\n  ) +\n  geom_vline(\n    data = harvard_phenology_2010,\n    aes(\n      xintercept = as.Date(transition_25)\n    )\n    ) +\n  scale_colour_discrete(\n    type = c(\n      \"blue\",\n      \"red\"\n      )\n  ) +\n  labs(\n    x = \"\",\n    y = \"Temperature (deg. C)\"\n  ) +\n  xlim(\n    c(\n    as.Date(\"2010-01-01\"),\n    as.Date(\"2010-06-30\")\n    )\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = \"none\"\n  )\n\np2 &lt;- ggplot(harvard_temp) +\n  geom_line(\n    aes(\n      date,\n      gdd\n    )\n  ) +\n  geom_point(\n    aes(\n      date,\n      gdd,\n      colour = tmean &gt; 5,\n      group = 1\n    )\n  ) +\n  scale_colour_discrete(\n    type = c(\n      \"blue\",\n      \"red\"\n      )\n  ) +\n  geom_vline(\n    data = harvard_phenology_2010,\n    aes(\n      xintercept = as.Date(transition_25)\n      )\n    ) +\n  geom_hline(\n    data = harvard_gdd_value,\n    aes(\n      yintercept = gdd\n    ),\n    lty = 2\n    ) +\n  labs(\n    x = \"\",\n    y = \"GDD (deg. C)\"\n  ) +\n  xlim(\n    c(\n    as.Date(\"2010-01-01\"),\n    as.Date(\"2010-06-30\")\n    )\n  ) +\n  ylim(c(0, 1000)) +\n  theme_bw()  +\n  theme(\n    legend.position = \"none\"\n  )\n\n# compositing\np + p2 + \n  plot_layout(ncol = 1) + \n  plot_annotation(\n    tag_levels = \"a\",\n    tag_prefix = \"(\",\n    tag_suffix = \")\"\n    )\n\n\n\n\n\nFigure 6.2: Example of the growing degree day (GDD) concept, showing both a temperature time series with days with a mean temperature below 5 degrees C in blue, those above as red (a), and the growing degree days (GDD) as the cummulative temperature sum of warm (red) days in the bottom panel (b). The observed spring phenology date (leaf out) and the corresponding GDD is marked with a vertical and horizontal black lines, respectively.\n\n\n\n\nFigure 6.2 shows how GDD increases during days when temperatures are above the threshold, as the season progresses. Spring leaf development in 2010 was observed on day-of-year (DOY) 114. According to the temperature record and our calculation of GDD, we found a GDD of 130.44\\(^\\circ\\)C on that day. The simplest model for the leaf-out date may thus be formulated as the day when a GDD of 130.44\\(^\\circ\\)C is reached. However, it is not guaranteed that the choice of this particular critical GDD value as the a parameter in our leaf-out model (often referred to as F* in literature) yields accurate predictions of leaf-out dates for multiple years and/or for multiple locations. To generalize our model, we have to pick a suitable value for the critical GDD (F*) such that the leaf-out date is accurately predicted across a larger set of years and/or sites when comparing to observations. Finding such a generalization is the next step in our development of a phenology model."
  },
  {
    "objectID": "phenology_modelling.html#growing-degree-day-model-optimization",
    "href": "phenology_modelling.html#growing-degree-day-model-optimization",
    "title": "6  Phenology modelling",
    "section": "6.3 Growing degree day model optimization",
    "text": "6.3 Growing degree day model optimization\nOur GDD-based leaf-out model can be written in the form of a function that takes the temperature time series as its first argument, and as two parameters the temperature threshold above which temperatures are accumulated, and the critical GDD that determines the DOY at which leaf-out is predicted.\n\ngdd_model &lt;- function(temp, par) {\n  # split out parameters from a simple\n  # vector of parameter values\n  temp_threshold &lt;- par[1]\n  gdd_crit &lt;- par[2]\n  \n  # accumulate growing degree days for\n  # temperature data\n  gdd &lt;- cumsum(ifelse(temp &gt; temp_threshold, temp - temp_threshold, 0))\n  \n  # figure out when the number of growing\n  # degree days exceeds the minimum value\n  # required for leaf development, only\n  # return the first value\n  doy &lt;- unlist(which(gdd &gt;= gdd_crit)[1])\n  \n  return(doy)\n}\n\nRunning the model on the original 2010 data with the previously observed values and parameters 5\\(^\\circ\\)C for the temperature threshold and 130.44\\(^\\circ\\)C for the critical GDD, should yield a predicted leaf-out date that matches the observed leaf-out date of 114.\n\n# confirm that the model function\n# returns expected results (i.e. DOY 114)\n# (we filter out the year 2010, but\n# removing the filter would run the\n# model for all years!)\nprediction &lt;- harvard_temp |&gt;\n  dplyr::filter(\n    year == 2010\n  ) |&gt;\n  group_by(year) |&gt;\n  summarize(\n    pred = gdd_model(\n      temp = tmean,\n      par = c(5, 130.44)\n    )  \n  )\n\nprint(prediction)\n\n# A tibble: 1 × 2\n   year  pred\n  &lt;dbl&gt; &lt;int&gt;\n1  2010   114\n\n\nHere, we picked the GDD at which the leaf-out was observed. The predicted leaf-out date, generated by a model that uses the observed GDD of the leaf-out date as its parameter for the critical GDD threshold is thus perfectly accurate. The prediction corresponds to the observed leaf-out date. However, will this parameter choice also generate accurate predictions for other years and/or sites?\n\n6.3.1 Phenology model calibration\nTo obtain accurate predictions across a larger set of years and sites, we estimate a general set of parameters for our growing degree day model, i.e., we calibrate the model parameters. To do so, we want to minimize the error (a cost function) between the model results (for a given set of parameters, i.e. the temperature threshold and critical GDD) and our observed data obtained for multiple sites and years. If you’ve taken any introduction to statistical or machine learning, this problem will sound fairly familiar. For example, in a least squares regression model calibrates two parameters - an intercept and a slope - to minimize the difference between observed and predicted values. In our example, we do not have a linear model but a more complex one with two parameters. Nevertheless, the same concept to model calibration (parameter optimization) applies.\nEstimating parameters efficiently for an arbitrary model (formulated as a function with the model parameters as one argument) can be done using various parameter optimization methods. In R, the most basic parameter optimization is implemented by the base-R nls() function, which implements a square error minimization for any (non-linear) function. Other examples include simulated annealing (GenSA R package), or Bayesian optimization methods (e.g., the BayesianTools R package). Here, we will use the GenSA R package which relies on the simulated annealing method to illustrate the estimation of model parameters.\nIn this example, the optimization minimizes a cost function which is defined as the root mean squared error (RMSE) between the observed and the predicted values.\n\n# run model and compare to true values\n# returns the RMSE\nrmse_gdd &lt;- function(par, data) {\n  \n  # split out data\n  drivers &lt;- data$drivers\n  validation &lt;- data$validation\n  \n  # calculate phenology predictions\n  # and put in a data frame\n  predictions &lt;- drivers |&gt;\n    group_by(year) |&gt;\n    summarise(\n      predictions = gdd_model(\n        temp = tmean,\n        par = par\n      )\n    )\n  \n  predictions &lt;- left_join(predictions, validation, by = \"year\")\n  \n  rmse &lt;- predictions |&gt;\n    summarise(\n      rmse = sqrt(mean((predictions - doy)^2, na.rm = TRUE))\n    ) |&gt;\n    pull(rmse)\n  \n  # return rmse value\n  return(rmse)\n}\n\n\n\n\n\n\n\nWarning\n\n\n\nBoth the cost function as the growing degree day function are not optimized for computational efficiency. Both functions are written with clarity in mind to teach the basic concepts of model optimization/parameterization. Model optimization relies on iteratively searching for the best parameter by running the model thousands of times. Slow code can make it practically impossible to converge on a solution as time requirements grow beyond what is workable. Within the context of this worked example, computational efficiency is not a concern. However, when implementing your own code, take care to optimize both functions for efficiency by vectorization and other techniques.\n\n\nDuring parameter optimization, we iteratively step through the parameter space, running the cost function (which in turn calls the main model, here gdd_model()), and find an optimal set of parameters that minimize the RMSE. Often, starting model parameters and limits to the parameter space are required. Defining the limits of the parameter space well can significantly reduce the time needed to converge on a solution, and is often bound by the physical constraints of the model. Similarly, temperature thresholds fall within a range that is determined by physiological limits of plant activity.\n\n# starting model parameters\npar = c(0, 130)\n\n# limits to the parameter space\nlower &lt;- c(-10,0)\nupper &lt;- c(45,500)\n\n# data needs to be provided in a consistent\n# single data file, a nested data structure\n# will therefore accept non standard data formats\ndata &lt;- list(\n  drivers = harvard_temp,\n  validation = harvard_phenology\n  )\n\n# optimize the model parameters\noptim_par = GenSA::GenSA(\n par = par,\n fn = rmse_gdd,\n lower = lower,\n upper = upper,\n control = list(\n   max.call = 4000\n   ),\n data = data\n)$par\n\nAfter an optimization routine (calling the cost function 4000 times) the optimal parameters were determined to be 2.811523, 228.27844 for the temperature threshold and number of accumulation days respectively. We can now plug these values back into our model and run it across all available years. When looking at this small dataset for Harvard forest, we see a reasonable agreement between observed and predicted values (Figure 6.3). Obviously, including more sites and years would increase the model performance.\n\n# run the model for all years\n# to get the phenology predictions\npredictions &lt;- harvard_temp |&gt;\n  group_by(year) |&gt;\n  summarize(\n   prediction = gdd_model(\n    temp = tmean,\n    par = optim_par\n  )  \n  )\n\n\n\nCode\n# join predicted with observed data\nvalidation &lt;- left_join(predictions, harvard_phenology)\n\nggplot(validation) +\n  geom_smooth(\n    aes(\n      doy,\n      prediction\n    ),\n    colour = \"grey25\",\n    method = \"lm\"\n  ) +\n  geom_point(\n    aes(\n      doy,\n      prediction\n    )\n  ) +\n  geom_abline(\n    intercept=0, \n    slope=1, \n    linetype=\"dotted\"\n    ) +\n  labs(\n    x = \"Observed leaf-out date (DOY)\",\n    y = \"Predicted leaf-out date (DOY)\"\n  ) +\n  theme_bw()  +\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\nFigure 6.3: Scatterplot of observed and predicted leaf-out dates as day-of-year (DOY). A linear regression is overplot as a full dark grey line with confidence intervals shown as a light grey envelope.\n\n\n\n\nMore advanced models exist. Such models may include not only temperature but also radiation, precipitation and or temporal lags or so-called chilling requirements, frost during the preceding winter months. For an overview of these more advanced models I refer to Basler (2016) and Hufkens et al. (2018)."
  },
  {
    "objectID": "phenology_modelling.html#spatial-scaling",
    "href": "phenology_modelling.html#spatial-scaling",
    "title": "6  Phenology modelling",
    "section": "6.4 Spatial scaling",
    "text": "6.4 Spatial scaling\nWith a basic model parameterized you can explore if and how the relationship between observed and predicted values scale across a larger landscape. I will use DAYMET raster data to scale the results spatially (relying on the same driver data as used during parameterization). First I download both minimum and maximum temperature data and average them to a mean daily value (as previously used), using the daymetr R package (Hufkens et al. 2018).\n\nlibrary(daymetr)\n\n# Download daily data\ndaymetr::download_daymet_tiles(\n  tiles = 11935,\n  start = 2012,\n  end = 2012,\n  param = c(\"tmin\",\"tmax\"),\n  path = paste0(here::here(), \"/data-raw/\"),\n  silent = TRUE\n  )\n\n# calculate the daily mean values\nr &lt;- daymetr::daymet_grid_tmean(\n  path = paste0(here::here(), \"/data-raw/\"),\n  product = 11935,\n  year = 2012,\n  internal = TRUE\n)\n\nFor convenience I reproject the data to geographic coordinates and limit the data to the first 180 days (layers) of the dataset to reduce the memory footprint of the calculations.\n\n# reproject to lat lon\nr &lt;- terra::project(\n  r,\n  \"+init=epsg:4326\"\n)\n\n# subset to first 180 days\nma_nh_temp &lt;- terra::subset(\n  r,\n  1:180\n)\n\nOne can then apply the model to this raster (cube) using the the terra::app() function and an appropriately formulated function, i.e. our growing degree day model gdd_model(). I refer to the terra::app() for the specifics on how to ensure your functions are compatible with raster processing, but the general rule is that the data argument comes first in the function applied (with all other parameters forwarded by name).\n\npredicted_phenology &lt;- terra::app(\n  ma_nh_temp,\n  fun = gdd_model,\n  par = optim_par\n)\n\nPlotting these results show a diverse response across the landscape, with higher elevation locations in the north west of the scene having a later leaf out date (in DOY) then coastal or urban areas (i.e. the greater Boston areas south east in the scene). This is both a reflection of the lapse rate as discussed in Chapter 4 and the urban heat island effect (Zhang, Friedl, and Schaaf 2004).\n\n\nCode\nlibrary(leaflet)\n\n# set te colour scale manually\npal &lt;- colorNumeric(\n  \"magma\",\n  values(predicted_phenology),\n  na.color = \"transparent\"\n  )\n\n# build the leaflet map\n# using ESRI tile servers\n# and the loaded demo raster\nleaflet() |&gt; \n  addProviderTiles(providers$Esri.WorldImagery, group = \"World Imagery\") |&gt;\n  addProviderTiles(providers$Esri.WorldTopoMap, group = \"World Topo\") |&gt;\n  addRasterImage(\n    predicted_phenology,\n    colors = pal,\n    opacity = 0.8,\n    group = \"Phenology model results\"\n    ) |&gt;\n  addLayersControl(\n    baseGroups = c(\"World Imagery\",\"World Topo\"),\n    position = \"topleft\",\n    options = layersControlOptions(collapsed = FALSE),\n    overlayGroups = c(\"Phenology model results\")\n    ) |&gt;\n  addLegend(\n    pal = pal,\n    values = values(predicted_phenology),\n    title = \"DOY\")\n\n\n\n\n\nFigure 6.4: Interactive map of the spatially scaled optimized growing degree model using DAYMET daily mean temperature data for tile 11935, including the greater Boston area in the south-east to the White Mountains in the north-west.\n\n\n\n\n\n\n\nAono, Yasuyuki, and Keiko Kazui. 2008. “Phenological Data Series of Cherry Tree Flowering in Kyoto, Japan, and Its Application to Reconstruction of Springtime Temperatures Since the 9th Century.” International Journal of Climatology 28 (7): 905–14. https://doi.org/10.1002/joc.1594.\n\n\nBasler, David. 2016. “Evaluating Phenological Models for the Prediction of Leaf-Out Dates in Six Temperate Tree Species Across Central Europe.” Agricultural and Forest Meteorology 217 (February): 10–21. https://doi.org/10.1016/j.agrformet.2015.11.007.\n\n\nDe Reaumur, R-AF. 1735. “Observations Du Thermometre, Faites a Paris Pendant l′annee 1735 Comparees Avec Celles Qui Onte Faites Sous La Ligne Et Al′Ile de France,a Alger Et En Quelques-Unes de Nos Iles de l′Amerique.” Memoires de l’Academie Royale Des Sciences de Paris 1735: 545–76.\n\n\nHufkens, Koen, David Basler, Tom Milliman, Eli K. Melaas, and Andrew D. Richardson. 2018. “An Integrated Phenology Modelling Framework in R.” Methods in Ecology and Evolution 9: 1–10. https://doi.org/10.1111/2041-210X.12970.\n\n\nRichardson, Andrew D. 2018. “Tracking Seasonal Rhythms of Plants in Diverse Ecosystems with Digital Camera Imagery.” New Phytologist, no. 11: 1–9. https://doi.org/10.1111/nph.15591.\n\n\nRichardson, Andrew D., Koen Hufkens, Tom Milliman, Donald M. Aubrecht, Min Chen, Josh M. Gray, Miriam R. Johnston, et al. 2018. “Tracking Vegetation Phenology Across Diverse North American Biomes Using PhenoCam Imagery.” Scientific Data 5 (1). https://doi.org/10.1038/sdata.2018.28.\n\n\nZhang, X, MA Friedl, and CB Schaaf. 2004. “Climate Controls on Vegetation Phenological Patterns in Northern Mid-and High Latitudes Inferred from MODIS Data.” Global Change Biology, no. 10: 1133–45. https://doi.org/10.1111/j.1365-2486.2004.00784.x."
  },
  {
    "objectID": "land_cover_classification.html#unsupervised-machine-learning",
    "href": "land_cover_classification.html#unsupervised-machine-learning",
    "title": "7  Land-Cover classification",
    "section": "7.1 Unsupervised machine learning",
    "text": "7.1 Unsupervised machine learning\nWorking of the concept as demonstrated in Figure 7.1 we can use the spectral and temporal information across the landscape to classify a Swiss alpine scene into locations which have little seasonality and those which have some. For example you can calculate the mean and standard deviation of a full year and see how much variability you see across a year. Regions with a low LAI signal with little variability are most likely not associated with vegetation (e.g. glaciers, see Figure 7.1). Classification of data in different classes (or clustering) can be accomplished using various methods. Clustering can either be unsupervised, where clustering is only defined by the number of classes one wants to divide the (multi-dimensional) dataset into, with no reference data to compare the results to.\nIn this example I use an unsupervised machine learning approach, called k-means clustering, to divide the dataset into two or more classes. These classes are clustered in a way which minimizes within-cluster variances, i.e. it ensures that pixels will look similar to each other (given a target number of clusters k to divide the dataset into).\nHere, we can use the lai_2012 dataset we previously downloaded, but we’ll use the raster representation as a starting point (as most geospatial data will come in multi-layer raster formats).\n\n# conversion from tidy data to a raster format\n# as it is common to use raster data\nr &lt;- MODISTools::mt_to_terra(\n  lai_2012,\n  reproject = TRUE\n  )\n\nAs a first step (ironically) I will convert this raster object back into a dataframe. However, this time it will be a wide data frame, where every pixel location is a row and every column a value for a given date (see Chapter 1). Alternatively, I could have converted the original lai_2012 data frame from a long format into a wide format using tidyr::pivot_wider(). Every row, representing a year for a given location, is a feature (vector) which contains the information on which the k-means clustering algorithm will operate.\n\n# convert a multi-layer raster image\n# to wide dataframe\ndf &lt;- as.data.frame(r, cell = TRUE)\n\n# the content of a single feature (vector)\n# limited to the first 5 values for brevity\nprint(df[1,1:5])\n\n   cell 2012-01-01 2012-01-05 2012-01-09 2012-01-13\n43   43   0.693547   1.309657 0.01870939   1.106453\n\n\nWe can now use the kmeans() algorithm to classify the data into two distinct groups or centers (k = 2). Note that we drop the first column from our dataframe as this contains the pixel indices, which are needed later on.\n\n# cluster the data \nclusters &lt;- kmeans(\n  df[,-1],\n  centers = 2\n)\n\nFinally, we map the cell values back onto those of the original extent of the LAI data (to retain their respective geographic position).\n\n# use the original raster layout as\n# a template for the new map (only\n# using a single layer)\nkmeans_map &lt;- terra::rast(r, nlyr=1)\n\n# assign to each cell value (location) of this\n# new map using the previously exported cell\n# values (NA values are omitted so a 1:1\n# mapping would not work)\nkmeans_map[df$cell] &lt;- clusters$cluster\n\n\n\nCode\nlibrary(leaflet)\n\n# set te colour scale manually\npalcol &lt;- colorFactor(\n  c(\"#78d203\", \"#f9ffa4\"),\n  domain = 1:2,\n  na.color = \"transparent\"\n  )\n\n# build the leaflet map\nleaflet() |&gt; \n  addProviderTiles(providers$Esri.WorldImagery, group = \"World Imagery\") |&gt;\n  addProviderTiles(providers$Esri.WorldTopoMap, group = \"World Topo\") |&gt;\n  addRasterImage(\n    kmeans_map,\n    colors = palcol,\n    opacity = 0.5,\n    method = \"ngb\",\n    group = \"k-means cluster results\"\n    ) |&gt;\n  addLayersControl(\n    baseGroups = c(\"World Imagery\",\"World Topo\"),\n    position = \"topleft\",\n    options = layersControlOptions(collapsed = FALSE),\n    overlayGroups = c(\"k-means cluster results\")\n    ) |&gt;\n  addLegend(\n    colors = palcol(1:2),\n    values = c(1, 2),\n    title = \"cluster\",\n    labels = c(1, 2)\n    )\n\n\n\n\n\nFigure 7.2: k-means classification map for two clusters and one year (2012) of leaf area index (LAI) data.\n\n\n\nAs the example uses an unsupervised classification we do not know what land cover types are included in this map. The model that we generate is therefore purely data informed, and not validated against external known Land-Use and Land-Cover locations. However, a quick visual inspection shows that for a k of 2 the split between the clusters divides vegetation from glaciers, water bodies, and urban areas (Figure 7.2). The (seasonal) differences in LAI were used in the k-means analysis to minimize the (seasonal) variance between pixels. In particular, our analysis with two classes separates areas with a seasonal dynamic from those without one. Although the k-means algorithm is fast it only has one parameter which can shape the outcome of algorithm (i.e. k). The model is therefore too inflexible for more complex classification tasks. The k-means model is therefore rarely used as a scalable solution to generate maps based upon in-situ validation data (see below).\nIn this example I used an index, i.e. LAI, which does not provide sufficient information to distinguish between more subtle Land-Use or Land-Cover classes (e.g. evergreen forests and or mixed forest types). In short, a better approach would use more data and a more sophisticated model approach to create an informed model which scales easily to different land cover types and new locations."
  },
  {
    "objectID": "land_cover_classification.html#supervised-machine-learning",
    "href": "land_cover_classification.html#supervised-machine-learning",
    "title": "7  Land-Cover classification",
    "section": "7.2 supervised machine learning",
    "text": "7.2 supervised machine learning\nIn contrast to k-means, one can use supervised classification of data where reference data is used as a benchmark on which to train a machine learning algorithm. A supervised machine learning algorithm will try to minimize the classification error on a known training or validation dataset. The methodology, model complexity and parameters are dependent on the provided data, the complexity of the problem and at times the available computational resources.\nFor example the MODIS land-cover and land-use product (MCD12Q2) use &gt;1500 validation locations, three years of temporal data and a variety of multiple spectral bands, as well as ancillary data (topography, view angle geometry) to train their boosted classification tree Friedl et al. (2010). In the below example I will try to recreate a part of the MCD12Q2 LULC workflow (red boxes, Figure 7.3). A full introduction to machine learning is beyond the scope of this course and I refer to Boehmke and Greenwell (2020), Kuhn and Silge (2022) and Stocker et al. (2023) for an introduction in machine learning using R.\n\n\n\n\n\nFigure 7.3: The full MODIS MCD12Q1 LULC classification workflow (after Friedl et al. 2010). The red boxes highlights the section covered in the worked example.\n\n\n\n\n\n7.2.1 Validation data\nCritical in our exercise is the reference data we use to classify different Land-Use and Land-Cover types with. Generally ground truth data is used in order to create a training dataset. This ground truth data are locations which are visually validated to belong to a particular Land-Use or Land-Cover class. These data are gathered by in-situ surveys, or leverage high(er) resolution data and visual interpretation to confirm the the Land-Use or Land-Cover type.\nFor example, the LUCAS database of the European Commission Joint Research Center provides three-yearly survey data on the Land-Use and Land-Cover state (d’Andrimont et al. 2020), while the Geo-Wiki project used crowdsourcing to confirm Land-Use and Land-Cover types using high resolution remote sensing imagery (Fritz et al. 2017). Other products use in house databases using similar approaches (Friedl et al. 2010). In this example, I will rely on the freely available crowdsourced (Geo-Wiki) dataset (Fritz et al. 2017). These validation labels which differ slightly from the International Geosphere-Biosphere Programme (IGBP) classes used in the MCD12Q1 product, but the general principles are the same.\n\n# Read the validation sites from\n# Fritz et al. 2017 straight from\n# Zenodo.org\nvalidation_sites &lt;- readr::read_csv(\n  \"https://zenodo.org/record/6572482/files/Global%20LULC%20reference%20data%20.csv?download=1\"\n)\n\nI will restrict the number of validation sites to a manageable number of 150 random locations for each Land-Use or Land-Cover class, limited to high quality locations on the northern hemisphere (as we will apply our analysis regionally to Switzerland). The training data in Fritz et al. (2017) contains various crowdsourcing competitions of which nr. 1 refers to land-use and nr. 4 to land-cover classes. The below routine shows how to gather 150 random locations for each Land-Use or Land-Cover class from Fritz et al. (2017). The locations of this data will be used to train and test the supervised machine learning algorithm.\n\n# filter out data by competition,\n# coverage percentage and latitude\n# (use round brackets to enclose complex\n# logical statements in a filter call!)\nvalidation_selection &lt;- validation_sites |&gt;\n    dplyr::filter(\n      (competition == 4 | competition == 1),\n      perc1 &gt; 80,\n      lat &gt; 0\n    )\n\n# the above selection includes all data\n# but we now subsample to 150 random locations\n# per (group_by()) land cover class (LC1)\n# set a seed for reproducibilty\nset.seed(0)\n\nvalidation_selection &lt;- validation_selection |&gt;\n    dplyr::slice_sample(n = 150, by = LC1)\n\n# split validation selection\n# by land cover type into a nested\n# list, for easier processing\n# later on\nvalidation_selection &lt;- validation_selection |&gt;\n    dplyr::group_by(LC1) |&gt;\n    dplyr::group_split()\n\n\n\n\n\n\n\nNote\n\n\n\nThe samples are generated randomly, although a random seed ensures some reproducibility it is wise to save your site selection on file.\n\n\n\n\n7.2.2 Multi-spectral training data\nAs with our k-means clustering example we need data to inform our supervised classification. In the interest of time, we gather MODIS data for only four spectral bands (1 to 7) from the MCD43A4 data product. This is a subset of what is used in the formal MCD12Q1 LULC product. The MCD43A4 data product provides daily multi-spectral data corrected for (geometric) view angle effects between the satellite and the sun.\nThe required MCD43A4 data is not readily available using the {MODISTools} R package, as previously introduced, and I will therefore rely on the {appears} R package (Hufkens 2023) to query and download training data. The {appears} package relies on the NASA AppEEARS API service which provides easy access to remote sensing data subsets similar to the ORNL DAAC service, as used by {MODISTools}. The provided data covers more data products, but does require a login (API key) limiting ad-hoc accessibility.\n\n\n\n\n\n\nWarning\n\n\n\nI refer to the {appears} documentation for the setup and use of an API key. The instructions below assume you have registered and have a working key installed in your R session.\n\n\n\n# for every row download the data for this\n# location and the specified reflectance\n# bands\ntask_nbar &lt;- lapply(validation_selection, function(x){\n  \n  # loop over all list items (i.e. land cover classes)\n  base_query &lt;- x |&gt;\n    dplyr::rowwise() |&gt;\n    do({\n      data.frame(\n        task = paste0(\"nbar_lc_\",.$LC1),\n        subtask = as.character(.$pixelID),\n        latitude = .$lat,\n        longitude = .$lon,\n        start = \"2012-01-01\",\n        end = \"2012-12-31\",\n        product = \"MCD43A4.061\",\n        layer = paste0(\"Nadir_Reflectance_Band\", 1:4)\n      )\n    }) |&gt;\n    dplyr::ungroup()\n  \n  # build a task JSON string \n  task &lt;- rs_build_task(\n    df = base_query\n  )\n  \n  # return task\n  return(task)\n})\n\n# Query the appeears API and process\n# data in batches - this function\n# requires an active API session/login\nrs_request_batch(\n  request = task_nbar,\n  workers = 10,\n  user = \"your_api_id\",\n  path = tempdir(),\n  verbose = TRUE,\n  time_out = 28800\n)\n\n\n\n\n\n\n\nNote\n\n\n\nAppEEARS downloads might take a while! In the code above the default directory is also set to the temporary directory. To use the above code make sure to change the download path to a more permanent one.\n\n\nWith both training and model validation data downloaded we can now train a supervised machine learning model! We do have to wrangle the data into a format that is acceptable for machine learning tools. In particular, we need to convert the data from a long format to a wide format (see Section 1.4.1), where every row is a feature vector. The {vroom} package is used to efficiently read in a large amount of similar CSV files into a large dataframe using a single list of files (alternatively you can loop over and append files using base R).\n\n# list all MCD43A4 files, note that\n# that list.files() uses regular\n# expressions when using wildcards\n# such as *, you can convert general\n# wildcard use to regular expressions\n# with glob2rx()\nfiles &lt;- list.files(\n  tempdir(),\n  glob2rx(\"*MCD43A4-061-results*\"),\n  recursive = TRUE,\n  full.names = TRUE\n)\n\n# read in the data (very fast)\n# with {vroom} and set all\n# fill values (&gt;=32767) to NA\nnbar &lt;- vroom::vroom(files)\nnbar[nbar &gt;= 32767] &lt;- NA\n\n# retain the required data only\n# and convert to a wide format\nnbar_wide &lt;- nbar |&gt;\n  dplyr::select(\n    Category,\n    ID,\n    Date,\n    Latitude,\n    Longitude,\n    starts_with(\"MCD43A4_061_Nadir\")\n  ) |&gt;\n  tidyr::pivot_wider(\n    values_from = starts_with(\"MCD43A4_061_Nadir\"),\n    names_from = Date\n  )\n\n# split out only the site name,\n# and land cover class from the\n# selection of validation sites\n# (this is a nested list so we\n# bind_rows across the list)\nsites &lt;- validation_selection |&gt;\n  dplyr::bind_rows() |&gt;\n  dplyr::select(\n    pixelID,\n    LC1\n  ) |&gt;\n  dplyr::rename(\n    Category = \"pixelID\"\n  )\n\n# combine the NBAR and land-use\n# land-cover labels by location\n# id (Category)\nml_df &lt;- left_join(nbar_wide, sites) |&gt;\n    dplyr::select(\n    LC1,\n    contains(\"band\")\n  )\n\n\n\n7.2.3 Model training\nThis example will try to follow the original MODIS MCD12Q1 workflow closely which calls for a boosted regression classification approach (Friedl et al. 2010). This method allows for the use of a combination of weak learners to be combined into a single robust ensemble classification models. The in depth discussion of the algorithm is beyond the scope of this course and I refer to specialized literature for more details. To properly evaluate or model we need to split our data in a true training dataset, and a test dataset. The test dataset will be used in the final model evaluation, where samples are independent of those contained within the training dataset (Boehmke and Greenwell 2020).\n\n# select packages\n# avoiding tidy catch alls\nlibrary(rsample)\n\n# create a data split across\n# land cover classes\nml_df_split &lt;- ml_df |&gt;\n  rsample::initial_split(\n  strata = LC1,\n  prop = 0.8\n)\n\n# select training and testing\n# data based on this split\ntrain &lt;- rsample::training(ml_df_split)\ntest &lt;- rsample::testing(ml_df_split)\n\nWith both a true training and testing dataset in place we can start to implement our supervised machine learning model. I will use a “tidy” machine learning modelling approach. Similar to the data management practices described in Chapter 1, a tidy modelling approach relies mostly on sentence like commands using a pipe (|&gt;) and workflows (recipes). This makes formulating models and their evaluation more intuitive for many. For an in depth discussion on tidy modelling in R I refer to Kuhn and Silge (2022). Now, let’s get started.\n\nModel structure and workflow\nTo specify our model I will use the {parsnip} R package which goal it is “… to provide a tidy, unified interface to models that can be used to try a range of models without getting bogged down in the syntactical minutiae of the underlying packages”. Parsnip therefore will remove some of the complexity of using a package such as {xgboost} might present. It also allows you to switch between different model structures with ease, only swapping a few arguments. Parsnip will make sure that model parameters are correctly populated and forwarded.\nFollowing (Friedl et al. 2010) we will implement a boosted regression (classification) regression tree using the {xgboost} R package, via the convenient {parsnip} R package interface. The model allows for a number of hyper-parameters, i.e. parameters which do not define the base {xgboost} algorithm but the implementation and structure of it. In this example hyper-parameters for the number of trees is fixed at 50, while the tree depth is flexible (parameters marked tune(), see below).\n\n\n\n\n\n\nNote\n\n\n\nThere are many more hyper-parameters. For brevity and speed these are kept at a minimum. However, model performance might increase when tuning hyper-parameters more extensively.\n\n\n\n# load the parsnip package\n# for tidy machine learning\n# modelling and workflows\n# to manage workflows\nlibrary(parsnip)\nlibrary(workflows)\n\n# specify our model structure\n# the model to be used and the\n# type of task we want to evaluate\nmodel_settings &lt;- parsnip::boost_tree(\n  trees = 50,\n  min_n = tune(),\n  tree_depth = tune(),\n  # learn_rate = tune()\n  ) |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"classification\")\n\n# create a workflow compatible with\n# the {tune} package which combines\n# model settings with the desired\n# model structure (data / formula)\nxgb_workflow &lt;- workflows::workflow() |&gt;\n  add_formula(as.factor(LC1) ~ .) |&gt;\n  add_model(model_settings)\n\nprint(xgb_workflow)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nas.factor(LC1) ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = 50\n  min_n = tune()\n  tree_depth = tune()\n\nComputational engine: xgboost \n\n\n\n\nHyperparameter settings\nTo optimize our model we need to configure (tune) the hyper-parameters which were left variable in the model settings. How we select these hyper-parameters is determined by sampling a parameter space, defined in our example by latin hypercube sampling. The {dials} R package provides a “tidymodels” compatible functions to support building latin hypercubes for our parameter search. In this example, I use the extract_param_set_dials() function to automatically populate the latin hypercube sampling settings. However, these can be specified manually when needed.\n\n# load the dials package\n# responsible for (hyper) parameter\n# sampling schemes to tune\n# parameters (as extracted)\n# from the model specifications\nlibrary(tune)\nlibrary(dials)\n\nhp_settings &lt;- dials::grid_latin_hypercube(\n  tune::extract_parameter_set_dials(xgb_workflow),\n  size = 3\n)\n\nprint(hp_settings)\n\n# A tibble: 3 × 2\n  min_n tree_depth\n  &lt;int&gt;      &lt;int&gt;\n1    33          6\n2     8         15\n3    25          9\n\n\n\n\nParameter estimation and cross-validation\nWe can move on to the actual model calibration. To ensure robustness of our model (hyper-) parameters across the training dataset the code below also implements a cross-validation to rotate through our training data using different subsets. The below code tunes the model parameters across the cross-validation folds and returns all these results in one variable, xgb_results.\n\n# set the folds (division into different)\n# cross-validation training datasets\nfolds &lt;- rsample::vfold_cv(train, v = 3)\n\n# optimize the model (hyper) parameters\n# using the:\n# 1. workflow (i.e. model)\n# 2. the cross-validation across training data\n# 3. the (hyper) parameter specifications\n# all data are saved for evaluation\nxgb_results &lt;- tune::tune_grid(\n  xgb_workflow,\n  resamples = folds,\n  grid = hp_settings,\n  control = tune::control_grid(save_pred = TRUE)\n)\n\nWhen the model is tuned across folds the results show varied model performance. In order to select the best model, according to a set metric, you can use the select_best() function (for a specific metric). This will extract the model parameters for the best model.\n\n# select the best model based upon\n# the root mean squared error\nxgb_best &lt;- tune::select_best(\n  xgb_results,\n  metric = \"roc_auc\"\n  )\n\n# cook up a model using finalize_workflow\n# which takes workflow (model) specifications\n# and combines it with optimal model\n# parameters into a functional model\nxgb_best_model &lt;- tune::finalize_workflow(\n  xgb_workflow,\n  xgb_best\n)\n\nprint(xgb_best_model)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nas.factor(LC1) ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = 50\n  min_n = 11\n  tree_depth = 5\n\nComputational engine: xgboost \n\n\nHowever, the returned object does not specify the model structure. To combine both the model structure (workflow) with the optimal parameters we need to combine both. In short, we need to finalize_workflow() which combines the model workflow with the model parameters into a final functional model which can take input data and return the desired (classification) results. Note that the resulting model (workflow) does not contain any tunable (tune()) fields as shown above! The selected model, given our provided constrains on the hyperparamters, has 50 trees, has minimum 11 data points in a node (min_n) and a tree depth of 5.\n\n\n\n7.2.4 Model evaluation\nIn machine learning model evaluation can be achieved in various ways and the standard exploration of the accuracy and variables of importance (and their relative contributions) to the model strength are beyond the scope of this example. I will only focus on the standard methodology and terminology as used in remote sensing and GIS fields.\n\nConfusion matrix and metrics\nIn the case of Land-Use and Land-Cover mapping classifications results are often reported as an confusion matrix and their associated metrics. The confusion matrix tells us something about accuracy (as perceived by users or producers), while the overall accuracy and Kappa coefficient allows us the compare the accuracy across modelled (map) results.\nIn (Table 7.1) a three class confusion matrix is shown with the model output along the top-bottom axis and the reference dataset shown along the left-right axis. In addition, the user and producer accuracy is shown. The user accuracy is the fraction of correctly classified classes with respect to the false positives (Type I) errors for a given class. For example, for a set of reference data (classes) the classified image commit false labels for class one (1) to classes two and three (2,3). These false positives are therefore also called commission errors. Similarly, the producer accuracy is the fraction of correctly classified classes with respect to the false negative (Type II) errors for a given class. For example, when the reference data identifies the data as class 1, but the classified image identifies it also as class 2 and 3. Values are omitted from the correct class and also called omission errors. In short, in remote sensing, the Type I and II errors are rephrased as the perspective taken during the assessment, either the user or the producer. In a strict machine learning context the user and producer accuracy would be called precision and recall, respectively. I refer to Boehmke and Greenwell (2020), Kuhn and Silge (2022) and Stocker et al. (2023) for an in depth discussion on machine learning model evaluation.\n\n\nTable 7.1: Confusion matrix with classification results from top to bottom, reference results from left to right. Results show the labels (1-3) and their respective false positive and negative results.\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\ntotal\nUser accuracy (Type I)\nKappa\n\n\n\n\n1\n49\n4\n4\n57\n0.89\n\n\n\n2\n2\n40\n2\n44\n0.90\n\n\n\n3\n3\n3\n59\n65\n0.90\n\n\n\ntotal\n54\n47\n65\n166\n\n\n\n\nProducer accuracy (Type II)\n0.90\n0.85\n0.90\n\n0.89\n\n\n\nKappa\n\n\n\n\n\n0.84\n\n\n\n\nSimple confusion matrices can be calculated using R using the table function comparing the true reference labels with the predicted values for our model.\n\n# run the model on our test data\n# using predict()\ntest_results &lt;- predict(xgb_best_model, test)\n\n# load the caret library to\n# access confusionMatrix functionality\nlibrary(caret)\n\nLoading required package: lattice\n\n# use caret's confusionMatrix function to get\n# a full overview of metrics\ncaret::confusionMatrix(\n  reference = as.factor(test$LC1), \n  data = as.factor(test_results$.pred_class)\n  )\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  1  2  3  4  5  6  7  8  9 10\n        1  25  0  2  0  1  2  0  2  1  0\n        2   0 29  0  0  0  0  1  0  0  0\n        3   1  0 25  0  0  0  0  1  0  0\n        4   0  1  1 27  6  1  1  0  0  1\n        5   1  1  0  2 24  0  2  0  1  0\n        6   0  0  1  0  0 27  0  0  1  1\n        7   0  1  1  2  0  1 23  1  0  0\n        8   1  0  0  0  0  1  1 28  0  1\n        9   0  2  1  1  0  0  0  1 30  0\n        10  0  0  0  0  0  0  0  0  0 26\n\nOverall Statistics\n                                          \n               Accuracy : 0.8489          \n                 95% CI : (0.8042, 0.8868)\n    No Information Rate : 0.1093          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.832           \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 6\nSensitivity           0.89286  0.85294  0.80645  0.84375  0.77419  0.84375\nSpecificity           0.97173  0.99639  0.99286  0.96057  0.97500  0.98925\nPos Pred Value        0.75758  0.96667  0.92593  0.71053  0.77419  0.90000\nNeg Pred Value        0.98921  0.98221  0.97887  0.98168  0.97500  0.98221\nPrevalence            0.09003  0.10932  0.09968  0.10289  0.09968  0.10289\nDetection Rate        0.08039  0.09325  0.08039  0.08682  0.07717  0.08682\nDetection Prevalence  0.10611  0.09646  0.08682  0.12219  0.09968  0.09646\nBalanced Accuracy     0.93229  0.92467  0.89965  0.90216  0.87460  0.91650\n                     Class: 7 Class: 8 Class: 9 Class: 10\nSensitivity           0.82143  0.84848  0.90909   0.89655\nSpecificity           0.97880  0.98561  0.98201   1.00000\nPos Pred Value        0.79310  0.87500  0.85714   1.00000\nNeg Pred Value        0.98227  0.98208  0.98913   0.98947\nPrevalence            0.09003  0.10611  0.10611   0.09325\nDetection Rate        0.07395  0.09003  0.09646   0.08360\nDetection Prevalence  0.09325  0.10289  0.11254   0.08360\nBalanced Accuracy     0.90011  0.91705  0.94555   0.94828\n\n\n\n\n\n7.2.5 Model scaling\nTo scale our new model to other locations, or regions we need to download additional data. I will demonstrate how to download geospatial data, rather than point data used during training, using the same {appeears} R package. This data will then be used to run our previously optimized model, and the results will be presented as a map. Since our workflow followed the original MODIS MCD12Q1 LULC map protocol I will compare both maps (while remapping some of the values from MODIS IGBP Land-Use and Land-Cover classes).\n\nData download\nTo scale our analysis spatially we need to download matching data, i.e. MODIS MCD43A4 NBAR data for bands 1 through 4, for a geographic regions. The {appeears} R package has an roi parameter which can take SpatRaster map data. Providing a SpatRaster map will match its extent to the AppEEARS query. I will use the map generated during our k-means clustering exercise as the input for building an API task. Alternatively, you can provide an extent yourself using a polygon, as specified using the {sf} R package (Section 3.1.2).\n\n# We can define an appeears\n# download task using a simple\n# dataframe and a map from which\n# an extent is extracted\ntask_df &lt;- data.frame(\n  task = \"raster_download\",\n  subtask = \"swiss\",\n  start = \"2012-01-01\",\n  end = \"2012-12-31\",\n  product = \"MCD43A4.061\",\n  layer = paste0(\"Nadir_Reflectance_Band\", 1:4)\n)\n\n# build the area based request/task\n# using the extent of our previous\n# kmeans map, export all results\n# as geotiff (rather than netcdf)\ntask &lt;- rs_build_task(\n  df = task_df,\n  roi = kmeans_map,\n  format = \"geotiff\"\n)\n\n# request the task to be executed\n# with results stored in a\n# temporary location (can be changed)\nrs_request(\n  request = task,\n  user = \"your_api_id\",\n  transfer = TRUE,\n  path = tempdir(),\n  verbose = TRUE\n)\n\n\n\n\n\n\n\nNote\n\n\n\nAppEEARS downloads might take a while! In the code above the default directory is also set to the temporary directory. To use the above code make sure to change the download path to a more permanent one.\n\n\n\n\nModel execution\nThe downloaded data (by default in tempdir()) are a collection of geotiff files of NBAR reflectance bands and their matching quality control data. In this demonstration I will not process the more nuanced quality control flags. Therefore, I only list the reflectance files, and read in this list of geotiffs into a stack by calling rast().\n\nfiles &lt;- list.files(\n  tempdir(),\n  \"*Reflectance*\",\n  recursive = TRUE,\n  full.names = TRUE\n)\n\n# load this spatial data to run the model\n# spatially\nswiss_multispec_data &lt;- terra::rast(files)\n\nThe model we created is specific when it comes to naming variables, i.e. the naming of the bands in our spatial data matters and has to match those of the model. Due to inconsistencies in the AppEEARS API one has to rename the band names ever so slightly.\n\n# the model only works when variable names\n# are consistent we therefore rename them\nband_names &lt;- data.frame(\n  name = names(swiss_multispec_data)\n) |&gt;\n  mutate(\n    date = as.Date(substr(name, 40, 46), format = \"%Y%j\"),\n    name = paste(substr(name, 1, 35), date, sep = \"_\"),\n    name = gsub(\"\\\\.\",\"_\", name)\n  )\n\n# reassign the names of the terra image stack\nnames(swiss_multispec_data) &lt;- band_names$name\n\nWith all band names in line with the expected variables in our model we can now scale it by using terra::predict(). This function is the {terra} R package equivalent of the predict() function for general statistical models. This function allows you to call a compatible model for SpatRaster data, by running it along the time/band axis of the raster stack. I ask for the model probabilities to be returned, as this gives a more granular overview of the model results (see type argument, Figure 7.4).\n\n# return probabilities, where each class is\n# associated with a layer in an image stack\n# and the probabilities reflect the probabilities\n# of the classification for said layer\nlulc_probabilities &lt;- terra::predict(\n  swiss_multispec_data,\n  xgb_best_model,\n  type = \"prob\"\n)\n\n\n\nCode\nggplot() +\n  tidyterra::geom_spatraster(data = lulc_probabilities) +\n  scale_fill_viridis_c(\n    na.value = NA,\n    name = \"Class probabilities\",\n    option = \"magma\"\n    ) +\n  scale_x_continuous(breaks = seq(-180, 180, 2)) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n    ) +\n  facet_wrap(~lyr)\n\n\n\n\n\nFigure 7.4: Classification probabilities, as generated for all 10 Land-Use and Land-Cover (LULC) classes within our xgboost algorithm, for the year 2012 across a Swiss spatial subset.\n\n\n\n\nOne can create a formal Land-Use and Land-Cover map by reducing this dataset by returning the layer for which the probability is highest, using which.max. As seen in Chapter 3, you can apply a function to all layers across pixels using the terra::app() function. The returned data is a Land-Use and Land-Cover map with classes 1 to 10, as shown below (Figure 7.5).\n\n# generate the map by selecting maximum probabilities\n# from the model output\nlulc_map &lt;- terra::app(lulc_probabilities, which.max)\n\n\n\nCode\nclasses &lt;- c(\n    \"Tree Cover\",\n    \"Shrub Cover\",\n    \"Herbaceous Vegetation & Grassland\",\n    \"Cultivated and Managed\",\n    \"Mosaic: Managed & Natural Vegetation\",\n    \"Regularly Flooded & Wetland\",\n    \"Urban & Built Up\",\n    \"Snow and Ice\",\n    \"Barren\",\n    \"Open Water\"\n  )\n\n# set te colour scale manually\npalcol &lt;- colorFactor(\n  c(\n    \"#05450a\",\n    \"#78d203\",\n    \"#009900\",\n    \"#c24f44\",\n    \"#ff6d4c\",\n    \"#27ff87\",\n    \"#a5a5a5\",\n    \"#69fff8\",\n    \"#f9ffa4\",\n    \"#1c0dff\"\n    ),\n  na.color = NA,\n  domain = 1:10\n)\n\n# build the leaflet map\nleaflet() |&gt; \n  addProviderTiles(providers$Esri.WorldImagery, group = \"World Imagery\") |&gt;\n  addProviderTiles(providers$Esri.WorldTopoMap, group = \"World Topo\") |&gt;\n  addRasterImage(\n    lulc_map,\n    colors = palcol,\n    opacity = 0.8,\n    method = \"ngb\",\n    group = \"XGBOOST\"\n  ) |&gt;\n  addRasterImage(\n    modis_lulc,\n    colors = palcol,\n    opacity = 0.8,\n    method = \"ngb\",\n    group = \"MODIS MCD12Q1\"\n  ) |&gt;\n  addLayersControl(\n    baseGroups = c(\"World Imagery\",\"World Topo\"),\n    position = \"topleft\",\n    options = layersControlOptions(collapsed = FALSE),\n    overlayGroups = c(\"XGBOOST\", \"MODIS MCD12Q1\")\n  ) |&gt;\n  hideGroup(\"MODIS MCD12Q1\") |&gt;\n  addLegend(\n    colors = palcol(1:10),\n    values = 1:10,\n    labels = classes,\n    title = \"Land-Use and Land-Cover class\"\n  )\n\n\n\n\n\nFigure 7.5: A supervised machine learning based Land-Use and Land-Cover (LULC) map, based upon four MODIS MCD43A4 bands (1-4) using a boosted regression tree classification (xgboost). LULC classes were defined by Fritz et al. 2017. For comparison MODIS MCD12Q1 LULC (IGBP) data was remapped to the LULC classes of Fritz et al. 2017.\n\n\n\n\n\n\n\nBoehmke, Brad, and Brandon M Greenwell. 2020. Hands-On Machine Learning with R. Boca Raton, US: CRC Press. https://www.routledge.com/Hands-On-Machine-Learning-with-R/Boehmke-Greenwell/p/book/9781138495685.\n\n\nd’Andrimont, Raphaël, Momchil Yordanov, Laura Martinez-Sanchez, Beatrice Eiselt, Alessandra Palmieri, Paolo Dominici, Javier Gallego, et al. 2020. “Harmonised LUCAS in-Situ Land Cover and Use Database for Field Surveys from 2006 to 2018 in the European Union.” Scientific Data 7 (1): 352. https://doi.org/10.1038/s41597-020-00675-z.\n\n\nFriedl, M A, D K McIver, J C F Hodges, X Y Zhang, D Muchoney, A H Strahler, C E Woodcock, et al. 2002. “Global Land Cover Mapping from MODIS: Algorithms and Early Results.” Remote Sensing of Environment 83 (1-2): 287–302. http://www.sciencedirect.com/science/article/B6V6V-46RDDFS-2/2/498151a48b317e5bb23b15033a694d4e.\n\n\nFriedl, M A, D Sulla-Menashe, B Tan, A Schneider, N Ramankutty, A Sibley, and X M Huang. 2010. “MODIS Collection 5 Global Land Cover: Algorithm Refinements and Characterization of New Datasets.” Remote Sensing of Environment 114 (1): 168–82. https://doi.org/10.1016/j.rse.2009.08.016.\n\n\nFritz, Steffen, Linda See, Christoph Perger, Ian McCallum, Christian Schill, Dmitry Schepaschenko, Martina Duerauer, et al. 2017. “A Global Dataset of Crowdsourced Land Cover and Land Use Reference Data.” Scientific Data 4 (1): 170075. https://doi.org/10.1038/sdata.2017.75.\n\n\nHuete, A, K Didan, T Miura, E P Rodriguez, X Gao, and L G Ferreira. 2002. “Overview of the Radiometric and Biophysical Performance of the MODIS Vegetation Indices.” Remote Sensing of Environment 83 (1-2): 195–213. citeulike-article-id:274524http://dx.doi.org/10.1016/S0034-4257(02)00096-2.\n\n\nHufkens, Koen. 2023. “Bluegreen-Labs/Appeears: Appeears: An Interface to the NASA AppEEARS API.” Zenodo. https://doi.org/10.5281/zenodo.7958270.\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with R: A Framework for Modeling in the Tidyverse. 1st edition. Beijing Boston Farnham Sebastopol Tokyo: O’Reilly Media.\n\n\nStocker, Benjamin, Koen Hufkens, Pepa Arán, and Pascal Schneider. 2023. “Applied Geodata Science.” Zenodo. https://doi.org/10.5281/zenodo.7740560.\n\n\nZeng, Yelu, Dalei Hao, Alfredo Huete, Benjamin Dechant, Joe Berry, Jing M. Chen, Joanna Joiner, et al. 2022. “Optical Vegetation Indices for Monitoring Terrestrial Ecosystems Globally.” Nature Reviews Earth & Environment 3 (7): 477–93. https://doi.org/10.1038/s43017-022-00298-5."
  },
  {
    "objectID": "exercises.html#phenology-trends-and-algorithms",
    "href": "exercises.html#phenology-trends-and-algorithms",
    "title": "8  Exercises",
    "section": "8.1 Phenology trends and algorithms",
    "text": "8.1 Phenology trends and algorithms\nThese exercises cover all materials up to Chapter 5. A proper understanding of these chapters is required to complete these exercises. Exercises are at times formulated in long form, not simple bullet points, in order to partially mimic formal descriptions as you would find in a methods section of an academic journal, or a reference manual.\n\n8.1.1 Physical geography and phenology\nInterpret the results of Section 4.3 and the fit model as shown in the collapsed note at the end of the section.\n\nWhat does the intercept indicate?\nHow can you interpret the slope?\nConvert the established relationship with altitude, to one with temperature\n\nHow would you go about this?\n\n\n\n\n8.1.2 Temporal and spatial anomalies\nFor a location near the Adirondacks in the North-Eastern United States (Figure 8.1) gather phenology data on both the greenup and maximum canopy development of a location centered on 43.5\\(^\\circ\\)N and 74.5\\(^\\circ\\)W. Gather data for all pixels 100 km around this location for years 2001 to 2010. Similarly, download land cover data for the year 2010 for the same spatial extent, and only consider IGBP broadleaf and mixed forest classes in your analysis.\nFor the years 2001 - 2009 calculate the long term mean (LTM) and standard deviation (SD) of the phenology metrics. Calculate location with an early greenup for 2010 (&lt; LTM - 1 SD) and locations with late maturity (&gt; LTM + 1 SD).\nDescribe the observed patterns and speculate about the underlying reasons. In addition, download a digital elevation map for the United States (30s resolution), and compare differences in altitude (e.g. a boxplot) across locations where you do or do not see any patterns in phenology.\n\n\n\n\n\nFigure 8.1: The location of the Adirondack mountains in the North-Eastern United States\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse details in Chapter 2, Chapter 3 and Chapter 4 to answer these questions, by considering all data products and methods mentioned. Additional meta-data will need to be consulted online to provide context or data conversion instructions in some cases. Where necessary consult the relevant scientific literature.\n\n\n\n\n8.1.3 Scaling the calculation of phenology metrics\nIn this exercise you will be required to download external data manually. First you will have to sign up for a NASA EarthData login to access the required data. The NASA EarthData login provides access to a wide range of data products.\nOnce signed in, download the data MCD13C1 product for the year 2022. The MCD13C1 product provides vegetation indices (VI), such as the Enhanced Vegetation Index (EVI) and Normalized Difference Vegetation Index (NDVI) data products on a down-sampled climate model grid (CMG, at 0.05\\(^\\circ\\) or ~5km resolution). This down-sampled data product reduces the volume of data to download and process, but should allow you to explore broad continental scale patterns when calculating vegetation (or land surface) phenology metrics.\nWith all data downloaded:\n\nCombine all EVI data (23 layers) into a single compressed geotiff file (write to file)\nRead in the geotiff file to work faster from memory\nFor a first trial crop the full dataset to 26\\(^\\circ\\)W, 20\\(^\\circ\\)E, 31\\(^\\circ\\)N, 70\\(^\\circ\\)N\nApply the algorithm as outlined in Section 5.4.1 using a start of season greenup signal of 25% the seasonal EVI amplitude\nAssess the performance of the algorithm across the globe and discuss its consistency.\n\nWhere does it fail? How does it fail?\nWhere necessary, inspect point locations to explore potential issues.\nIf possible, address any issues by altering the original algorithm.\n\nCould you scale this globally?\n\nHow long would it take?\nCan you improve calculation times?\n\n\nPlot the global phenology maps, and its various iterations in the R markdown notebook.\n\n\n\n\n\n\nNote\n\n\n\nAll information for this exercise can be found in Chapter 3 and Chapter 5. As before, additional meta-data will need to be consulted online to provide context or data conversion instructions in some cases."
  },
  {
    "objectID": "exercises.html#phenology-modelling",
    "href": "exercises.html#phenology-modelling",
    "title": "8  Exercises",
    "section": "8.2 Phenology modelling",
    "text": "8.2 Phenology modelling\n\nHow can you improve the model used to regionally scale the results in Chapter 6?\n\nProvide at least three (3) ways to improve the model used.\n\nImplement at least one of these methods\nStatistically compare the results with the MODIS MCD12Q2 phenology product"
  },
  {
    "objectID": "exercises.html#land-use-and-land-cover-modelling",
    "href": "exercises.html#land-use-and-land-cover-modelling",
    "title": "8  Exercises",
    "section": "8.3 Land-Use and Land-Cover modelling",
    "text": "8.3 Land-Use and Land-Cover modelling\n\nHow can you improve the model used to map LULC?\n\nProvide at least three (4) ways to improve the model used.\n\nImplement at least two of these methods\nDemonstrate improved model skill across the two generated maps\n\n\n\n\n\n\n\nNote\n\n\n\nUse the appropriate APIs to download the required data."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aono, Yasuyuki, and Keiko Kazui. 2008. “Phenological Data Series\nof Cherry Tree Flowering in Kyoto, Japan, and\nIts Application to Reconstruction of Springtime Temperatures Since the\n9th Century.” International Journal of Climatology 28\n(7): 905–14. https://doi.org/10.1002/joc.1594.\n\n\nAugspurger, Carol K. 2013. “Reconstructing Patterns of\nTemperature, Phenology, and Frost Damage over 124 Years:\n{Spring} Damage Risk Is Increasing.”\nEcology 94 (1): 41–50. https://doi.org/10.1890/12-0200.1.\n\n\nBasler, David. 2016. “Evaluating Phenological Models for the\nPrediction of Leaf-Out Dates in Six Temperate Tree Species Across\nCentral Europe.” Agricultural and Forest\nMeteorology 217 (February): 10–21. https://doi.org/10.1016/j.agrformet.2015.11.007.\n\n\nBoehmke, Brad, and Brandon M Greenwell. 2020. Hands-On\nMachine Learning with R.\nBoca Raton, US: CRC Press. https://www.routledge.com/Hands-On-Machine-Learning-with-R/Boehmke-Greenwell/p/book/9781138495685.\n\n\nCleland, E, I Chuine, A Menzel, H Mooney, and M Schwartz. 2007.\n“Shifting Plant Phenology in Response to Global Change.”\nTrends in Ecology & Evolution 22 (7): 357–65. https://doi.org/10.1016/j.tree.2007.04.003.\n\n\nCrimmins, Theresa M., Michael A. Crimmins, Katharine L. Gerst, Alyssa H.\nRosemartin, and Jake F. Weltzin. 2017. “USA\nNational Phenology Network’s\nVolunteer-Contributed Observations Yield Predictive Models of\nPhenological Transitions.” PLoS ONE 12 (8): 1–17. https://doi.org/10.1371/journal.pone.0182919.\n\n\nd’Andrimont, Raphaël, Momchil Yordanov, Laura Martinez-Sanchez, Beatrice\nEiselt, Alessandra Palmieri, Paolo Dominici, Javier Gallego, et al.\n2020. “Harmonised LUCAS in-Situ Land Cover and Use\nDatabase for Field Surveys from 2006 to 2018 in the\nEuropean Union.” Scientific\nData 7 (1): 352. https://doi.org/10.1038/s41597-020-00675-z.\n\n\nDe Reaumur, R-AF. 1735. “Observations Du Thermometre, Faites a\nParis Pendant l′annee 1735 Comparees Avec Celles Qui Onte\nFaites Sous La Ligne Et Al′Ile de France,a\nAlger Et En Quelques-Unes de Nos Iles de\nl′Amerique.” Memoires de l’Academie Royale Des\nSciences de Paris 1735: 545–76.\n\n\nFriedl, M A, D K McIver, J C F Hodges, X Y Zhang, D Muchoney, A H\nStrahler, C E Woodcock, et al. 2002. “Global Land Cover Mapping\nfrom MODIS: Algorithms and Early Results.”\nRemote Sensing of Environment 83 (1-2): 287–302. http://www.sciencedirect.com/science/article/B6V6V-46RDDFS-2/2/498151a48b317e5bb23b15033a694d4e.\n\n\nFriedl, M A, D Sulla-Menashe, B Tan, A Schneider, N Ramankutty, A\nSibley, and X M Huang. 2010. “MODIS\nCollection 5 Global Land Cover: Algorithm\nRefinements and Characterization of New Datasets.” Remote\nSensing of Environment 114 (1): 168–82. https://doi.org/10.1016/j.rse.2009.08.016.\n\n\nFritz, Steffen, Linda See, Christoph Perger, Ian McCallum, Christian\nSchill, Dmitry Schepaschenko, Martina Duerauer, et al. 2017. “A\nGlobal Dataset of Crowdsourced Land Cover and Land Use Reference\nData.” Scientific Data 4 (1): 170075. https://doi.org/10.1038/sdata.2017.75.\n\n\nGanguly, Sangram, Mark a. Friedl, Bin Tan, Xiaoyang Y Zhang, and Manish\nVerma. 2010. “Land Surface Phenology from MODIS:\nCharacterization of the Collection 5 Global\nLand Cover Dynamics Product.” Remote Sensing of\nEnvironment 114 (8): 1805–16. https://doi.org/10.1016/j.rse.2010.04.005.\n\n\nGu, Lianhong, Paul J Hanson, W Mac Post, Dale P Kaiser, Bai Yang,\nRamakrishna Nemani, Stephen G Pallardy, and Tilden Meyers. 2008.\n“The 2007 Eastern US Spring\nFreeze: Increased Cold\nDamage in a Warming\nWorld.” BioScience 58 (3): 253–62. https://doi.org/10.1641/b580311.\n\n\nHuete, A, K Didan, T Miura, E P Rodriguez, X Gao, and L G Ferreira.\n2002. “Overview of the Radiometric and Biophysical Performance of\nthe MODIS Vegetation Indices.” Remote Sensing of\nEnvironment 83 (1-2): 195–213. citeulike-article-id:274524http://dx.doi.org/10.1016/S0034-4257(02)00096-2.\n\n\nHufkens, Koen. 2023. “Bluegreen-Labs/Appeears: Appeears: An\nInterface to the NASA AppEEARS\nAPI.” Zenodo. https://doi.org/10.5281/zenodo.7958270.\n\n\nHufkens, Koen, David Basler, Tom Milliman, Eli K. Melaas, and Andrew D.\nRichardson. 2018. “An Integrated Phenology Modelling Framework in\nR.” Methods in Ecology and Evolution 9:\n1–10. https://doi.org/10.1111/2041-210X.12970.\n\n\nHufkens, Koen, Mark A. Friedl, Trevor F. Keenan, Oliver Sonnentag, Amey\nBailey, John O’Keefe, and Andrew D. Richardson. 2012. “Ecological\nImpacts of a Widespread Frost Event Following Early Spring\nLeaf-Out.” Global Change Biology 18 (7): 2365–77. https://doi.org/10.1111/j.1365-2486.2012.02712.x.\n\n\nHufkens, Koen, Mark Friedl, Oliver Sonnentag, Bobby H. Braswell, Thomas\nMilliman, and Andrew D. Richardson. 2012. “Linking Near-Surface\nand Satellite Remote Sensing Measurements of Deciduous Broadleaf Forest\nPhenology.” Remote Sensing of Environment 117\n(February): 307–21. https://doi.org/10.1016/j.rse.2011.10.006.\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with\nR: A Framework for\nModeling in the Tidyverse. 1st edition.\nBeijing Boston Farnham Sebastopol Tokyo: O’Reilly Media.\n\n\nLieth, Helmut. 2013. Phenology and Seasonality Modeling. Vol.\n8. Springer Science & Business Media.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.\nGeocomputation in R. Chapman & Hall.\n\n\nMenzel, Annette, and Volker Dose. 2005. “Analysis of Long-Term\nTime Series of the Beginning of Flowering by Bayesian\nFunction Estimation.” Meteorologische Zeitschrift 14\n(3): 429–34. https://doi.org/10.1127/0941-2948/2005/0040.\n\n\nMenzel, Annette, Tim H. Sparks, Nicole Estrella, Elisabeth Koch, Anto\nAasa, Rein Ahas, Kerstin Alm-Kübler, et al. 2006. “European\nPhenological Response to Climate Change Matches the Warming\nPattern.” Global Change Biology 12 (10): 1969–76. https://doi.org/10.1111/j.1365-2486.2006.01193.x.\n\n\nMyneni, RB, FG Hall, Piers J. Sellers, and Alexander L. Marshak. 1995.\n“The Interpretation of Spectral Vegetation Indexes.”\nTransactions on Geoscience and Remote Sensing 33 (2): 481–86.\nhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=377948.\n\n\nPeñuelas, J., and I. Filella. 2009. “Phenology Feedbacks on\nClimate Change.” Science 324 (5929): 887. http://www.sciencemag.org/content/324/5929/887.short.\n\n\nPiao, Shilong, Qiang Liu, Anping Chen, Ivan A Janssens, Yongshuo Fu,\nJunhu Dai, Lingli Liu, Xu Lian, Miaogen Shen, and Xiaolin Zhu. 2019.\n“Plant Phenology and Global Climate Change: Current Progresses and\nChallenges.” Global Change Biology, March. https://doi.org/10.1111/gcb.14619.\n\n\nRichardson, Andrew D. 2018. “Tracking Seasonal Rhythms of Plants\nin Diverse Ecosystems with Digital Camera Imagery.” New\nPhytologist, no. 11: 1–9. https://doi.org/10.1111/nph.15591.\n\n\nRichardson, Andrew D, T Andy Black, Philippe Ciais, Nicolas Delbart,\nMark a Friedl, Nadine Gobron, David Y Hollinger, et al. 2010.\n“Influence of Spring and Autumn Phenological Transitions on Forest\nEcosystem Productivity.” Philosophical Transactions of the\nRoyal Society of London. Series B, Biological Sciences 365 (1555):\n3227–46. https://doi.org/10.1098/rstb.2010.0102.\n\n\nRichardson, Andrew D., Koen Hufkens, Tom Milliman, Donald M. Aubrecht,\nMin Chen, Josh M. Gray, Miriam R. Johnston, et al. 2018. “Tracking\nVegetation Phenology Across Diverse North\nAmerican Biomes Using PhenoCam\nImagery.” Scientific Data 5 (1). https://doi.org/10.1038/sdata.2018.28.\n\n\nStocker, Benjamin, Koen Hufkens, Pepa Arán, and Pascal Schneider. 2023.\n“Applied Geodata Science.”\nZenodo. https://doi.org/10.5281/zenodo.7740560.\n\n\nVitasse, Yann, Annabel Josée Porté, Antoine Kremer, Richard Michalet,\nand Sylvain Delzon. 2009. “Responses of Canopy Duration to\nTemperature Changes in Four Temperate Tree Species: Relative\nContributions of Spring and Autumn Leaf Phenology.”\nOecologia 161 (1): 187–98. https://doi.org/10.1007/s00442-009-1363-4.\n\n\nWang, Xuhui, Shilong Piao, Xiangtao Xu, Philippe Ciais, Natasha MacBean,\nRanga B. Myneni, and Laurent Li. 2015. “Has the Advancing Onset of\nSpring Vegetation Green-up Slowed down or Changed Abruptly over the Last\nThree Decades?: 30-Year Change of Spring Vegetation Phenology.”\nGlobal Ecology and Biogeography 24 (6): 621–31. https://doi.org/10.1111/geb.12289.\n\n\nZeng, Yelu, Dalei Hao, Alfredo Huete, Benjamin Dechant, Joe Berry, Jing\nM. Chen, Joanna Joiner, et al. 2022. “Optical Vegetation Indices\nfor Monitoring Terrestrial Ecosystems Globally.” Nature\nReviews Earth & Environment 3 (7): 477–93. https://doi.org/10.1038/s43017-022-00298-5.\n\n\nZhang, X, MA Friedl, and CB Schaaf. 2004. “Climate Controls on\nVegetation Phenological Patterns in Northern Mid-and High Latitudes\nInferred from MODIS Data.” Global Change\nBiology, no. 10: 1133–45. https://doi.org/10.1111/j.1365-2486.2004.00784.x.\n\n\nZhang, Xiaoyang, Mark A Friedl, Crystal B Schaaf, Alan H Strahler, John\nC F Hodges, Feng Gao, Bradley C Reed, and Alfredo Huete. 2003.\n“Monitoring Vegetation Phenology Using MODIS.”\nRemote Sensing of Environment 84 (3): 471–75. http://www.sciencedirect.com/science/article/B6V6V-478RS7T-1/2/19c385401eecea8964bca155ac01eb71."
  },
  {
    "objectID": "appendix_setup.html#required-packages",
    "href": "appendix_setup.html#required-packages",
    "title": "Appendix A — Setup",
    "section": "A.1 Required packages",
    "text": "A.1 Required packages\nTo run this course a set of R packages is required. The below script lets you quickly install all of them, in the correct order.\n\n# Windows is sensitive to the order of installation\n# of some of the packages so follow this order \n# as noted below, on other systems this might be\n# less of an issue but follow the instructions\n# regardless\n\n# First install the rlang packages which provides\n# R functionalities needed by/for other packages\n# in particular dplyr \ninstall.packages(\"rlang\")\n\n# next install dplyr needed to manipulate\n# tidy data as discussed in the R crash course\ninstall.packages(\"dplyr\")\n\n# for convenient plotting we also install\n# ggplot2 (this is not required but most plots shown\n# are generated using ggplot)\ninstall.packages(\"ggplot2\")\n\n# to deal with relative paths in R markdown\n# install the here package\ninstall.packages(\"here\")\n\n# next up are the installs of the geospatial\n# libraries, these can be installed in one\n# go\ninstall.packages(c(\"terra\",\"tidyterra\",\"sf\",\"MODISTools\"))"
  },
  {
    "objectID": "appendix_licensing.html",
    "href": "appendix_licensing.html",
    "title": "Appendix B — Licensing",
    "section": "",
    "text": "All rights belong to the referenced right holders. In absence of any explicit reference to right holders (in figure captions or other materials) the rights reside with the author of this manuscript under the below license.\n\n\n\n\n\n\nLicense\n\n\n\nPlease note that this work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License."
  },
  {
    "objectID": "appendix_config.html",
    "href": "appendix_config.html",
    "title": "Appendix C — Book configuration",
    "section": "",
    "text": "The book was rendered on github using the following package configuration:\n\nrenv::diagnostics()\n\nDiagnostics Report [renv 0.17.3]\n================================\n\n# Session Info =======================\nR version 4.3.1 (2023-06-16)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.1    fastmap_1.1.1     cli_3.6.1        \n [5] htmltools_0.5.5   tools_4.3.1       rmarkdown_2.21    knitr_1.42       \n [9] jsonlite_1.8.4    xfun_0.39         digest_0.6.31     rlang_1.1.1      \n[13] renv_0.17.3       evaluate_0.20    \n\n# Project ============================\nProject path: \"~/work/handful_of_pixels/handful_of_pixels\"\n\n# Status =============================\n* The project is already synchronized with the lockfile.\n\n# Packages ===========================\n                     Library Source   Lockfile Source Path Dependency\nDBI                    1.1.3   RSPM      1.1.3   CRAN  [1]   indirect\nDiceDesign               1.9   RSPM        1.9   CRAN  [1]   indirect\nGPfit                  1.0-8   RSPM      1.0-8   CRAN  [1]   indirect\nGenSA                  1.1.9   RSPM      1.1.9   CRAN  [1]     direct\nKernSmooth           2.23-22   RSPM    2.23-22   CRAN  [1]   indirect\nMASS                  7.3-60   CRAN     7.3-60   CRAN  [2]   indirect\nMODISTools             1.1.4   RSPM      1.1.4   CRAN  [1]     direct\nMatrix                 1.6-0   RSPM      1.6-0   CRAN  [1]   indirect\nModelMetrics         1.2.2.2   RSPM    1.2.2.2   CRAN  [1]   indirect\nR6                     2.5.1   RSPM      2.5.1   CRAN  [1]   indirect\nRColorBrewer           1.1-3   RSPM      1.1-3   CRAN  [1]   indirect\nRcpp                  1.0.10   RSPM     1.0.10   CRAN  [1]   indirect\nSQUAREM               2021.1   RSPM     2021.1   CRAN  [1]   indirect\nV8                     4.3.0   RSPM      4.3.0   CRAN  [1]   indirect\nappeears                 1.1 GitHub        1.1 GitHub  [1]     direct\naskpass                  1.1   RSPM        1.1   CRAN  [1]   indirect\nassertthat             0.2.1   RSPM      0.2.1   CRAN  [1]   indirect\nbackports              1.4.1   RSPM      1.4.1   CRAN  [1]   indirect\nbase64enc              0.1-3   RSPM      0.1-3   CRAN  [1]   indirect\nbit                    4.0.5   RSPM      4.0.5   CRAN  [1]   indirect\nbit64                  4.0.5   RSPM      4.0.5   CRAN  [1]   indirect\nblob                   1.2.4   RSPM      1.2.4   CRAN  [1]   indirect\nboot                1.3-28.1   CRAN       &lt;NA&gt;   &lt;NA&gt;  [2]       &lt;NA&gt;\nbroom                  1.0.5   RSPM      1.0.5   CRAN  [1]   indirect\nbslib                  0.4.2   RSPM      0.4.2   CRAN  [1]   indirect\ncachem                 1.0.8   RSPM      1.0.8   CRAN  [1]   indirect\ncallr                  3.7.3   RSPM      3.7.3   CRAN  [1]   indirect\ncaret                 6.0-94   RSPM     6.0-94   CRAN  [1]     direct\ncellranger             1.1.0   RSPM      1.1.0   CRAN  [1]   indirect\nclass                 7.3-22   CRAN     7.3-22   CRAN  [2]   indirect\nclassInt               0.4-9   RSPM      0.4-9   CRAN  [1]   indirect\ncli                    3.6.1   RSPM      3.6.1   CRAN  [1]   indirect\nclipr                  0.8.0   RSPM      0.8.0   CRAN  [1]   indirect\nclock                  0.7.0   RSPM      0.7.0   CRAN  [1]   indirect\ncluster                2.1.4   CRAN       &lt;NA&gt;   &lt;NA&gt;  [2]       &lt;NA&gt;\ncodetools             0.2-19   CRAN     0.2-19   CRAN  [2]   indirect\ncolorspace             2.1-0   RSPM      2.1-0   CRAN  [1]   indirect\ncompiler                &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\nconflicted             1.2.0   RSPM      1.2.0   CRAN  [1]   indirect\ncpp11                  0.4.3   RSPM      0.4.3   CRAN  [1]   indirect\ncrayon                 1.5.2   RSPM      1.5.2   CRAN  [1]   indirect\ncrosstalk              1.2.0   RSPM      1.2.0   CRAN  [1]   indirect\ncrul                   1.4.0   RSPM      1.4.0   CRAN  [1]   indirect\ncurl                   5.0.0   RSPM      5.0.0   CRAN  [1]   indirect\ndata.table            1.14.8   RSPM     1.14.8   CRAN  [1]   indirect\ndaymetr                  1.7   RSPM        1.7   CRAN  [1]     direct\ndbplyr                 2.3.3   RSPM      2.3.3   CRAN  [1]   indirect\ndiagram                1.6.5   RSPM      1.6.5   CRAN  [1]   indirect\ndials                  1.2.0   RSPM      1.2.0   CRAN  [1]     direct\ndigest                0.6.31   RSPM     0.6.31   CRAN  [1]   indirect\ndplyr                  1.1.2   RSPM      1.1.2   CRAN  [1]     direct\ndtplyr                 1.3.1   RSPM      1.3.1   CRAN  [1]   indirect\ne1071                 1.7-13   RSPM     1.7-13   CRAN  [1]   indirect\nellipsis               0.3.2   RSPM      0.3.2   CRAN  [1]   indirect\nevaluate                0.20   RSPM       0.20   CRAN  [1]   indirect\nfansi                  1.0.4   RSPM      1.0.4   CRAN  [1]   indirect\nfarver                 2.1.1   RSPM      2.1.1   CRAN  [1]   indirect\nfastmap                1.1.1   RSPM      1.1.1   CRAN  [1]   indirect\nfilelock               1.0.2   RSPM      1.0.2   CRAN  [1]   indirect\nfontawesome            0.5.1   RSPM      0.5.1   CRAN  [1]   indirect\nforcats                1.0.0   RSPM      1.0.0   CRAN  [1]   indirect\nforeach                1.5.2   RSPM      1.5.2   CRAN  [1]   indirect\nforeign               0.8-84   CRAN       &lt;NA&gt;   &lt;NA&gt;  [2]       &lt;NA&gt;\nfs                     1.6.2   RSPM      1.6.2   CRAN  [1]   indirect\nfurrr                  0.3.1   RSPM      0.3.1   CRAN  [1]   indirect\nfuture                1.33.0   RSPM     1.33.0   CRAN  [1]   indirect\nfuture.apply          1.11.0   RSPM     1.11.0   CRAN  [1]   indirect\ngargle                 1.5.2   RSPM      1.5.2   CRAN  [1]   indirect\ngenerics               0.1.3   RSPM      0.1.3   CRAN  [1]   indirect\ngeodata                0.5-8   RSPM      0.5-8   CRAN  [1]     direct\ngeojson                0.3.4   RSPM      0.3.4   CRAN  [1]   indirect\ngeojsonio             0.11.1   RSPM     0.11.1   CRAN  [1]   indirect\ngeojsonsf              2.0.3   RSPM      2.0.3   CRAN  [1]   indirect\ngeometries             0.2.2   RSPM      0.2.2   CRAN  [1]   indirect\ngetPass                0.2-2   RSPM      0.2-2   CRAN  [1]   indirect\nggplot2                3.4.2   RSPM      3.4.2   CRAN  [1]     direct\nglobals               0.16.2   RSPM     0.16.2   CRAN  [1]   indirect\nglue                   1.6.2   RSPM      1.6.2   CRAN  [1]   indirect\ngoogledrive            2.1.1   RSPM      2.1.1   CRAN  [1]   indirect\ngooglesheets4          1.1.1   RSPM      1.1.1   CRAN  [1]   indirect\ngower                  1.0.1   RSPM      1.0.1   CRAN  [1]   indirect\ngrDevices               &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\ngraphics                &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\ngrid                    &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\ngridExtra                2.3   RSPM        2.3   CRAN  [1]   indirect\ngtable                 0.3.3   RSPM      0.3.3   CRAN  [1]   indirect\nhardhat                1.3.0   RSPM      1.3.0   CRAN  [1]   indirect\nhaven                  2.5.3   RSPM      2.5.3   CRAN  [1]   indirect\nhere                   1.0.1   RSPM      1.0.1   CRAN  [1]     direct\nhexbin                1.28.3   RSPM     1.28.3   CRAN  [1]     direct\nhighr                   0.10   RSPM       0.10   CRAN  [1]   indirect\nhms                    1.1.3   RSPM      1.1.3   CRAN  [1]   indirect\nhtmltools              0.5.5   RSPM      0.5.5   CRAN  [1]   indirect\nhtmlwidgets            1.6.2   RSPM      1.6.2   CRAN  [1]   indirect\nhttpcode               0.3.0   RSPM      0.3.0   CRAN  [1]   indirect\nhttr                   1.4.5   RSPM      1.4.5   CRAN  [1]     direct\nids                    1.0.1   RSPM      1.0.1   CRAN  [1]   indirect\ninfer                  1.0.4   RSPM      1.0.4   CRAN  [1]   indirect\nipred                 0.9-14   RSPM     0.9-14   CRAN  [1]   indirect\nisoband                0.2.7   RSPM      0.2.7   CRAN  [1]   indirect\niterators             1.0.14   RSPM     1.0.14   CRAN  [1]   indirect\njqr                    1.2.3   RSPM      1.2.3   CRAN  [1]   indirect\njquerylib              0.1.4   RSPM      0.1.4   CRAN  [1]   indirect\njsonify                1.2.2   RSPM      1.2.2   CRAN  [1]   indirect\njsonlite               1.8.4   RSPM      1.8.4   CRAN  [1]   indirect\nkeyring                1.3.1   RSPM      1.3.1   CRAN  [1]   indirect\nknitr                   1.42   RSPM       1.42   CRAN  [1]     direct\nlabeling               0.4.2   RSPM      0.4.2   CRAN  [1]   indirect\nlattice               0.21-8   CRAN     0.21-8   CRAN  [2]   indirect\nlava                 1.7.2.1   RSPM    1.7.2.1   CRAN  [1]   indirect\nlazyeval               0.2.2   RSPM      0.2.2   CRAN  [1]   indirect\nleaflet           2.1.2.9000 GitHub 2.1.2.9000 GitHub  [1]     direct\nleaflet.providers      1.9.0   RSPM      1.9.0   CRAN  [1]   indirect\nlhs                    1.1.6   RSPM      1.1.6   CRAN  [1]   indirect\nlifecycle              1.0.3   RSPM      1.0.3   CRAN  [1]   indirect\nlistenv                0.9.0   RSPM      0.9.0   CRAN  [1]   indirect\nlubridate              1.9.2   RSPM      1.9.2   CRAN  [1]   indirect\nmagrittr               2.0.3   RSPM      2.0.3   CRAN  [1]   indirect\nmemoise                2.0.1   RSPM      2.0.1   CRAN  [1]   indirect\nmethods                 &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\nmgcv                   1.9-0   RSPM      1.9-0   CRAN  [1]   indirect\nmime                    0.12   RSPM       0.12   CRAN  [1]   indirect\nmodeldata              1.2.0   RSPM      1.2.0   CRAN  [1]   indirect\nmodelenv               0.1.1   RSPM      0.1.1   CRAN  [1]   indirect\nmodelr                0.1.11   RSPM     0.1.11   CRAN  [1]   indirect\nmunsell                0.5.0   RSPM      0.5.0   CRAN  [1]   indirect\nncdf4                   1.21   RSPM       1.21   CRAN  [1]   indirect\nnlme                 3.1-162   CRAN    3.1-162   CRAN  [2]   indirect\nnnet                  7.3-19   CRAN     7.3-19   CRAN  [2]   indirect\nnumDeriv          2016.8-1.1   RSPM 2016.8-1.1   CRAN  [1]   indirect\nopenssl                2.0.6   RSPM      2.0.6   CRAN  [1]   indirect\npROC                  1.18.4   RSPM     1.18.4   CRAN  [1]   indirect\nparallel                &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\nparallelly            1.36.0   RSPM     1.36.0   CRAN  [1]   indirect\nparsnip                1.1.1   RSPM      1.1.1   CRAN  [1]     direct\npatchwork              1.1.2   RSPM      1.1.2   CRAN  [1]     direct\npillar                 1.9.0   RSPM      1.9.0   CRAN  [1]   indirect\npkgconfig              2.0.3   RSPM      2.0.3   CRAN  [1]   indirect\nplyr                   1.8.8   RSPM      1.8.8   CRAN  [1]   indirect\npng                    0.1-8   RSPM      0.1-8   CRAN  [1]   indirect\nprettyunits            1.1.1   RSPM      1.1.1   CRAN  [1]   indirect\nprocessx               3.8.2   RSPM      3.8.2   CRAN  [1]   indirect\nprodlim           2023.03.31   RSPM 2023.03.31   CRAN  [1]   indirect\nprogress               1.2.2   RSPM      1.2.2   CRAN  [1]   indirect\nprogressr             0.13.0   RSPM     0.13.0   CRAN  [1]   indirect\nprotolite              2.2.0   RSPM      2.2.0   CRAN  [1]   indirect\nproxy                 0.4-27   RSPM     0.4-27   CRAN  [1]   indirect\nps                     1.7.5   RSPM      1.7.5   CRAN  [1]   indirect\npurrr                  1.0.1   RSPM      1.0.1   CRAN  [1]     direct\nragg                   1.2.5   RSPM      1.2.5   CRAN  [1]   indirect\nrapidjsonr             1.2.0   RSPM      1.2.0   CRAN  [1]   indirect\nrappdirs               0.3.3   RSPM      0.3.3   CRAN  [1]   indirect\nraster                3.6-20   RSPM     3.6-20   CRAN  [1]   indirect\nreadr                  2.1.4   RSPM      2.1.4   CRAN  [1]     direct\nreadxl                 1.4.3   RSPM      1.4.3   CRAN  [1]   indirect\nrecipes                1.0.6   RSPM      1.0.6   CRAN  [1]   indirect\nrematch                1.0.1   RSPM      1.0.1   CRAN  [1]   indirect\nrematch2               2.1.2   RSPM      2.1.2   CRAN  [1]   indirect\nrenv                  0.17.3   RSPM     0.17.3   CRAN  [1]     direct\nreprex                 2.0.2   RSPM      2.0.2   CRAN  [1]   indirect\nreshape2               1.4.4   RSPM      1.4.4   CRAN  [1]   indirect\nrlang                  1.1.1   RSPM      1.1.1   CRAN  [1]   indirect\nrmarkdown               2.21   RSPM       2.21   CRAN  [1]     direct\nrpart                 4.1.19   CRAN     4.1.19   CRAN  [2]   indirect\nrprojroot              2.0.3   RSPM      2.0.3   CRAN  [1]   indirect\nrsample                1.1.1   RSPM      1.1.1   CRAN  [1]     direct\nrstudioapi              0.14   RSPM       0.14   CRAN  [1]   indirect\nrvest                  1.0.3   RSPM      1.0.3   CRAN  [1]   indirect\ns2                     1.1.3   RSPM      1.1.3   CRAN  [1]   indirect\nsass                   0.4.6   RSPM      0.4.6   CRAN  [1]   indirect\nscales                 1.2.1   RSPM      1.2.1   CRAN  [1]     direct\nselectr                0.4-2   RSPM      0.4-2   CRAN  [1]   indirect\nsf                    1.0-12   RSPM     1.0-12   CRAN  [1]     direct\nsfheaders              0.4.2   RSPM      0.4.2   CRAN  [1]   indirect\nshape                  1.4.6   RSPM      1.4.6   CRAN  [1]   indirect\nsignal                 0.7-7   RSPM      0.7-7   CRAN  [1]     direct\nslider                 0.3.0   RSPM      0.3.0   CRAN  [1]   indirect\nsodium                 1.2.1   RSPM      1.2.1   CRAN  [1]   indirect\nsp                     1.6-0   RSPM      1.6-0   CRAN  [1]   indirect\nspatial               7.3-16   CRAN       &lt;NA&gt;   &lt;NA&gt;  [2]       &lt;NA&gt;\nsplines                 &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\nstats                   &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\nstats4                  &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\nstringi               1.7.12   RSPM     1.7.12   CRAN  [1]   indirect\nstringr                1.5.0   RSPM      1.5.0   CRAN  [1]   indirect\nsurvival               3.5-5   CRAN      3.5-5   CRAN  [2]   indirect\nsys                    3.4.1   RSPM      3.4.1   CRAN  [1]   indirect\nsystemfonts            1.0.4   RSPM      1.0.4   CRAN  [1]   indirect\nterra                 1.7-29   RSPM     1.7-29   CRAN  [1]     direct\ntextshaping            0.3.6   RSPM      0.3.6   CRAN  [1]   indirect\ntibble                 3.2.1   RSPM      3.2.1   CRAN  [1]   indirect\ntidymodels             1.1.0   RSPM      1.1.0   CRAN  [1]     direct\ntidyr                  1.3.0   RSPM      1.3.0   CRAN  [1]     direct\ntidyselect             1.2.0   RSPM      1.2.0   CRAN  [1]   indirect\ntidyterra              0.4.0   RSPM      0.4.0   CRAN  [1]     direct\ntidyverse              2.0.0   RSPM      2.0.0   CRAN  [1]     direct\ntimeDate            4022.108   RSPM   4022.108   CRAN  [1]   indirect\ntimechange             0.2.0   RSPM      0.2.0   CRAN  [1]   indirect\ntinytex                 0.45   RSPM       0.45   CRAN  [1]   indirect\ntools                   &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\ntriebeard              0.4.1   RSPM      0.4.1   CRAN  [1]   indirect\ntune                   1.1.1   RSPM      1.1.1   CRAN  [1]     direct\ntzdb                   0.4.0   RSPM      0.4.0   CRAN  [1]   indirect\nunits                  0.8-2   RSPM      0.8-2   CRAN  [1]   indirect\nurltools               1.7.3   RSPM      1.7.3   CRAN  [1]   indirect\nutf8                   1.2.3   RSPM      1.2.3   CRAN  [1]   indirect\nutils                   &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  [2]   indirect\nuuid                   1.1-1   RSPM      1.1-1   CRAN  [1]   indirect\nvctrs                  0.6.2   RSPM      0.6.2   CRAN  [1]   indirect\nviridis                0.6.3   RSPM      0.6.3   CRAN  [1]   indirect\nviridisLite            0.4.2   RSPM      0.4.2   CRAN  [1]   indirect\nvroom                  1.6.3   RSPM      1.6.3   CRAN  [1]     direct\nwarp                   0.2.0   RSPM      0.2.0   CRAN  [1]   indirect\nwithr                  2.5.0   RSPM      2.5.0   CRAN  [1]   indirect\nwk                     0.7.2   RSPM      0.7.2   CRAN  [1]   indirect\nworkflows              1.1.3   RSPM      1.1.3   CRAN  [1]     direct\nworkflowsets           1.0.1   RSPM      1.0.1   CRAN  [1]   indirect\nxfun                    0.39   RSPM       0.39   CRAN  [1]   indirect\nxgboost              1.7.5.1   RSPM    1.7.5.1   CRAN  [1]     direct\nxml2                   1.3.5   RSPM      1.3.5   CRAN  [1]   indirect\nyaml                   2.3.7   RSPM      2.3.7   CRAN  [1]   indirect\nyardstick              1.2.0   RSPM      1.2.0   CRAN  [1]   indirect\n\n[1]: /home/runner/work/handful_of_pixels/handful_of_pixels/renv/library/R-4.3/x86_64-pc-linux-gnu\n[2]: /home/runner/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/5cd49154                       \n\n# ABI ================================\n* No ABI conflicts were detected in the set of installed packages.\n\n# User Profile =======================\n[1] Source  Package Require Version Dev    \n&lt;0 rows&gt; (or 0-length row.names)\n\n# Settings ===========================\nList of 11\n $ bioconductor.version     : chr(0) \n $ external.libraries       : chr(0) \n $ ignored.packages         : chr(0) \n $ package.dependency.fields: chr [1:3] \"Imports\" \"Depends\" \"LinkingTo\"\n $ r.version                : chr(0) \n $ snapshot.type            : chr \"implicit\"\n $ use.cache                : logi TRUE\n $ vcs.ignore.cellar        : logi TRUE\n $ vcs.ignore.library       : logi TRUE\n $ vcs.ignore.local         : logi TRUE\n $ vcs.manage.ignores       : logi TRUE\n\n# Options ============================\nList of 9\n $ defaultPackages                     : chr [1:6] \"datasets\" \"utils\" \"grDevices\" \"graphics\" ...\n $ download.file.method                : NULL\n $ download.file.extra                 : NULL\n $ install.packages.compile.from.source: NULL\n $ pkgType                             : chr \"source\"\n $ repos                               : Named chr \"https://cloud.r-project.org\"\n  ..- attr(*, \"names\")= chr \"CRAN\"\n $ renv.consent                        : logi TRUE\n $ renv.project.path                   : chr \"/home/runner/work/handful_of_pixels/handful_of_pixels\"\n $ renv.verbose                        : logi TRUE\n\n# Environment Variables ==============\nHOME                        = /home/runner\nLANG                        = C.UTF-8\nMAKE                        = make\nR_LIBS                      = &lt;NA&gt;\nR_LIBS_SITE                 = /opt/R/4.3.1/lib/R/site-library\nR_LIBS_USER                 = /home/runner/work/handful_of_pixels/handful_of_pixels/renv/library/R-4.3/x86_64-pc-linux-gnu:/home/runner/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/5cd49154\nRENV_CONFIG_REPOS_OVERRIDE  = https://packagemanager.rstudio.com/cran/__linux__/jammy/latest\nRENV_DEFAULT_R_ENVIRON      = &lt;NA&gt;\nRENV_DEFAULT_R_ENVIRON_USER = &lt;NA&gt;\nRENV_DEFAULT_R_LIBS         = &lt;NA&gt;\nRENV_DEFAULT_R_LIBS_SITE    = /opt/R/4.3.1/lib/R/site-library\nRENV_DEFAULT_R_LIBS_USER    = /home/runner/work/_temp/Library\nRENV_DEFAULT_R_PROFILE      = &lt;NA&gt;\nRENV_DEFAULT_R_PROFILE_USER = &lt;NA&gt;\nRENV_PROJECT                = /home/runner/work/handful_of_pixels/handful_of_pixels\n\n# PATH ===============================\n- /home/runner/.local/bin\n- /opt/pipx_bin\n- /home/runner/.cargo/bin\n- /home/runner/.config/composer/vendor/bin\n- /usr/local/.ghcup/bin\n- /home/runner/.dotnet/tools\n- /snap/bin\n- /usr/local/sbin\n- /usr/local/bin\n- /usr/sbin\n- /usr/bin\n- /sbin\n- /bin\n- /usr/games\n- /usr/local/games\n- /snap/bin\n\n# Cache ==============================\nThere are a total of 200 packages installed in the renv cache.\nCache path: \"~/.cache/R/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu\""
  }
]